lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance <- cumsum(pima_eigenvalues) / sum(pima_eigenvalues)
# Find the number of components that explain at least 90% of the variance
num_components <- which(cumulative_variance >= 0.9)[1]
# Retain the selected number of components
selected_pcs <- pima_pca$x[, 1:num_components]
selected_pcs
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance <- cumsum(pima_eigenvalues) / sum(pima_eigenvalues)
# Find the number of components that explain at least 90% of the variance
num_components <- which(cumulative_variance >= 0.95)[1]
# Retain the selected number of components
selected_pcs <- pima_pca$x[, 1:num_components]
selected_pcs
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.9)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
selected_pcs_pima
selected_pcs_lisbon
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.9)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
ncol(selected_pcs_pima)
ncol(selected_pcs_lisbon)
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.95)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
ncol(selected_pcs_pima)
ncol(selected_pcs_lisbon)
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.9)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
ncol(selected_pcs_pima)
ncol(selected_pcs_lisbon)
#Ex b
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Function to handle missing or infinite values
handle_missing_infinite <- function(data) {
for (col in names(data)){
data[[col]][is.infinite(data[[col]])] <- NA
data[[col]][is.nan(data[[col]])] <- NA
}
data <- na.omit(data)
return(data)
}
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Apply the function to handle missing or infinite values
lisbon_encoded <- handle_missing_infinite(lisbon_encoded)
# Compute SVD for Pima dataset
pima_svd <- svd(scale(pima_encoded[, -ncol(pima_encoded)]))
# Compute SVD for Lisbon dataset
lisbon_svd <- svd(scale(lisbon_encoded[, -c(1, 2, 22, 23, 24)]))
# Plot singular values for Pima dataset
plot(pima_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Pima Dataset")
# Plot singular values for Lisbon dataset
plot(lisbon_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Lisbon Dataset")
# Determine a proporção acumulada da variância explicada por cada componente
cumulative_variance_svd_pima <- cumsum(pima_svd$d^2 / sum(pima_svd$d^2))
cumulative_variance_svd_lisbon <- cumsum(lisbon_svd$d^2 / sum(lisbon_svd$d^2))
# Encontre o número de componentes que explicam pelo menos 90% da variância
k_svd_pima <- which(cumulative_variance_svd_pima >= 0.9)[1]
k_svd_lisbon <- which(cumulative_variance_svd_lisbon >= 0.9)[1]
# Retenha o número selecionado de componentes
selected_svd_pcs_pima <- pima_svd$u[, 1:k_svd_pima] %*% diag(pima_svd$d[1:k_svd_pima])
selected_svd_pcs_lisbon <- lisbon_svd$u[, 1:k_svd_lisbon] %*% diag(lisbon_svd$d[1:k_svd_lisbon])
# Mostrar os componentes selecionados no terminal
selected_svd_pcs_pima
selected_svd_pcs_lisbon
#Ex b
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Function to handle missing or infinite values
handle_missing_infinite <- function(data) {
for (col in names(data)){
data[[col]][is.infinite(data[[col]])] <- NA
data[[col]][is.nan(data[[col]])] <- NA
}
data <- na.omit(data)
return(data)
}
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Apply the function to handle missing or infinite values
lisbon_encoded <- handle_missing_infinite(lisbon_encoded)
# Compute SVD for Pima dataset
pima_svd <- svd(scale(pima_encoded[, -ncol(pima_encoded)]))
# Compute SVD for Lisbon dataset
lisbon_svd <- svd(scale(lisbon_encoded[, -c(1, 2, 22, 23, 24)]))
# Plot singular values for Pima dataset
plot(pima_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Pima Dataset")
# Plot singular values for Lisbon dataset
plot(lisbon_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Lisbon Dataset")
# Determine a proporção acumulada da variância explicada por cada componente
cumulative_variance_svd_pima <- cumsum(pima_svd$d^2 / sum(pima_svd$d^2))
cumulative_variance_svd_lisbon <- cumsum(lisbon_svd$d^2 / sum(lisbon_svd$d^2))
# Encontre o número de componentes que explicam pelo menos 90% da variância
k_svd_pima <- which(cumulative_variance_svd_pima >= 0.9)[1]
k_svd_lisbon <- which(cumulative_variance_svd_lisbon >= 0.9)[1]
# Retenha o número selecionado de componentes
selected_svd_pcs_pima <- pima_svd$u[, 1:k_svd_pima] %*% diag(pima_svd$d[1:k_svd_pima])
selected_svd_pcs_lisbon <- lisbon_svd$u[, 1:k_svd_lisbon] %*% diag(lisbon_svd$d[1:k_svd_lisbon])
# Mostrar os componentes selecionados no terminal
ncol(selected_svd_pcs_pima)
ncol(selected_svd_pcs_lisbon)
library(dplyr)
library(rpart)
library(sparklyr)
library(caret)
library(randomForest) #Implements Random Forest
library(class) #Implements KNN
library(e1071) #Implements SVM
library(pROC) #ROC Curve
set.seed(123)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#########################AUXILIAR FUNCTIONS###################################################
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df <- read.csv("../../mdle_data/out/merged_dataset.csv")
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Creates the new column
df$Active.Energy..kWh. <- ifelse(df$Active.Energy..kWh. > mean_energy, "High", "Low")
#Transforms the new column to factor
df$Active.Energy..kWh. <- as.factor(df$Active.Energy..kWh.)
str(df)
#Without Spark
df <- na.omit(df)
#train_index <- sample(1:nrow(df), size = 0.7 * nrow(df))
train_index <- sample(2, nrow(df), replace = TRUE, prob = c(0.7,0.3))
df_train <- df[train_index==1, ]
df_test <- df[train_index==2, ]
# Train Random Forest model
rf_model <- randomForest(Active.Energy.Class ~ ., data = df_train)
# Train Random Forest model
rf_model <- randomForest(Active.Energy..kWh. ~ ., data = df_train)
predictions <- predict(rf_model, newdata = df_test)
probabilities <- predict(rf_model, newdata = df_test, type = "prob")
confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy.Class)
confusion_Matrix <- confusionMatrix(predictions, df_test$Active.Energy..kWh. )
#confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy.Class)
print(confusion_matrix)
confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy..kWh. )
#confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy.Class)
print(confusion_matrix)
#Calcules the ROC curve
roc_curve <- roc(df_test$Active.Energy.Class, probabilities[, 2])
#Calcules the ROC curve
roc_curve <- roc(df_test$Active.Energy..kWh., probabilities[, 2])
plot(roc_curve, col = "blue", main = "Curva ROC para o Modelo Random Forest")
abline(a = 0, b = 1, lty = 2, col = "red")
df <- subset(df, select = -c(datetime, Date, Hour))
numeric_cols <- sapply(df, is.numeric)
#Don't normalize
numeric_cols["Active.Energy..kWh."] <- FALSE
#Normalize the numeric columns
df[numeric_cols] <- as.data.frame(lapply(df[numeric_cols], normalize))
train_index <- sample(2, nrow(df), replace = TRUE, prob = c(0.7,0.3))
df_train <- df[train_index==1, ]
df_test <- df[train_index==2, ]
library(dplyr)
library(rpart)
library(sparklyr)
library(caret)
library(randomForest) #Implements Random Forest
library(class) #Implements KNN
library(e1071) #Implements SVM
library(pROC) #ROC Curve
set.seed(123)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#########################AUXILIAR FUNCTIONS###################################################
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df <- read.csv("../../mdle_data/out/merged_dataset.csv")
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Creates the new column
df$Active.Energy.Class <- ifelse(df$Active.Energy..kWh. > mean_energy, "High", "Low")
#Transforms the new column to factor
df$Active.Energy.Class <- as.factor(df$Active.Energy.Class)
#Remove column Active.Energy..kWh.
df <- df %>% select(-c(Active.Energy..kWh.))
str(df)
#Without Spark
df <- na.omit(df)
#train_index <- sample(1:nrow(df), size = 0.7 * nrow(df))
train_index <- sample(2, nrow(df), replace = TRUE, prob = c(0.7,0.3))
df_train <- df[train_index==1, ]
df_test <- df[train_index==2, ]
# Train Random Forest model
rf_model <- randomForest(Active.Energy.Class ~ ., data = df_train)
predictions <- predict(rf_model, newdata = df_test)
probabilities <- predict(rf_model, newdata = df_test, type = "prob")
confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy..kWh. )
confusion_matrix <- confusionMatrix(predictions, df_test$Active.Energy.Class)
print(confusion_matrix)
df <- subset(df, select = -c(datetime, Date, Hour))
numeric_cols <- sapply(df, is.numeric)
#Don't normalize
numeric_cols["Active.Energy.Class"] <- FALSE
#Normalize the numeric columns
df[numeric_cols] <- as.data.frame(lapply(df[numeric_cols], normalize))
train_index <- sample(2, nrow(df), replace = TRUE, prob = c(0.7,0.3))
df_train <- df[train_index==1, ]
df_test <- df[train_index==2, ]
# Separate features and labels
features <- names(df)[!names(df) %in% c("Active.Energy.Class")]
train_features <- df_train[, features]
train_labels <- df_train$Active.Energy.Class
test_features <- df_test[, features]
test_labels <- df_test$Active.Energy.Class
# Verificar valores ausentes nos dados de treino
anyNA(train_features)
anyNA(test_features)
anyNA(train_labels)
anyNA(test_labels)
#Define number of neighbours
k <- 3
#Train the model
predictions <- knn(train = train_features, test = test_features, cl = train_labels, k = k)
confusion_matrix <- confusionMatrix(predictions, test_labels)
print(confusion_matrix)
probabilities <- predict(rf_model, newdata = df_test, type = "prob")
set.seed(123)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#########################AUXILIAR FUNCTIONS###################################################
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df <- read.csv("../../mdle_data/out/merged_dataset.csv")
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Creates the new column
df$Active.Energy.Class <- ifelse(df$Active.Energy..kWh. > mean_energy, "High", "Low")
#Transforms the new column to factor
df$Active.Energy.Class <- as.factor(df$Active.Energy.Class)
#Remove column Active.Energy..kWh.
df <- df %>% select(-c(Active.Energy..kWh.))
#############################################################################################
################SMV##########################################################################
df <- na.omit(df)
df <- subset(df, select = -c(datetime, Date, Hour))
train_index <- sample(1:nrow(df), size = 0.7 * nrow(df))
df_train <- df[train_index, ]
df_test <- df[-train_index, ]
# Separate features and labels
train_features <- df_train[, features]
# Separate features and labels
features <- df %>% names()
features <- features[features != "Active.Energy.Class"]
train_features <- df_train[, features]
train_labels <- df_train$Active.Energy.Class
test_features <- df_test[, features]
test_labels <- df_test$Active.Energy.Class
# Trains SVM model
svm_model <- svm(train_features, train_labels, kernel = "radial")
predictions <- predict(svm_model, test_features)
confusion_matrix <- confusionMatrix(predictions, test_labels)
print(confusion_matrix)
set.seed(123)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#########################AUXILIAR FUNCTIONS###################################################
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df <- read.csv("../../mdle_data/out/merged_dataset.csv")
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Creates the new column
df$Active.Energy.Class <- ifelse(df$Active.Energy..kWh. > mean_energy, "High", "Low")
#Transforms the new column to factor
df$Active.Energy.Class <- as.factor(df$Active.Energy.Class)
#Remove column Active.Energy..kWh.
df <- df %>% select(-c(Active.Energy..kWh.))
#############################################################################################
################SMV##########################################################################
df <- na.omit(df)
df <- subset(df, select = -c(datetime, Date, Hour))
set.seed(123)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#########################AUXILIAR FUNCTIONS###################################################
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
df <- read.csv("../../mdle_data/out/merged_dataset.csv")
#Calculate the mean of column Active.Energy..kWh.
mean_energy <- mean(df$Active.Energy..kWh., na.rm = TRUE)
#Creates the new column
df$Active.Energy.Class <- ifelse(df$Active.Energy..kWh. > mean_energy, "High", "Low")
#Transforms the new column to factor
df$Active.Energy.Class <- as.factor(df$Active.Energy.Class)
#Remove column Active.Energy..kWh.
df <- df %>% select(-c(Active.Energy..kWh.))
#############################################################################################
################SMV##########################################################################
df <- na.omit(df)
train_index <- sample(1:nrow(df), size = 0.7 * nrow(df))
df_train <- df[train_index, ]
df_test <- df[-train_index, ]
# Separate features and labels
features <- df %>% names()
features <- features[features != "Active.Energy.Class"]
train_features <- df_train[, features]
train_labels <- df_train$Active.Energy.Class
test_features <- df_test[, features]
test_labels <- df_test$Active.Energy.Class
# Trains SVM model
svm_model <- svm(train_features, train_labels, kernel = "radial")
