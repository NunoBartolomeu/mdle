# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.95)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
ncol(selected_pcs_pima)
ncol(selected_pcs_lisbon)
#Feature Reduction
#Ex a) PCA decomposition
# Load necessary libraries
library(dplyr)
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
lisbon_encoded <- lisbon %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Remove constant columns
lisbon_encoded <- lisbon_encoded[, -which(sapply(lisbon_encoded, function(x) length(unique(x))) == 1)]
# Perform PCA
pima_pca <- prcomp(pima_encoded[, -ncol(pima_encoded)], center = TRUE, scale. = TRUE)
lisbon_pca <- prcomp(lisbon_encoded[, -c(1, 2, 22, 23, 24)], center = TRUE, scale. = TRUE)
# Compute eigenvalues
pima_eigenvalues <- (pima_pca$sdev)^2
lisbon_eigenvalues <- (lisbon_pca$sdev)^2
# Sort eigenvalues in decreasing order
pima_eigenvalues <- sort(pima_eigenvalues, decreasing = TRUE)
lisbon_eigenvalues <- sort(lisbon_eigenvalues, decreasing = TRUE)
# Plot the eigenvalues
plot(pima_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Pima Dataset")
plot(lisbon_eigenvalues, type = "b", xlab = "Principal Component", ylab = "Eigenvalue", main = "Eigenvalues - Lisbon Dataset")
# Determine the cumulative proportion of variance explained by each component
cumulative_variance_pima <- cumsum(pima_pca$sdev^2 / sum(pima_pca$sdev^2))
cumulative_variance_lisbon <- cumsum(lisbon_pca$sdev^2 / sum(lisbon_pca$sdev^2))
# Find the number of components that explain at least 90% of the variance
k_pima <- which(cumulative_variance_pima >= 0.9)[1]
k_lisbon <- which(cumulative_variance_lisbon >= 0.9)[1]
# Retain the selected number of components
selected_pcs_pima <- pima_pca$x[, 1:k_pima]
selected_pcs_lisbon <- lisbon_pca$x[, 1:k_lisbon]
# Show selected components in terminal
ncol(selected_pcs_pima)
ncol(selected_pcs_lisbon)
#Ex b
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Function to handle missing or infinite values
handle_missing_infinite <- function(data) {
for (col in names(data)){
data[[col]][is.infinite(data[[col]])] <- NA
data[[col]][is.nan(data[[col]])] <- NA
}
data <- na.omit(data)
return(data)
}
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Apply the function to handle missing or infinite values
lisbon_encoded <- handle_missing_infinite(lisbon_encoded)
# Compute SVD for Pima dataset
pima_svd <- svd(scale(pima_encoded[, -ncol(pima_encoded)]))
# Compute SVD for Lisbon dataset
lisbon_svd <- svd(scale(lisbon_encoded[, -c(1, 2, 22, 23, 24)]))
# Plot singular values for Pima dataset
plot(pima_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Pima Dataset")
# Plot singular values for Lisbon dataset
plot(lisbon_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Lisbon Dataset")
# Determine a proporção acumulada da variância explicada por cada componente
cumulative_variance_svd_pima <- cumsum(pima_svd$d^2 / sum(pima_svd$d^2))
cumulative_variance_svd_lisbon <- cumsum(lisbon_svd$d^2 / sum(lisbon_svd$d^2))
# Encontre o número de componentes que explicam pelo menos 90% da variância
k_svd_pima <- which(cumulative_variance_svd_pima >= 0.9)[1]
k_svd_lisbon <- which(cumulative_variance_svd_lisbon >= 0.9)[1]
# Retenha o número selecionado de componentes
selected_svd_pcs_pima <- pima_svd$u[, 1:k_svd_pima] %*% diag(pima_svd$d[1:k_svd_pima])
selected_svd_pcs_lisbon <- lisbon_svd$u[, 1:k_svd_lisbon] %*% diag(lisbon_svd$d[1:k_svd_lisbon])
# Mostrar os componentes selecionados no terminal
selected_svd_pcs_pima
selected_svd_pcs_lisbon
#Ex b
# Load datasets
pima <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/pima.csv")
lisbon <- read.csv("C:/Users/pedro/Desktop/Mestrado/MDLE/lab/lab02/Lisbon_ 2023-01-01_2023-01-31.csv")
# Function to handle missing or infinite values
handle_missing_infinite <- function(data) {
for (col in names(data)){
data[[col]][is.infinite(data[[col]])] <- NA
data[[col]][is.nan(data[[col]])] <- NA
}
data <- na.omit(data)
return(data)
}
# Encode categorical variables
pima_encoded <- pima %>%
mutate(across(where(is.character), as.factor)) %>%
mutate(across(where(is.factor), as.integer))
# Apply the function to handle missing or infinite values
lisbon_encoded <- handle_missing_infinite(lisbon_encoded)
# Compute SVD for Pima dataset
pima_svd <- svd(scale(pima_encoded[, -ncol(pima_encoded)]))
# Compute SVD for Lisbon dataset
lisbon_svd <- svd(scale(lisbon_encoded[, -c(1, 2, 22, 23, 24)]))
# Plot singular values for Pima dataset
plot(pima_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Pima Dataset")
# Plot singular values for Lisbon dataset
plot(lisbon_svd$d, type = "b", xlab = "Singular Value", ylab = "Value", main = "Singular Values - Lisbon Dataset")
# Determine a proporção acumulada da variância explicada por cada componente
cumulative_variance_svd_pima <- cumsum(pima_svd$d^2 / sum(pima_svd$d^2))
cumulative_variance_svd_lisbon <- cumsum(lisbon_svd$d^2 / sum(lisbon_svd$d^2))
# Encontre o número de componentes que explicam pelo menos 90% da variância
k_svd_pima <- which(cumulative_variance_svd_pima >= 0.9)[1]
k_svd_lisbon <- which(cumulative_variance_svd_lisbon >= 0.9)[1]
# Retenha o número selecionado de componentes
selected_svd_pcs_pima <- pima_svd$u[, 1:k_svd_pima] %*% diag(pima_svd$d[1:k_svd_pima])
selected_svd_pcs_lisbon <- lisbon_svd$u[, 1:k_svd_lisbon] %*% diag(lisbon_svd$d[1:k_svd_lisbon])
# Mostrar os componentes selecionados no terminal
ncol(selected_svd_pcs_pima)
ncol(selected_svd_pcs_lisbon)
library(dplyr)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
consumo_horario <- read.csv("../mdle_data/consumos_horario_codigo_postal.csv", sep = ";")
census <- read.csv("../mdle_data/INE/Census/Census_final_data.csv", sep = ",")
census <- census %>% rename(Zip.Code = 'postal_code')
# Combinar os data frames usando merge()
merged_df <- merge(census, consumo_horario, by = "Zip.Code")
View(census)
View(consumo_horario)
View(consumo_horario)
data <- consumo_horario %>% select(-Date.Time)
View(data)
View(census)
df <- data %>%
mutate(datetime = paste(Date, "T", Hour, ":00", sep = ""))
View(df)
df <- df %>%
select(datetime, Date, Hour, `Zip Code`, `Active Energy (kWh)`)
df <- df %>%
select(datetime, Date, Hour, `Zip.Code`, `Active Energy (kWh)`)
df <- df %>%
select(datetime, Date, Hour, `Zip.Code`, `Active.Energy..KWh`)
df <- df %>%
select(datetime, Date, Hour, `Zip.Code`, `Active.Energy..KWh.`)
df <- df %>%
select(datetime, Date, Hour, Zip.Code, `Active.Energy..KWh.`)
df <- df %>%
select(datetime, Date, Hour, Zip.Code, Active.Energy..KWh.)
df <- df %>%
select(datetime, Date, Hour, Zip.Code, Active.Energy..kWh.)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
# List of filenames
filenames <- c(
"Lisbon_2023-01-01_2023-01-31.csv",
"Lisbon_2022-12-01_2022-12-31.csv",
"Lisbon_2023-02-01_2023-02-28.csv",
"Lisbon_2023-03-01_2023-03-31.csv",
"Lisbon_2023-04-01_2023-04-30.csv",
"Lisbon_2023-05-01_2023-05-31.csv",
"Lisbon_2023-06-01_2023-06-30.csv",
"Lisbon_2023-07-01_2023-07-31.csv",
"Lisbon_2023-08-01_2023-08-31.csv",
"Lisbon_2023-09-01_2023-09-30.csv",
"Lisbon_2023-10-01_2023-10-31.csv",
"Lisbon_2023-11-01_2023-11-30.csv",
"Lisbon_2023-12-01_2023-12-31.csv"
)
# Initialize an empty data frame to store merged data
merged_data <- data.frame()
# Loop through each file, read it, and append to merged_data
for (filename in filenames) {
file_path <- paste("../mdle_data/Weather/", filename, sep="")
data <- read.csv(file_path, header=TRUE)
merged_data <- rbind(merged_data, data)
}
print(dirname(current_path))
# Loop through each file, read it, and append to merged_data
for (filename in filenames) {
file_path <- paste("../mdle_data/Weather/", filename, sep="")
data <- read.csv(file_path, header=TRUE)
merged_data <- rbind(merged_data, data)
}
# Write merged data to a new CSV file
write.csv(merged_data, file="../mdle_data/Weather/Lisbon_2022-12-01_2023-12-31_full.csv", row.names = FALSE)
#
variance_lisbon <- apply(merged_data, 2, function(x) var(x, na.rm = TRUE))
#
variance_lisbon <- apply(merged_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance_lisbon <- data.frame(features = colnames(merged_data), variance = variance_lisbon)
print("Lisbon Dataset:")
print(relevance_lisbon)
# Remove columns with 0 variance
valid_indices <- which(relevance_lisbon$variance > 0 | relevance_lisbon$features == "datetime")
# Remove the specified columns
filtered_merged_data <- merged_data[, valid_indices]
print("Lisbon Dataset (filtered):")
print(filtered_merged_data)
# Write the filtered data to a new CSV file
write.csv(filtered_merged_data, file="../mdle_data/Weather/Lisbon_2022-12-01_2023-12-31_filtered.csv", row.names=FALSE)
# Write the filtered data to a new CSV file
write.csv(filtered_merged_data, file="../mdle_data/Weather/filtered_weather_Lisbon.csv", row.names=FALSE)
write.csv(df, file="../mdle_data/out/filtered_consumption_Lisbon.csv")
#Seleção de atributos por variância
variance <- apply(agregated_data, 2, function(x) var(x, na.rm = TRUE))
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
#Armazena os dados bgri2021 com a nova coluna com os códigos postais num CSV
write.csv(bgri2021_data, file="../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv")
bgri2021_data <- read.csv("../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv", sep = ",")
#Retira as colunas desnecessárias
census_data <- subset(bgri2021_data, select = -c(longitude, latitude, SHAPE_Length, SHAPE_Area, NUTS3, NUTS2, NUTS1,
SUBSECCAO, SECSSNUM21, SSNUM21, SECNUM21, DTMNFRSEC21, DTMNFR21,
DTMN21, DT21, BGRI2021, OBJECTID, Y, X))
#Armazena os dados censeus após a remoção de colunas desnecessárias num CSV
write.csv(census_data, file="../mdle_data/INE/Census/Census_by_postalCode.csv")
#Filtra o código postal para permanecer os 4 primeiros dígitos e soma os valores nos casos em que os códigos postais sejam iguais
census_data$postal_code <- gsub("-", "", census_data$postal_code)
census_data$postal_code <- substr(census_data$postal_code, 1, 4)
aggregated_data <- census_data %>%
group_by(postal_code) %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
aggregated_data <- aggregated_data %>% slice(-n())
View(aggregated_data)
postal_code <- aggregated_data %>% select(postal_code)
View(postal_code)
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance <- data.frame(features = colnames(aggregated_data), variance = variance)
print("Census Dataset:")
print(relevance)
View(aggregated_data)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0 | relevance$features == "X.1")
filtered_merged_data <- merged_data[, valid_indices]
filtered_data <- aggregated_data[, valid_indices]
print(filtered_data)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0)
filtered_data <- aggregated_data[, valid_indices]
print("Census Dataset (filtered):")
print(filtered_data)
filtered_data <- filtered_data %>% select(-"X.1")
print("Census Dataset (filtered):")
print(filtered_data)
#Armazena os dados finais dos census com a filtragem dos códigos postais
write.csv(aggregated_data, file="../mdle_data/out/filtered_census.csv")
library(dplyr)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
library(dplyr)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
View(census)
View(census)
View(consumption)
bgri2021_data <- read.csv("../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv", sep = ",")
#Retira as colunas desnecessárias
census_data <- subset(bgri2021_data, select = -c(longitude, latitude, SHAPE_Length, SHAPE_Area, NUTS3, NUTS2, NUTS1,
SUBSECCAO, SECSSNUM21, SSNUM21, SECNUM21, DTMNFRSEC21, DTMNFR21,
DTMN21, DT21, BGRI2021, OBJECTID, Y, X))
#Armazena os dados censeus após a remoção de colunas desnecessárias num CSV
write.csv(census_data, file="../mdle_data/INE/Census/Census_by_postalCode.csv")
#Filtra o código postal para permanecer os 4 primeiros dígitos e soma os valores nos casos em que os códigos postais sejam iguais
census_data$postal_code <- gsub("-", "", census_data$postal_code)
census_data$postal_code <- substr(census_data$postal_code, 1, 4)
aggregated_data <- census_data %>%
group_by(postal_code) %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
aggregated_data <- aggregated_data %>% slice(-n())
postal_code <- aggregated_data %>% select(postal_code)
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance <- data.frame(features = colnames(aggregated_data), variance = variance)
print("Census Dataset:")
print(relevance)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0)
filtered_data <- aggregated_data[, valid_indices]
filtered_data <- filtered_data %>% select(-"X.1")
print("Census Dataset (filtered):")
print(filtered_data)
census <- census %>% rename(Zip.Code = 'postal_code')
View(census)
print(census)
bgri2021_data <- read.csv("../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv", sep = ",")
#Retira as colunas desnecessárias
census_data <- subset(bgri2021_data, select = -c(longitude, latitude, SHAPE_Length, SHAPE_Area, NUTS3, NUTS2, NUTS1,
SUBSECCAO, SECSSNUM21, SSNUM21, SECNUM21, DTMNFRSEC21, DTMNFR21,
DTMN21, DT21, BGRI2021, OBJECTID, Y, X))
#Armazena os dados censeus após a remoção de colunas desnecessárias num CSV
write.csv(census_data, file="../mdle_data/INE/Census/Census_by_postalCode.csv")
#Filtra o código postal para permanecer os 4 primeiros dígitos e soma os valores nos casos em que os códigos postais sejam iguais
census_data$postal_code <- gsub("-", "", census_data$postal_code)
census_data$postal_code <- substr(census_data$postal_code, 1, 4)
aggregated_data <- census_data %>%
group_by(postal_code) %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
aggregated_data <- aggregated_data %>% slice(-n())
postal_code <- aggregated_data %>% select(postal_code)
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance <- data.frame(features = colnames(aggregated_data), variance = variance)
print("Census Dataset:")
print(relevance)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0)
filtered_data <- aggregated_data[, valid_indices]
print("Census Dataset (filtered):")
print(filtered_data)
census <- census %>% rename(Zip.Code = 'postal_code')
census <- filtered_data %>% rename(Zip.Code = 'postal_code')
census <- census %>% select(-c("X.1", "X"))
View(census)
census <- census %>% select(-c("X.1"))
View(census)
#Armazena os dados finais dos census com a filtragem dos códigos postais
write.csv(aggregated_data, file="../mdle_data/out/filtered_census.csv")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
View(census)
bgri2021_data <- read.csv("../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv", sep = ",")
#Retira as colunas desnecessárias
census_data <- subset(bgri2021_data, select = -c(longitude, latitude, SHAPE_Length, SHAPE_Area, NUTS3, NUTS2, NUTS1,
SUBSECCAO, SECSSNUM21, SSNUM21, SECNUM21, DTMNFRSEC21, DTMNFR21,
DTMN21, DT21, BGRI2021, OBJECTID, Y, X))
#Armazena os dados censeus após a remoção de colunas desnecessárias num CSV
write.csv(census_data, file="../mdle_data/INE/Census/Census_by_postalCode.csv")
#Filtra o código postal para permanecer os 4 primeiros dígitos e soma os valores nos casos em que os códigos postais sejam iguais
census_data$postal_code <- gsub("-", "", census_data$postal_code)
census_data$postal_code <- substr(census_data$postal_code, 1, 4)
aggregated_data <- census_data %>%
group_by(postal_code) %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
aggregated_data <- aggregated_data %>% slice(-n())
postal_code <- aggregated_data %>% select(postal_code)
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance <- data.frame(features = colnames(aggregated_data), variance = variance)
print("Census Dataset:")
print(relevance)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0)
filtered_data <- aggregated_data[, valid_indices]
print("Census Dataset (filtered):")
print(filtered_data)
census <- filtered_data %>% rename(Zip.Code = 'postal_code')
census <- census %>% select(-c("X.1"))
#Armazena os dados finais dos census com a filtragem dos códigos postais
write.csv(aggregated_data, file="../mdle_data/out/filtered_census.csv")
View(census)
#Armazena os dados finais dos census com a filtragem dos códigos postais
write.csv(census, file="../mdle_data/out/filtered_census.csv")
View(census)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
View(census)
bgri2021_data <- read.csv("../mdle_data/INE/BGRI2021_1106-LX/BGRI2021_with_postalCodes.csv", sep = ",")
#Retira as colunas desnecessárias
census_data <- subset(bgri2021_data, select = -c(longitude, latitude, SHAPE_Length, SHAPE_Area, NUTS3, NUTS2, NUTS1,
SUBSECCAO, SECSSNUM21, SSNUM21, SECNUM21, DTMNFRSEC21, DTMNFR21,
DTMN21, DT21, BGRI2021, OBJECTID, Y, X))
#Armazena os dados censeus após a remoção de colunas desnecessárias num CSV
write.csv(census_data, file="../mdle_data/INE/Census/Census_by_postalCode.csv")
#Filtra o código postal para permanecer os 4 primeiros dígitos e soma os valores nos casos em que os códigos postais sejam iguais
census_data$postal_code <- gsub("-", "", census_data$postal_code)
census_data$postal_code <- substr(census_data$postal_code, 1, 4)
aggregated_data <- census_data %>%
group_by(postal_code) %>%
summarise(across(where(is.numeric), sum, na.rm = TRUE))
aggregated_data <- aggregated_data %>% slice(-n())
postal_code <- aggregated_data %>% select(postal_code)
#Seleção de atributos por variância
variance <- apply(aggregated_data, 2, function(x) var(x, na.rm = TRUE))
# Combine relevance measures for each dataset
relevance <- data.frame(features = colnames(aggregated_data), variance = variance)
print("Census Dataset:")
print(relevance)
# Remove columns with 0 variance
valid_indices <- which(relevance$variance > 0)
filtered_data <- aggregated_data[, valid_indices]
print("Census Dataset (filtered):")
print(filtered_data)
census <- filtered_data %>% rename(Zip.Code = 'postal_code')
census <- census %>% select(-c("X.1"))
#Armazena os dados finais dos census com a filtragem dos códigos postais
write.csv(census, file="../mdle_data/out/filtered_census.csv", row.names = FALSE)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
View(census)
#Merge the dataset "census" and "consumption" by Zip Code
df <- merge(census, consumo_horario, by = "Zip.Code")
#Merge the dataset "census" and "consumption" by Zip Code
df <- merge(census, consumption, by = "Zip.Code")
View(df)
View(consumption)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
consumo_horario <- read.csv("../mdle_data/consumos_horario_codigo_postal.csv", sep = ";")
data <- consumo_horario %>% select(-Date.Time)
df <- data %>%
mutate(datetime = paste(Date, "T", Hour, ":00", sep = ""))
df <- df %>%
select(datetime, Date, Hour, Zip.Code, Active.Energy..kWh.)
write.csv(df, file="../mdle_data/out/filtered_consumption_Lisbon.csv", row.names = FALSE)
library(dplyr)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
#Merge the dataset "census" and "consumption" by Zip Code
df <- merge(census, consumption, by = "Zip.Code")
View(df)
library(dplyr)
# Set the working directory to the 'src' directory
# Getting the path of your current open file
current_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(current_path ))
#Read all the datasets "filtered_weather_Lisbon.csv", "filtered_consumption_Lisbon.csv", "filtered_census.csv"
weather <- read.csv("../mdle_data/out/filtered_weather_Lisbon.csv", sep= ",")
consumption <- read.csv("../mdle_data/out/filtered_consumption_Lisbon.csv", sep= ",")
census <- read.csv("../mdle_data/out/filtered_census.csv", sep= ",")
#Merge the dataset "census" and "consumption" by Zip Code
df <- merge(consumption, census, by = "Zip.Code")
View(consumption)
View(df)
#Merge the dataset "weather" to "df"
df <- merge(df, weather, by = "datetime")
View(df)
write("../mdle_data/out/merged_dataset.csv")
write("../mdle_data/out/merged_dataset.csv", row.names = FALSE)
write.csv(df, "../mdle_data/out/merged_dataset.csv", row.names = FALSE)
