24/04/13 12:32:09.292 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:32:09.495 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 12:32:09.517 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 12:32:09.582 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 12:32:09.671 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:32:09.671 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 12:32:09.671 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:32:09.672 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 12:32:09.698 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 12:32:09.709 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 12:32:09.710 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 12:32:09.765 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 12:32:09.766 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 12:32:09.766 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 12:32:09.766 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 12:32:09.767 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 12:32:09.853 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51051.
24/04/13 12:32:09.885 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 12:32:09.922 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 12:32:09.948 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 12:32:09.949 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 12:32:09.952 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 12:32:09.979 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-d4d5d751-7814-4231-937a-ea20ad54e209
24/04/13 12:32:10.007 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 12:32:10.031 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 12:32:10.035 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 12:32:10.259 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 12:32:10.375 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 12:32:10.421 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51051/jars/sparklyr-3.0-2.12.jar with timestamp 1713007929487
24/04/13 12:32:10.497 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 12:32:10.504 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 12:32:10.516 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51051/jars/sparklyr-3.0-2.12.jar with timestamp 1713007929487
24/04/13 12:32:10.556 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51051 after 19 ms (0 ms spent in bootstraps)
24/04/13 12:32:10.564 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51051/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-101650f1-582e-4fdd-bf19-08341d5f8c0a\userFiles-ab342ec5-0b95-4a76-8390-7e95f99812d1\fetchFileTemp5692991556329080863.tmp
24/04/13 12:32:11.039 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-101650f1-582e-4fdd-bf19-08341d5f8c0a/userFiles-ab342ec5-0b95-4a76-8390-7e95f99812d1/sparklyr-3.0-2.12.jar to class loader
24/04/13 12:32:11.051 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51071.
24/04/13 12:32:11.051 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51071
24/04/13 12:32:11.053 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 12:32:11.064 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51071, None)
24/04/13 12:32:11.069 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51071 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51071, None)
24/04/13 12:32:11.077 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51071, None)
24/04/13 12:32:11.079 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51071, None)
24/04/13 12:32:11.382 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 12:32:11.390 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 12:32:15.041 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 12:32:15.170 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:32:15.512 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 12:32:15.684 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 12:32:15.684 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 12:32:15.685 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 12:32:15.728 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 12:32:15.874 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 12:32:15.875 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 12:32:16.890 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 12:32:18.225 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 12:32:18.228 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 12:32:18.295 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 12:32:18.295 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 12:32:18.323 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 12:32:18.448 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 12:32:18.449 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 12:32:18.494 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 12:32:18.605 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:32:18.607 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:32:18.625 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 12:32:18.625 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 12:32:18.626 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 12:32:18.626 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:32:18.626 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:32:18.628 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:32:18.628 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:32:18.630 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:32:18.630 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:37:36.480 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 12:37:36.480 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 12:37:36.497 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 12:37:36.523 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 12:37:36.536 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 12:37:36.536 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 12:37:36.545 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 12:37:36.548 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 12:37:36.554 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 12:37:36.554 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 12:37:36.554 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-38e02ea2-3a9c-4564-89db-a159bdf738d6
24/04/13 12:37:36.556 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-101650f1-582e-4fdd-bf19-08341d5f8c0a
24/04/13 12:37:48.538 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:37:48.725 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 12:37:48.744 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 12:37:48.792 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 12:37:48.876 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:37:48.876 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 12:37:48.877 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:37:48.877 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 12:37:48.903 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 12:37:48.914 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 12:37:48.915 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 12:37:48.963 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 12:37:48.963 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 12:37:48.964 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 12:37:48.964 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 12:37:48.964 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 12:37:49.050 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51172.
24/04/13 12:37:49.081 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 12:37:49.115 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 12:37:49.138 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 12:37:49.138 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 12:37:49.143 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 12:37:49.175 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-357d3199-7dc5-4b88-9dc8-93543692c4ef
24/04/13 12:37:49.204 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 12:37:49.232 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 12:37:49.238 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 12:37:49.485 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 12:37:49.624 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 12:37:49.702 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51172/jars/sparklyr-3.0-2.12.jar with timestamp 1713008268714
24/04/13 12:37:49.865 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 12:37:49.875 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 12:37:49.894 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51172/jars/sparklyr-3.0-2.12.jar with timestamp 1713008268714
24/04/13 12:37:49.969 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51172 after 33 ms (0 ms spent in bootstraps)
24/04/13 12:37:49.979 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51172/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-e2e3c7da-2871-4979-9b1b-383a58572873\userFiles-c6cd7b0d-dac8-483f-85fc-ee58ec78897d\fetchFileTemp6945004696946724388.tmp
24/04/13 12:37:50.245 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-e2e3c7da-2871-4979-9b1b-383a58572873/userFiles-c6cd7b0d-dac8-483f-85fc-ee58ec78897d/sparklyr-3.0-2.12.jar to class loader
24/04/13 12:37:50.264 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51192.
24/04/13 12:37:50.264 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51192
24/04/13 12:37:50.267 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 12:37:50.279 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51192, None)
24/04/13 12:37:50.284 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51192 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51192, None)
24/04/13 12:37:50.288 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51192, None)
24/04/13 12:37:50.290 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51192, None)
24/04/13 12:37:50.613 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 12:37:50.623 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 12:37:53.811 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 12:37:53.965 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:37:54.153 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 12:37:54.358 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 12:37:54.359 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 12:37:54.359 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 12:37:54.398 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 12:37:54.521 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 12:37:54.522 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 12:37:55.484 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 12:37:57.038 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 12:37:57.040 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 12:37:57.106 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 12:37:57.106 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 12:37:57.134 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 12:37:57.253 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 12:37:57.254 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 12:37:57.291 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 12:37:57.373 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:37:57.376 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:37:57.398 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 12:37:57.398 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 12:37:57.399 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 12:37:57.401 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:37:57.401 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:37:57.404 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:37:57.404 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:37:57.407 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:37:57.407 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:42:06.191 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 12:42:06.194 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 12:42:06.217 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 12:42:06.243 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 12:42:06.263 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 12:42:06.263 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 12:42:06.276 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 12:42:06.280 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 12:42:06.285 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 12:42:06.286 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 12:42:06.287 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-a521caff-e2ca-4e92-980e-bfd38d9c0925
24/04/13 12:42:06.288 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-e2e3c7da-2871-4979-9b1b-383a58572873
24/04/13 12:42:18.253 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:42:18.482 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 12:42:18.508 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 12:42:18.574 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 12:42:18.658 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:42:18.659 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 12:42:18.659 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:42:18.660 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 12:42:18.692 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 12:42:18.705 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 12:42:18.706 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 12:42:18.775 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 12:42:18.776 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 12:42:18.776 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 12:42:18.776 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 12:42:18.777 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 12:42:18.898 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51419.
24/04/13 12:42:18.931 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 12:42:18.979 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 12:42:19.014 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 12:42:19.015 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 12:42:19.019 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 12:42:19.054 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-dbcb9f7a-b8a0-44f2-811b-08001b4bb1e0
24/04/13 12:42:19.075 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 12:42:19.096 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 12:42:19.100 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 12:42:19.273 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 12:42:19.363 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 12:42:19.415 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51419/jars/sparklyr-3.0-2.12.jar with timestamp 1713008538475
24/04/13 12:42:19.500 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 12:42:19.508 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 12:42:19.522 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51419/jars/sparklyr-3.0-2.12.jar with timestamp 1713008538475
24/04/13 12:42:19.575 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51419 after 24 ms (0 ms spent in bootstraps)
24/04/13 12:42:19.580 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51419/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-301e41de-1f6a-4634-811c-dfa73a0ce538\userFiles-079ff47f-098b-4f26-80e5-a0c4b4bd0a42\fetchFileTemp4237697913008202037.tmp
24/04/13 12:42:19.749 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-301e41de-1f6a-4634-811c-dfa73a0ce538/userFiles-079ff47f-098b-4f26-80e5-a0c4b4bd0a42/sparklyr-3.0-2.12.jar to class loader
24/04/13 12:42:19.770 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51439.
24/04/13 12:42:19.771 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51439
24/04/13 12:42:19.773 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 12:42:19.781 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51439, None)
24/04/13 12:42:19.784 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51439 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51439, None)
24/04/13 12:42:19.787 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51439, None)
24/04/13 12:42:19.788 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51439, None)
24/04/13 12:42:20.052 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 12:42:20.059 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 12:42:23.005 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 12:42:23.149 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:42:23.402 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 12:42:23.533 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 12:42:23.534 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 12:42:23.534 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 12:42:23.573 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 12:42:23.694 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 12:42:23.694 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 12:42:24.684 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 12:42:26.064 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 12:42:26.066 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 12:42:26.119 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 12:42:26.120 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 12:42:26.142 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 12:42:26.261 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 12:42:26.263 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 12:42:26.300 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 12:42:26.379 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:42:26.381 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:42:26.400 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 12:42:26.400 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 12:42:26.401 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 12:42:26.402 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:42:26.402 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:42:26.403 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:42:26.403 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:42:26.405 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:42:26.405 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:43:33.626 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 12:43:33.627 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 12:43:33.649 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 12:43:33.678 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 12:43:33.688 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 12:43:33.689 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 12:43:33.730 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 12:43:33.734 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 12:43:33.744 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 12:43:33.745 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 12:43:33.745 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-301e41de-1f6a-4634-811c-dfa73a0ce538
24/04/13 12:43:33.747 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-9a7c49fe-e1b1-40e0-97d9-1b11a50d4515
24/04/13 12:43:45.917 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:43:46.102 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 12:43:46.121 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 12:43:46.172 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 12:43:46.246 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:43:46.246 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 12:43:46.246 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:43:46.247 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 12:43:46.273 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 12:43:46.286 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 12:43:46.287 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 12:43:46.339 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 12:43:46.339 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 12:43:46.339 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 12:43:46.340 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 12:43:46.340 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 12:43:46.432 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51522.
24/04/13 12:43:46.458 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 12:43:46.494 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 12:43:46.521 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 12:43:46.521 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 12:43:46.524 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 12:43:46.548 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-ac81dfb4-df4d-4a2d-9489-72e6bbfff7dc
24/04/13 12:43:46.565 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 12:43:46.579 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 12:43:46.582 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 12:43:46.726 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 12:43:46.801 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 12:43:46.847 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51522/jars/sparklyr-3.0-2.12.jar with timestamp 1713008626097
24/04/13 12:43:46.923 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 12:43:46.929 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 12:43:46.940 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51522/jars/sparklyr-3.0-2.12.jar with timestamp 1713008626097
24/04/13 12:43:46.980 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51522 after 18 ms (0 ms spent in bootstraps)
24/04/13 12:43:46.984 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51522/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-56aa03a2-fb01-4ac1-b7a0-c323613dc273\userFiles-25cf282e-6753-4e89-a32f-cae39f790783\fetchFileTemp1328904515856139121.tmp
24/04/13 12:43:47.151 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-56aa03a2-fb01-4ac1-b7a0-c323613dc273/userFiles-25cf282e-6753-4e89-a32f-cae39f790783/sparklyr-3.0-2.12.jar to class loader
24/04/13 12:43:47.164 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51544.
24/04/13 12:43:47.164 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51544
24/04/13 12:43:47.166 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 12:43:47.174 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51544, None)
24/04/13 12:43:47.177 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51544 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51544, None)
24/04/13 12:43:47.180 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51544, None)
24/04/13 12:43:47.181 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51544, None)
24/04/13 12:43:47.505 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 12:43:47.515 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 12:43:50.723 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 12:43:50.847 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:43:51.044 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 12:43:51.282 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 12:43:51.283 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 12:43:51.284 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 12:43:51.336 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 12:43:51.482 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 12:43:51.483 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 12:43:52.512 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 12:43:54.045 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 12:43:54.048 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 12:43:54.137 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 12:43:54.138 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 12:43:54.168 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 12:43:54.333 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 12:43:54.336 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 12:43:54.390 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 12:43:54.493 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:43:54.496 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:43:54.515 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 12:43:54.515 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 12:43:54.516 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 12:43:54.517 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:43:54.517 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:43:54.519 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:43:54.520 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:43:54.523 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:43:54.523 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:43:55.749 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 12:43:55.750 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 12:43:55.771 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 12:43:55.798 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 12:43:55.810 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 12:43:55.810 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 12:43:55.825 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 12:43:55.829 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 12:43:55.838 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 12:43:55.839 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 12:43:55.839 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-56aa03a2-fb01-4ac1-b7a0-c323613dc273
24/04/13 12:43:55.841 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-a04a070e-d1e6-47b1-8a95-69505d659bda
24/04/13 12:44:08.078 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:44:08.266 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 12:44:08.284 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 12:44:08.338 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 12:44:08.440 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:44:08.441 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 12:44:08.441 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 12:44:08.442 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 12:44:08.471 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 12:44:08.483 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 12:44:08.483 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 12:44:08.535 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 12:44:08.535 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 12:44:08.536 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 12:44:08.536 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 12:44:08.536 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 12:44:08.636 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51598.
24/04/13 12:44:08.664 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 12:44:08.700 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 12:44:08.723 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 12:44:08.723 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 12:44:08.726 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 12:44:08.750 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-7ca1aa6a-1c7d-4f43-89c1-e3a8cfe0dc52
24/04/13 12:44:08.767 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 12:44:08.781 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 12:44:08.784 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 12:44:08.941 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 12:44:09.041 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 12:44:09.095 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51598/jars/sparklyr-3.0-2.12.jar with timestamp 1713008648260
24/04/13 12:44:09.188 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 12:44:09.197 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 12:44:09.211 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51598/jars/sparklyr-3.0-2.12.jar with timestamp 1713008648260
24/04/13 12:44:09.269 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51598 after 27 ms (0 ms spent in bootstraps)
24/04/13 12:44:09.275 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51598/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae\fetchFileTemp2189255524959162469.tmp
24/04/13 12:44:09.432 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653/userFiles-24e5bf15-daff-402c-9931-355a3aad76ae/sparklyr-3.0-2.12.jar to class loader
24/04/13 12:44:09.444 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51619.
24/04/13 12:44:09.445 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51619
24/04/13 12:44:09.446 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 12:44:09.463 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51619, None)
24/04/13 12:44:09.467 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51619 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51619, None)
24/04/13 12:44:09.469 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51619, None)
24/04/13 12:44:09.470 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51619, None)
24/04/13 12:44:09.725 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 12:44:09.733 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 12:44:14.341 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 12:44:14.484 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 12:44:14.759 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 12:44:14.891 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 12:44:14.892 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 12:44:14.892 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 12:44:14.931 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 12:44:15.052 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 12:44:15.054 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 12:44:15.945 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 12:44:17.228 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 12:44:17.229 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 12:44:17.285 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 12:44:17.286 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 12:44:17.308 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 12:44:17.458 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 12:44:17.460 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 12:44:17.504 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 12:44:17.606 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:44:17.609 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:44:17.628 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 12:44:17.628 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 12:44:17.629 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 12:44:17.630 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:44:17.630 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:44:17.632 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:44:17.632 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:44:17.635 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:44:17.635 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:44:18.521 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 224.2571 ms
24/04/13 12:44:18.683 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 12:44:18.691 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,006993 s
24/04/13 12:44:23.212 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 12:44:25.222 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.873 ms
24/04/13 12:44:25.273 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 12:44:25.291 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 12:44:25.292 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 12:44:25.292 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 12:44:25.293 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 12:44:25.297 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 12:44:25.379 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 12:44:25.440 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 12:44:25.443 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:51619 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 12:44:25.448 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 12:44:25.465 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 12:44:25.467 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 12:44:25.528 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 12:44:25.542 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 12:44:26.210 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 241.7076 ms
24/04/13 12:44:26.245 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/13 12:44:26.258 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 743 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 12:44:26.262 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 12:44:26.271 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,954 s
24/04/13 12:44:26.276 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 12:44:26.276 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 12:44:26.277 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 1,003541 s
24/04/13 12:44:26.615 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 209.1345 ms
24/04/13 12:44:30.618 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 12:44:30.619 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 12:44:30.619 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 12:44:30.619 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 12:44:30.619 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 12:44:30.621 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 12:44:30.627 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 12:44:30.629 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 12:44:30.630 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:51619 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 12:44:30.630 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 12:44:30.631 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 12:44:30.631 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 12:44:30.632 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 12:44:30.633 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 12:44:30.689 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/13 12:44:30.691 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 12:44:30.692 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 12:44:30.692 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,070 s
24/04/13 12:44:30.692 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 12:44:30.692 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 12:44:30.693 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,074967 s
24/04/13 12:44:30.941 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:51619 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 12:44:30.953 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:51619 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 12:44:33.351 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:44:33.352 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:44:33.365 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 12:44:33.365 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 12:44:33.367 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 12:44:33.367 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 12:44:33.450 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 41.7413 ms
24/04/13 12:44:33.485 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.4307 ms
24/04/13 12:44:33.501 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.436 ms
24/04/13 14:12:41.076 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.util.Failure.recover(Try.scala:234)
	at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.tryFailure(Promise.scala:112)
	at scala.concurrent.Promise.tryFailure$(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)
	... 7 more
24/04/13 14:12:51.090 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.util.Failure.recover(Try.scala:234)
	at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.tryFailure(Promise.scala:112)
	at scala.concurrent.Promise.tryFailure$(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)
	... 7 more
24/04/13 14:13:01.096 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
24/04/13 14:13:11.099 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at scala.util.Failure.recover(Try.scala:234)
	at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.complete(Promise.scala:53)
	at scala.concurrent.Promise.complete$(Promise.scala:52)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
	at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
	at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
	at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
	at scala.concurrent.Promise.tryFailure(Promise.scala:112)
	at scala.concurrent.Promise.tryFailure$(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from DESKTOP-LH06ASP:51598 in 10000 milliseconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)
	... 7 more
24/04/13 14:13:19.856 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/13 14:13:19.861 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/13 14:13:19.862 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/13 14:13:19.863 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/13 14:14:52.421 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:14:52.424 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:14:52.424 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/13 14:14:52.425 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:14:52.429 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:14:52.430 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26), which has no missing parents
24/04/13 14:14:52.458 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 107.3 KiB, free 912.2 MiB)
24/04/13 14:14:52.468 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 912.2 MiB)
24/04/13 14:14:52.469 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:51619 (size: 27.1 KiB, free: 912.3 MiB)
24/04/13 14:14:52.469 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/13 14:14:52.470 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:14:52.470 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/13 14:14:52.503 dispatcher-event-loop-0 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/13 14:14:52.504 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/13 14:14:52.506 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/13 14:14:52.788 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 168.3469 ms
24/04/13 14:14:53.233 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_13_0 stored as values in memory (estimated size 686.4 KiB, free 911.5 MiB)
24/04/13 14:14:53.234 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_13_0 in memory on DESKTOP-LH06ASP:51619 (size: 686.4 KiB, free: 911.6 MiB)
24/04/13 14:14:53.245 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 3.5557 ms
24/04/13 14:14:53.885 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 626.1107 ms
24/04/13 14:14:53.922 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 2)
[rdd_13_0]
24/04/13 14:14:53.930 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2528 bytes result sent to driver
24/04/13 14:14:53.931 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1459 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:14:53.932 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/13 14:14:53.932 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 1,500 s
24/04/13 14:14:53.933 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:14:53.933 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/13 14:14:53.933 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 1,510912 s
24/04/13 14:14:54.227 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 181.8932 ms
24/04/13 14:15:09.598 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:15:09.599 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:15:09.625 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:15:09.657 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:15:09.698 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:15:09.698 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:15:09.702 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:15:09.707 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:15:09.713 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:15:09.715 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:15:09.715 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:15:09.716 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-8aa966d2-9fcc-48ba-b4b1-2864df897098
24/04/13 14:15:09.718 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae
24/04/13 14:15:09.721 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:15:09.721 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653
24/04/13 14:15:09.726 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-36e10d67-cbe0-4a98-a7fe-dfc6296f2653\userFiles-24e5bf15-daff-402c-9931-355a3aad76ae\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:15:22.029 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:15:22.304 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 14:15:22.342 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 14:15:22.440 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 14:15:22.559 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:15:22.560 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 14:15:22.560 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:15:22.561 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 14:15:22.591 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 14:15:22.603 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 14:15:22.604 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 14:15:22.666 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 14:15:22.666 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 14:15:22.666 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 14:15:22.667 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 14:15:22.668 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 14:15:22.785 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 57601.
24/04/13 14:15:22.832 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 14:15:22.886 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 14:15:22.926 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 14:15:22.927 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 14:15:22.931 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 14:15:22.963 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-33b4db5a-1040-420f-8747-37e8da2e9d1c
24/04/13 14:15:22.990 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 14:15:23.013 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 14:15:23.016 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 14:15:23.297 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 14:15:23.393 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 14:15:23.450 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:57601/jars/sparklyr-3.0-2.12.jar with timestamp 1713014122293
24/04/13 14:15:23.563 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 14:15:23.570 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 14:15:23.582 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:57601/jars/sparklyr-3.0-2.12.jar with timestamp 1713014122293
24/04/13 14:15:23.635 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:57601 after 24 ms (0 ms spent in bootstraps)
24/04/13 14:15:23.640 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:57601/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf\fetchFileTemp2283290484549399743.tmp
24/04/13 14:15:23.816 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667/userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf/sparklyr-3.0-2.12.jar to class loader
24/04/13 14:15:23.833 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57621.
24/04/13 14:15:23.833 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:57621
24/04/13 14:15:23.835 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 14:15:23.844 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 57621, None)
24/04/13 14:15:23.849 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:57621 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 57621, None)
24/04/13 14:15:23.854 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 57621, None)
24/04/13 14:15:23.857 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 57621, None)
24/04/13 14:15:24.539 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 14:15:24.555 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 14:15:32.490 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 14:15:32.645 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:15:32.938 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 14:15:33.276 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 14:15:33.277 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 14:15:33.277 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 14:15:33.333 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 14:15:33.493 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 14:15:33.494 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 14:15:34.955 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 14:15:37.313 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 14:15:37.318 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 14:15:37.406 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 14:15:37.406 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 14:15:37.455 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 14:15:37.784 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 14:15:37.787 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 14:15:37.870 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 14:15:38.078 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:15:38.082 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:15:38.119 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 14:15:38.120 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 14:15:38.121 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 14:15:38.123 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:15:38.123 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:15:38.126 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:15:38.126 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:15:38.131 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:15:38.133 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:15:39.189 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 222.3137 ms
24/04/13 14:15:39.330 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:15:39.336 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005068 s
24/04/13 14:15:47.170 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 14:15:49.703 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 18.6367 ms
24/04/13 14:15:49.815 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:15:49.847 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:15:49.848 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 14:15:49.848 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:15:49.850 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:15:49.856 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 14:15:50.082 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:15:50.219 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:15:50.227 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:57621 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:15:50.240 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 14:15:50.277 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:15:50.281 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 14:15:50.381 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:15:50.405 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 14:15:51.619 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 611.1015 ms
24/04/13 14:15:51.716 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1516 bytes result sent to driver
24/04/13 14:15:51.736 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1375 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:15:51.744 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 14:15:51.765 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 1,869 s
24/04/13 14:15:51.773 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:15:51.778 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 14:15:51.781 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 1,963921 s
24/04/13 14:15:52.862 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:57621 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:15:52.980 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 990.9383 ms
24/04/13 14:16:01.163 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:16:01.167 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:16:01.168 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 14:16:01.169 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:16:01.169 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:16:01.175 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 14:16:01.183 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:16:01.191 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:16:01.192 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:57621 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:16:01.193 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 14:16:01.194 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:16:01.194 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 14:16:01.197 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:16:01.199 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 14:16:01.435 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1473 bytes result sent to driver
24/04/13 14:16:01.443 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 245 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:16:01.447 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 14:16:01.448 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,271 s
24/04/13 14:16:01.449 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:16:01.449 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 14:16:01.450 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,286069 s
24/04/13 14:16:05.474 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:16:05.474 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:16:05.492 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:16:05.492 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:16:05.494 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:16:05.494 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:16:05.662 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 84.5195 ms
24/04/13 14:16:05.730 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 15.1939 ms
24/04/13 14:16:05.761 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 11.5047 ms
24/04/13 14:20:27.837 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:20:27.839 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:20:28.041 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:20:28.070 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:20:28.090 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:20:28.091 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:20:28.096 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:20:28.101 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:20:28.107 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:20:28.112 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:20:28.113 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:20:28.114 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf
24/04/13 14:20:28.118 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:20:28.118 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-4b3085ef-925b-41df-b035-3288bf1da35f
24/04/13 14:20:28.120 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667
24/04/13 14:20:28.123 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1c94725b-6e57-4d57-9372-2bdcd0a1a667\userFiles-e212d81b-efaa-4a4c-a11e-c5843b8297bf\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:20:40.358 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:20:40.586 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 14:20:40.613 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 14:20:40.683 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 14:20:40.769 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:20:40.770 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 14:20:40.770 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:20:40.771 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 14:20:40.809 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 14:20:40.823 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 14:20:40.824 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 14:20:40.902 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 14:20:40.903 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 14:20:40.903 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 14:20:40.904 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 14:20:40.905 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 14:20:41.035 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 58257.
24/04/13 14:20:41.068 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 14:20:41.113 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 14:20:41.150 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 14:20:41.150 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 14:20:41.155 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 14:20:41.187 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-e3c4c18e-befb-46aa-bf77-a440ac20fd50
24/04/13 14:20:41.211 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 14:20:41.229 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 14:20:41.232 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 14:20:41.429 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 14:20:41.523 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 14:20:41.567 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:58257/jars/sparklyr-3.0-2.12.jar with timestamp 1713014440577
24/04/13 14:20:41.652 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 14:20:41.660 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 14:20:41.671 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:58257/jars/sparklyr-3.0-2.12.jar with timestamp 1713014440577
24/04/13 14:20:41.711 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:58257 after 19 ms (0 ms spent in bootstraps)
24/04/13 14:20:41.716 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:58257/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697\fetchFileTemp3567781967067454299.tmp
24/04/13 14:20:41.875 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-47c64739-2b1e-4318-8c06-584f604e396a/userFiles-19bebceb-a721-4b76-a529-70a10efd1697/sparklyr-3.0-2.12.jar to class loader
24/04/13 14:20:41.887 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58277.
24/04/13 14:20:41.887 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:58277
24/04/13 14:20:41.889 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 14:20:41.897 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 58277, None)
24/04/13 14:20:41.902 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:58277 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 58277, None)
24/04/13 14:20:41.905 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 58277, None)
24/04/13 14:20:41.906 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 58277, None)
24/04/13 14:20:42.172 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 14:20:42.180 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 14:20:46.381 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 14:20:46.511 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:20:46.801 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 14:20:47.079 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 14:20:47.079 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 14:20:47.079 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 14:20:47.124 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 14:20:47.286 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 14:20:47.287 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 14:20:48.429 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 14:20:49.739 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 14:20:49.741 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 14:20:49.799 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 14:20:49.799 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 14:20:49.826 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 14:20:49.958 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 14:20:49.960 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 14:20:49.997 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 14:20:50.102 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:20:50.104 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:20:50.122 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 14:20:50.122 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 14:20:50.123 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 14:20:50.124 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:20:50.124 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:20:50.126 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:20:50.126 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:20:50.128 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:20:50.128 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:20:50.824 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 174.9306 ms
24/04/13 14:20:50.942 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:20:50.949 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005175 s
24/04/13 14:20:55.379 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 14:20:57.040 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.3417 ms
24/04/13 14:20:57.126 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:20:57.149 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:20:57.150 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 14:20:57.151 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:20:57.153 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:20:57.158 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 14:20:57.266 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:20:57.341 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:20:57.345 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:58277 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:20:57.349 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 14:20:57.369 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:20:57.371 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 14:20:57.451 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:20:57.468 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 14:20:58.420 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 400.6442 ms
24/04/13 14:20:58.479 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/13 14:20:58.526 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1088 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:20:58.530 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 14:20:58.538 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 1,354 s
24/04/13 14:20:58.548 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:20:58.549 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 14:20:58.552 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 1,424348 s
24/04/13 14:20:59.101 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 335.1959 ms
24/04/13 14:21:02.901 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:21:02.902 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:21:02.902 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 14:21:02.902 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:21:02.902 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:21:02.903 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 14:21:02.910 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:21:02.912 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:21:02.913 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:58277 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:21:02.913 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 14:21:02.914 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:21:02.915 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 14:21:02.916 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:21:02.916 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 14:21:02.961 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/13 14:21:02.964 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 49 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:21:02.964 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 14:21:02.965 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,060 s
24/04/13 14:21:02.965 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:21:02.965 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 14:21:02.966 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,064538 s
24/04/13 14:21:03.069 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:58277 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:21:05.909 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:21:05.910 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:21:05.911 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/13 14:21:05.911 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:21:05.914 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:21:05.915 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26), which has no missing parents
24/04/13 14:21:05.944 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 107.3 KiB, free 912.1 MiB)
24/04/13 14:21:05.947 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 912.1 MiB)
24/04/13 14:21:05.948 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:58277 (size: 27.1 KiB, free: 912.3 MiB)
24/04/13 14:21:05.949 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/13 14:21:05.949 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:21:05.949 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/13 14:21:05.966 dispatcher-event-loop-1 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/13 14:21:05.966 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/13 14:21:05.967 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/13 14:21:06.223 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 157.0672 ms
24/04/13 14:21:06.961 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_13_0 stored as values in memory (estimated size 686.4 KiB, free 911.4 MiB)
24/04/13 14:21:06.966 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_13_0 in memory on DESKTOP-LH06ASP:58277 (size: 686.4 KiB, free: 911.6 MiB)
24/04/13 14:21:06.981 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 6.059 ms
24/04/13 14:21:07.191 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 189.5375 ms
24/04/13 14:21:07.231 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 2)
[rdd_13_0]
24/04/13 14:21:07.233 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2399 bytes result sent to driver
24/04/13 14:21:07.235 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1285 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:21:07.235 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/13 14:21:07.236 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 1,319 s
24/04/13 14:21:07.236 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:21:07.237 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/13 14:21:07.237 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 1,327206 s
24/04/13 14:21:07.353 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_2_piece0 on DESKTOP-LH06ASP:58277 in memory (size: 27.1 KiB, free: 911.6 MiB)
24/04/13 14:21:07.639 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 239.0039 ms
24/04/13 14:21:10.012 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:21:10.013 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:21:10.015 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:21:10.015 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:21:10.018 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:21:10.019 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:21:10.073 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 30.0697 ms
24/04/13 14:21:10.108 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.1103 ms
24/04/13 14:21:10.123 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.7561 ms
24/04/13 14:21:40.861 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:21:40.862 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:21:40.881 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:21:40.903 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:21:40.938 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:21:40.938 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:21:40.941 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:21:40.944 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:21:40.948 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:21:40.950 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:21:40.951 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:21:40.952 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697
24/04/13 14:21:40.955 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:21:40.955 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-9ea739dc-8aeb-4057-a69e-b69af2f2493e
24/04/13 14:21:40.957 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a
24/04/13 14:21:40.962 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-47c64739-2b1e-4318-8c06-584f604e396a\userFiles-19bebceb-a721-4b76-a529-70a10efd1697\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:21:53.487 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:21:53.729 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 14:21:53.757 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 14:21:53.821 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 14:21:53.905 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:21:53.906 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 14:21:53.906 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:21:53.907 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 14:21:53.937 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 14:21:53.951 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 14:21:53.952 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 14:21:54.019 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 14:21:54.020 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 14:21:54.020 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 14:21:54.021 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 14:21:54.021 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 14:21:54.148 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 58431.
24/04/13 14:21:54.179 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 14:21:54.225 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 14:21:54.259 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 14:21:54.260 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 14:21:54.265 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 14:21:54.296 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-4c394c6f-f79f-47d6-878b-02bdbd39422e
24/04/13 14:21:54.318 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 14:21:54.334 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 14:21:54.338 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 14:21:54.523 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 14:21:54.678 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 14:21:54.753 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:58431/jars/sparklyr-3.0-2.12.jar with timestamp 1713014513720
24/04/13 14:21:54.827 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 14:21:54.834 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 14:21:54.845 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:58431/jars/sparklyr-3.0-2.12.jar with timestamp 1713014513720
24/04/13 14:21:54.884 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:58431 after 18 ms (0 ms spent in bootstraps)
24/04/13 14:21:54.888 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:58431/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3\fetchFileTemp2068621777965906065.tmp
24/04/13 14:21:55.078 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-2a7278c0-6434-4837-af8f-58d8a3a155cc/userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3/sparklyr-3.0-2.12.jar to class loader
24/04/13 14:21:55.148 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58456.
24/04/13 14:21:55.149 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:58456
24/04/13 14:21:55.158 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 14:21:55.189 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 58456, None)
24/04/13 14:21:55.200 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:58456 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 58456, None)
24/04/13 14:21:55.214 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 58456, None)
24/04/13 14:21:55.217 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 58456, None)
24/04/13 14:21:55.753 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 14:21:55.763 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 14:22:01.126 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 14:22:01.242 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:22:01.502 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 14:22:01.645 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 14:22:01.645 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 14:22:01.646 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 14:22:01.684 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 14:22:01.806 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 14:22:01.807 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 14:22:02.700 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 14:22:04.077 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 14:22:04.079 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 14:22:04.134 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 14:22:04.134 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 14:22:04.157 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 14:22:04.277 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 14:22:04.279 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 14:22:04.317 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 14:22:04.422 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:22:04.424 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:22:04.440 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 14:22:04.440 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 14:22:04.441 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 14:22:04.442 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:22:04.442 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:22:04.444 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:22:04.444 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:22:04.446 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:22:04.446 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:22:05.623 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 345.7755 ms
24/04/13 14:22:05.833 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:22:05.843 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,008602 s
24/04/13 14:22:10.421 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 14:22:12.263 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 17.7554 ms
24/04/13 14:22:12.365 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:22:12.387 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:22:12.388 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 14:22:12.388 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:22:12.390 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:22:12.394 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 14:22:12.482 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:22:12.560 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:22:12.564 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:58456 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:22:12.569 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 14:22:12.590 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:22:12.591 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 14:22:12.653 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:22:12.671 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 14:22:13.311 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 197.8355 ms
24/04/13 14:22:13.332 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1516 bytes result sent to driver
24/04/13 14:22:13.341 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 699 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:22:13.343 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 14:22:13.350 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,936 s
24/04/13 14:22:13.354 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:22:13.354 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 14:22:13.355 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,989051 s
24/04/13 14:22:13.520 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:58456 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:22:13.977 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 446.1016 ms
24/04/13 14:22:18.933 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:22:18.934 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:22:18.934 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 14:22:18.934 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:22:18.934 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:22:18.935 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 14:22:18.941 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:22:18.943 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:22:18.944 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:58456 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:22:18.944 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 14:22:18.945 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:22:18.945 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 14:22:18.946 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:22:18.947 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 14:22:19.014 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1473 bytes result sent to driver
24/04/13 14:22:19.016 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:22:19.016 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 14:22:19.017 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,081 s
24/04/13 14:22:19.018 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:22:19.018 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 14:22:19.018 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,085411 s
24/04/13 14:22:19.074 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:58456 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:22:22.177 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:22:22.179 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:22:22.180 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/13 14:22:22.180 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:22:22.183 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:22:22.185 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26), which has no missing parents
24/04/13 14:22:22.209 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 107.3 KiB, free 912.2 MiB)
24/04/13 14:22:22.211 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 912.2 MiB)
24/04/13 14:22:22.212 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:58456 (size: 27.1 KiB, free: 912.3 MiB)
24/04/13 14:22:22.213 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/13 14:22:22.214 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:22:22.214 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/13 14:22:22.235 dispatcher-event-loop-0 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/13 14:22:22.235 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/13 14:22:22.236 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/13 14:22:22.441 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 118.6783 ms
24/04/13 14:22:23.338 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_13_0 stored as values in memory (estimated size 686.4 KiB, free 911.5 MiB)
24/04/13 14:22:23.370 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_13_0 in memory on DESKTOP-LH06ASP:58456 (size: 686.4 KiB, free: 911.6 MiB)
24/04/13 14:22:23.386 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 9.2981 ms
24/04/13 14:22:23.699 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 294.801 ms
24/04/13 14:22:23.745 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 2)
[rdd_13_0]
24/04/13 14:22:23.747 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2442 bytes result sent to driver
24/04/13 14:22:23.751 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1535 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:22:23.751 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/13 14:22:23.753 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 1,567 s
24/04/13 14:22:23.755 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:22:23.756 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/13 14:22:23.757 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 1,578888 s
24/04/13 14:22:24.147 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 260.5934 ms
24/04/13 14:22:27.800 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:22:27.802 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:22:27.802 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:26)
24/04/13 14:22:27.803 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:22:27.806 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:22:27.807 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at collect at utils.scala:26), which has no missing parents
24/04/13 14:22:27.816 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 107.3 KiB, free 911.4 MiB)
24/04/13 14:22:27.819 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 911.4 MiB)
24/04/13 14:22:27.820 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-LH06ASP:58456 (size: 27.1 KiB, free: 911.6 MiB)
24/04/13 14:22:27.821 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
24/04/13 14:22:27.821 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:22:27.822 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/04/13 14:22:27.837 dispatcher-event-loop-1 WARN TaskSetManager: Stage 3 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/13 14:22:27.837 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/13 14:22:27.838 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/04/13 14:22:27.847 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO BlockManager: Found block rdd_13_0 locally
24/04/13 14:22:27.855 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: 1 block locks were not released by task 0.0 in stage 3.0 (TID 3)
[rdd_13_0]
24/04/13 14:22:27.856 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2485 bytes result sent to driver
24/04/13 14:22:27.858 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 32 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:22:27.859 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/04/13 14:22:27.859 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at utils.scala:26) finished in 0,051 s
24/04/13 14:22:27.860 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:22:27.860 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/04/13 14:22:27.861 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 4 finished: collect at utils.scala:26, took 0,059686 s
24/04/13 14:22:30.303 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:22:30.304 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:22:30.305 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:22:30.306 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:22:30.307 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:22:30.307 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:22:30.370 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 32.1211 ms
24/04/13 14:22:30.410 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.4084 ms
24/04/13 14:22:30.422 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.5806 ms
24/04/13 14:29:35.212 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:29:35.212 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:29:35.244 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:29:35.263 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:29:35.297 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:29:35.298 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:29:35.300 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:29:35.305 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:29:35.309 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:29:35.311 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:29:35.311 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:29:35.311 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-0149c649-a058-48cd-a0dc-ba20ae1d60c4
24/04/13 14:29:35.313 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc
24/04/13 14:29:35.316 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:29:35.317 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3
24/04/13 14:29:35.319 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-2a7278c0-6434-4837-af8f-58d8a3a155cc\userFiles-a0a161f4-7fa4-444d-930c-a87a3ace98c3\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:29:47.411 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:29:47.628 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 14:29:47.655 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 14:29:47.716 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 14:29:47.801 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:29:47.802 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 14:29:47.802 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:29:47.803 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 14:29:47.829 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 14:29:47.842 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 14:29:47.842 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 14:29:47.901 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 14:29:47.902 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 14:29:47.902 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 14:29:47.903 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 14:29:47.903 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 14:29:48.058 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 59009.
24/04/13 14:29:48.092 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 14:29:48.129 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 14:29:48.152 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 14:29:48.153 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 14:29:48.155 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 14:29:48.178 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-94bfd556-fda5-4031-bda5-72adcb55a48f
24/04/13 14:29:48.196 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 14:29:48.211 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 14:29:48.214 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 14:29:48.356 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 14:29:48.505 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 14:29:48.575 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:59009/jars/sparklyr-3.0-2.12.jar with timestamp 1713014987622
24/04/13 14:29:48.682 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 14:29:48.689 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 14:29:48.703 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:59009/jars/sparklyr-3.0-2.12.jar with timestamp 1713014987622
24/04/13 14:29:48.768 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:59009 after 37 ms (0 ms spent in bootstraps)
24/04/13 14:29:48.778 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:59009/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57\fetchFileTemp7782732309141153033.tmp
24/04/13 14:29:48.948 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2/userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57/sparklyr-3.0-2.12.jar to class loader
24/04/13 14:29:48.960 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59031.
24/04/13 14:29:48.960 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:59031
24/04/13 14:29:48.962 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 14:29:48.970 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59031, None)
24/04/13 14:29:48.973 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:59031 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 59031, None)
24/04/13 14:29:48.976 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59031, None)
24/04/13 14:29:48.977 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 59031, None)
24/04/13 14:29:49.294 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 14:29:49.303 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 14:29:53.787 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 14:29:53.920 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:29:54.206 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 14:29:54.338 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 14:29:54.339 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 14:29:54.339 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 14:29:54.380 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 14:29:54.510 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 14:29:54.512 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 14:29:55.543 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 14:29:56.986 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 14:29:56.988 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 14:29:57.039 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 14:29:57.039 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 14:29:57.060 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 14:29:57.184 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 14:29:57.185 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 14:29:57.224 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 14:29:57.303 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:29:57.305 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:29:57.326 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 14:29:57.327 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 14:29:57.328 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 14:29:57.329 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:29:57.329 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:29:57.331 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:29:57.331 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:29:57.333 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:29:57.333 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:29:58.060 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 201.3114 ms
24/04/13 14:29:58.195 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:29:58.202 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005815 s
24/04/13 14:30:02.430 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 14:30:03.789 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.2061 ms
24/04/13 14:30:03.843 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:30:03.859 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:30:03.860 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 14:30:03.860 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:30:03.861 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:30:03.864 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 14:30:03.930 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:30:03.986 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:30:03.989 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:59031 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:30:03.993 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 14:30:04.007 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:30:04.009 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 14:30:04.059 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:30:04.075 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 14:30:04.539 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 159.5234 ms
24/04/13 14:30:04.560 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1516 bytes result sent to driver
24/04/13 14:30:04.569 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 520 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:30:04.571 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 14:30:04.576 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,696 s
24/04/13 14:30:04.579 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:30:04.579 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 14:30:04.580 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,736672 s
24/04/13 14:30:04.833 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:59031 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:30:04.894 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 237.7769 ms
24/04/13 14:30:09.280 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:30:09.282 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:30:09.282 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 14:30:09.282 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:30:09.282 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:30:09.283 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 14:30:09.288 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:30:09.290 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:30:09.291 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:59031 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:30:09.291 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 14:30:09.292 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:30:09.292 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 14:30:09.293 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:30:09.294 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 14:30:09.340 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/13 14:30:09.343 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 50 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:30:09.343 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 14:30:09.344 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,059 s
24/04/13 14:30:09.344 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:30:09.344 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 14:30:09.344 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,063476 s
24/04/13 14:30:11.572 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:59031 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:30:12.037 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:30:12.038 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:30:12.039 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/13 14:30:12.039 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:30:12.041 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:30:12.043 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26), which has no missing parents
24/04/13 14:30:12.064 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 107.3 KiB, free 912.2 MiB)
24/04/13 14:30:12.067 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.1 KiB, free 912.2 MiB)
24/04/13 14:30:12.069 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:59031 (size: 27.1 KiB, free: 912.3 MiB)
24/04/13 14:30:12.070 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/13 14:30:12.070 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:30:12.071 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/13 14:30:12.089 dispatcher-event-loop-1 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/13 14:30:12.089 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/13 14:30:12.090 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/13 14:30:12.313 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 151.8155 ms
24/04/13 14:30:13.049 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_13_0 stored as values in memory (estimated size 686.4 KiB, free 911.5 MiB)
24/04/13 14:30:13.050 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_13_0 in memory on DESKTOP-LH06ASP:59031 (size: 686.4 KiB, free: 911.6 MiB)
24/04/13 14:30:13.066 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 5.2037 ms
24/04/13 14:30:13.208 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 127.3652 ms
24/04/13 14:30:13.237 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 2)
[rdd_13_0]
24/04/13 14:30:13.238 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2442 bytes result sent to driver
24/04/13 14:30:13.240 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1168 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:30:13.240 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/13 14:30:13.241 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 1,197 s
24/04/13 14:30:13.241 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:30:13.241 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/13 14:30:13.242 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 1,204550 s
24/04/13 14:30:13.514 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 194.584 ms
24/04/13 14:30:16.880 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:30:16.880 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:30:16.884 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:30:16.884 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:30:16.885 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:30:16.885 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:30:16.939 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 24.8882 ms
24/04/13 14:30:16.965 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 3.8802 ms
24/04/13 14:30:16.976 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.2181 ms
24/04/13 14:30:33.073 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:30:33.073 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:30:33.099 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:30:33.131 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:30:33.160 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:30:33.161 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:30:33.168 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:30:33.183 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:30:33.187 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:30:33.189 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:30:33.189 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:30:33.190 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2
24/04/13 14:30:33.193 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:30:33.193 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-97410210-b375-41c9-ac6b-c4fcfaa224ad
24/04/13 14:30:33.195 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57
24/04/13 14:30:33.197 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-38e8c421-52c2-4511-a732-95cc5eb2b9d2\userFiles-d9a49146-ceab-403c-a25f-8f95c2d1ed57\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:30:45.313 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:30:45.492 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/13 14:30:45.512 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/13 14:30:45.559 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/13 14:30:45.662 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:30:45.662 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/13 14:30:45.663 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/13 14:30:45.664 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/13 14:30:45.691 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/13 14:30:45.702 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/13 14:30:45.702 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/13 14:30:45.750 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/13 14:30:45.751 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/13 14:30:45.751 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/13 14:30:45.752 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/13 14:30:45.752 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/13 14:30:45.835 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 59137.
24/04/13 14:30:45.860 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/13 14:30:45.892 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/13 14:30:45.914 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/13 14:30:45.915 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/13 14:30:45.918 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/13 14:30:45.941 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-4d270396-e767-4537-81b8-0a36185fb9eb
24/04/13 14:30:45.963 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/13 14:30:45.983 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/13 14:30:45.987 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/13 14:30:46.160 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/13 14:30:46.265 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/13 14:30:46.316 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:59137/jars/sparklyr-3.0-2.12.jar with timestamp 1713015045486
24/04/13 14:30:46.403 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/13 14:30:46.412 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/13 14:30:46.423 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:59137/jars/sparklyr-3.0-2.12.jar with timestamp 1713015045486
24/04/13 14:30:46.468 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:59137 after 22 ms (0 ms spent in bootstraps)
24/04/13 14:30:46.473 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:59137/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd\fetchFileTemp293355838455235786.tmp
24/04/13 14:30:46.625 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce/userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd/sparklyr-3.0-2.12.jar to class loader
24/04/13 14:30:46.647 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59157.
24/04/13 14:30:46.647 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:59157
24/04/13 14:30:46.648 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/13 14:30:46.656 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59157, None)
24/04/13 14:30:46.659 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:59157 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 59157, None)
24/04/13 14:30:46.662 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59157, None)
24/04/13 14:30:46.663 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 59157, None)
24/04/13 14:30:46.914 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/13 14:30:46.921 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/13 14:30:50.667 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/13 14:30:50.805 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/13 14:30:51.072 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/13 14:30:51.202 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/13 14:30:51.202 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/13 14:30:51.203 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/13 14:30:51.241 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/13 14:30:51.378 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/13 14:30:51.379 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/13 14:30:52.310 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/13 14:30:53.572 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/13 14:30:53.574 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/13 14:30:53.624 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/13 14:30:53.624 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/13 14:30:53.646 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/13 14:30:53.767 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/13 14:30:53.769 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/13 14:30:53.805 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/13 14:30:53.883 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:30:53.885 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:30:53.902 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/13 14:30:53.902 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/13 14:30:53.903 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/13 14:30:53.904 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:30:53.904 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:30:53.906 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:30:53.906 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:30:53.908 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:30:53.908 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:30:54.704 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 177.2981 ms
24/04/13 14:30:54.814 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:30:54.821 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005260 s
24/04/13 14:30:59.569 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/13 14:31:01.087 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.8452 ms
24/04/13 14:31:01.151 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:31:01.170 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:31:01.171 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/13 14:31:01.171 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:31:01.172 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:31:01.176 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/13 14:31:01.255 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:31:01.312 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:31:01.317 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:59157 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:31:01.322 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/13 14:31:01.341 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:31:01.343 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/13 14:31:01.401 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:31:01.417 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/13 14:31:02.116 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 208.7345 ms
24/04/13 14:31:02.144 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/13 14:31:02.156 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 768 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:31:02.159 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/13 14:31:02.165 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,971 s
24/04/13 14:31:02.168 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:31:02.169 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/13 14:31:02.170 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 1,017992 s
24/04/13 14:31:02.574 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 265.4538 ms
24/04/13 14:31:06.425 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/13 14:31:06.426 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/13 14:31:06.426 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/13 14:31:06.426 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/13 14:31:06.426 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/13 14:31:06.427 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/13 14:31:06.433 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/13 14:31:06.435 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/13 14:31:06.436 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:59157 (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:31:06.437 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/13 14:31:06.438 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/13 14:31:06.438 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/13 14:31:06.439 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/13 14:31:06.440 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/13 14:31:06.494 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/13 14:31:06.496 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 57 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/13 14:31:06.496 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/13 14:31:06.497 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,069 s
24/04/13 14:31:06.497 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/13 14:31:06.497 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/13 14:31:06.498 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,072662 s
24/04/13 14:31:06.569 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:59157 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/13 14:31:09.135 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:31:09.136 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:31:09.139 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/13 14:31:09.139 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/13 14:31:09.141 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/13 14:31:09.141 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/13 14:31:09.205 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 34.163 ms
24/04/13 14:31:09.240 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.129 ms
24/04/13 14:31:09.254 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.9453 ms
24/04/13 14:41:10.005 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/13 14:41:10.006 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/13 14:41:10.025 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/13 14:41:10.043 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/13 14:41:10.056 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/13 14:41:10.057 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/13 14:41:10.060 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/13 14:41:10.063 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/13 14:41:10.067 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:41:10.069 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/13 14:41:10.069 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/13 14:41:10.070 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-6305f741-2bfa-4398-89c9-23b33d43bbfd
24/04/13 14:41:10.072 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd
24/04/13 14:41:10.074 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/13 14:41:10.074 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce
24/04/13 14:41:10.077 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-0409ad2a-12b6-4fa4-a74d-6f36854a71ce\userFiles-ae1658e1-9644-46e0-bd81-8d9c33cf23dd\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 13:49:04.932 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 13:49:05.141 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 13:49:05.164 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 13:49:05.229 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 13:49:05.300 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 13:49:05.300 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 13:49:05.300 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 13:49:05.301 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 13:49:05.327 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 13:49:05.339 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 13:49:05.339 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 13:49:05.403 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 13:49:05.403 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 13:49:05.403 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 13:49:05.404 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 13:49:05.404 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 13:49:05.634 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 52909.
24/04/14 13:49:05.703 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 13:49:05.752 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 13:49:05.781 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 13:49:05.782 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 13:49:05.785 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 13:49:05.820 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-12c9ae43-1dc8-4d65-9430-17187eacf657
24/04/14 13:49:05.849 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 13:49:05.871 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 13:49:05.875 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 13:49:06.099 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 13:49:06.240 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 13:49:06.295 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:52909/jars/sparklyr-3.0-2.12.jar with timestamp 1713098945133
24/04/14 13:49:06.385 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 13:49:06.393 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 13:49:06.406 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:52909/jars/sparklyr-3.0-2.12.jar with timestamp 1713098945133
24/04/14 13:49:06.454 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:52909 after 23 ms (0 ms spent in bootstraps)
24/04/14 13:49:06.460 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:52909/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551\fetchFileTemp6631462731201399848.tmp
24/04/14 13:49:06.631 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7/userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551/sparklyr-3.0-2.12.jar to class loader
24/04/14 13:49:06.655 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52929.
24/04/14 13:49:06.657 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:52929
24/04/14 13:49:06.660 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 13:49:06.672 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 52929, None)
24/04/14 13:49:06.677 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:52929 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 52929, None)
24/04/14 13:49:06.682 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 52929, None)
24/04/14 13:49:06.685 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 52929, None)
24/04/14 13:49:07.037 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 13:49:07.048 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 13:49:11.162 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 13:49:11.299 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 13:49:11.623 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 13:49:11.823 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 13:49:11.824 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 13:49:11.825 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 13:49:11.877 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 13:49:12.033 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 13:49:12.034 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 13:49:13.073 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 13:49:14.539 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 13:49:14.542 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 13:49:14.681 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 13:49:14.682 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 13:49:14.712 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 13:49:14.844 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 13:49:14.847 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 13:49:14.887 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 13:49:15.015 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 13:49:15.018 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 13:49:15.043 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 13:49:15.043 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 13:49:15.044 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 13:49:15.045 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 13:49:15.046 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 13:49:15.047 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 13:49:15.048 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 13:49:15.050 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 13:49:15.051 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 13:49:15.913 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 207.7208 ms
24/04/14 13:49:16.043 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 13:49:16.052 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,006854 s
24/04/14 13:49:20.445 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 13:49:21.795 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 6.0017 ms
24/04/14 13:49:21.856 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 13:49:21.871 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 13:49:21.871 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 13:49:21.872 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 13:49:21.873 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 13:49:21.876 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 13:49:21.945 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 13:49:22.662 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 13:49:22.665 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:52929 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 13:49:22.668 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 13:49:22.682 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 13:49:22.683 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 13:49:22.735 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 13:49:22.746 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 13:49:23.260 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 150.1748 ms
24/04/14 13:49:23.284 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/14 13:49:23.293 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 568 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 13:49:23.296 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 13:49:23.303 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 1,410 s
24/04/14 13:49:23.307 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 13:49:23.308 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 13:49:23.308 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 1,452454 s
24/04/14 13:49:23.584 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 186.7969 ms
24/04/14 13:49:26.880 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 13:49:26.881 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 13:49:26.882 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 13:49:26.882 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 13:49:26.882 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 13:49:26.883 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 13:49:26.889 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 13:49:26.892 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 13:49:26.893 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:52929 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 13:49:26.894 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 13:49:26.895 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 13:49:26.895 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 13:49:26.896 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 13:49:26.897 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 13:49:26.948 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1387 bytes result sent to driver
24/04/14 13:49:26.979 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 83 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 13:49:26.980 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 13:49:26.981 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,096 s
24/04/14 13:49:26.981 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 13:49:26.981 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 13:49:26.982 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,100775 s
24/04/14 13:49:27.015 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:52929 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 13:49:29.429 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 13:49:29.429 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 13:49:29.430 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 13:49:29.431 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 13:49:29.432 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 13:49:29.432 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 13:49:29.490 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 32.4874 ms
24/04/14 13:49:29.517 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.4782 ms
24/04/14 13:49:29.533 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 9.4714 ms
24/04/14 14:26:47.624 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
24/04/14 14:26:57.644 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
24/04/14 14:27:07.658 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
24/04/14 14:27:17.664 executor-heartbeater WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
24/04/14 14:27:21.272 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/14 14:27:21.272 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/14 14:27:21.273 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/14 14:27:21.273 heartbeat-receiver-event-loop-thread WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)
24/04/14 14:33:06.916 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 14:33:06.918 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 14:33:06.986 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 14:33:07.032 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 14:33:07.060 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 14:33:07.061 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 14:33:07.067 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 14:33:07.077 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 14:33:07.083 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:33:07.090 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 14:33:07.090 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 14:33:07.092 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7
24/04/14 14:33:07.095 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:33:07.098 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-1d125f5e-9c87-4fe9-8e76-4beaf0584330
24/04/14 14:33:07.102 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551
24/04/14 14:33:07.104 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-db34bbd2-fe53-4c64-ae52-7bd619767ae7\userFiles-0be224cd-92a3-49da-812d-5a5c55bc0551\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:33:19.574 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 14:33:19.769 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 14:33:19.792 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 14:33:19.848 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 14:33:19.944 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 14:33:19.945 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 14:33:19.945 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 14:33:19.946 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 14:33:19.975 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 14:33:19.986 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 14:33:19.986 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 14:33:20.042 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 14:33:20.042 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 14:33:20.043 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 14:33:20.043 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 14:33:20.043 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 14:33:20.136 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 59040.
24/04/14 14:33:20.164 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 14:33:20.202 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 14:33:20.229 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 14:33:20.230 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 14:33:20.233 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 14:33:20.261 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-ff822d63-acef-4b0c-bfb9-7a71dca23e43
24/04/14 14:33:20.289 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 14:33:20.312 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 14:33:20.316 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 14:33:20.542 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 14:33:20.648 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 14:33:20.695 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:59040/jars/sparklyr-3.0-2.12.jar with timestamp 1713101599761
24/04/14 14:33:20.773 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 14:33:20.780 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 14:33:20.791 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:59040/jars/sparklyr-3.0-2.12.jar with timestamp 1713101599761
24/04/14 14:33:20.831 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:59040 after 18 ms (0 ms spent in bootstraps)
24/04/14 14:33:20.841 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:59040/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342\fetchFileTemp8645257544339804310.tmp
24/04/14 14:33:21.003 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-1d588a5f-f06b-4af3-aec5-5cef90013474/userFiles-e54e21ee-340f-4764-a475-4d2badf19342/sparklyr-3.0-2.12.jar to class loader
24/04/14 14:33:21.016 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59060.
24/04/14 14:33:21.016 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:59060
24/04/14 14:33:21.018 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 14:33:21.027 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59060, None)
24/04/14 14:33:21.031 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:59060 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 59060, None)
24/04/14 14:33:21.034 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 59060, None)
24/04/14 14:33:21.035 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 59060, None)
24/04/14 14:33:21.339 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 14:33:21.347 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 14:33:25.105 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 14:33:25.259 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 14:33:25.489 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 14:33:25.735 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 14:33:25.736 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 14:33:25.736 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 14:33:25.780 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 14:33:25.917 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 14:33:25.918 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 14:33:26.934 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 14:33:28.263 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 14:33:28.265 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 14:33:28.331 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 14:33:28.331 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 14:33:28.357 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 14:33:28.480 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 14:33:28.481 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 14:33:28.518 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 14:33:28.647 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:33:28.650 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:33:28.666 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 14:33:28.667 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 14:33:28.667 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 14:33:28.668 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:33:28.668 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:33:28.670 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:33:28.670 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:33:28.672 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 14:33:28.672 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 14:33:29.388 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 188.9822 ms
24/04/14 14:33:29.500 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:33:29.507 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,004950 s
24/04/14 14:33:33.760 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 14:33:35.098 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 6.276 ms
24/04/14 14:33:35.145 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:33:35.159 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 14:33:35.159 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 14:33:35.160 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 14:33:35.163 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 14:33:35.166 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 14:33:35.236 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 14:33:35.310 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 14:33:35.315 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:59060 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:33:35.320 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 14:33:35.343 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 14:33:35.345 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 14:33:35.398 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 14:33:35.414 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 14:33:35.952 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 190.0173 ms
24/04/14 14:33:35.977 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/14 14:33:35.988 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 599 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 14:33:35.989 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 14:33:35.995 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,813 s
24/04/14 14:33:35.998 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 14:33:35.999 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 14:33:35.999 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,853488 s
24/04/14 14:33:36.270 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 193.7579 ms
24/04/14 14:33:39.429 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:33:39.431 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 14:33:39.431 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 14:33:39.431 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 14:33:39.431 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 14:33:39.432 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 14:33:39.437 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 14:33:39.439 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 14:33:39.439 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:59060 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:33:39.440 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 14:33:39.440 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 14:33:39.440 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 14:33:39.441 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 14:33:39.442 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 14:33:39.510 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1516 bytes result sent to driver
24/04/14 14:33:39.512 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 71 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 14:33:39.512 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 14:33:39.513 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,080 s
24/04/14 14:33:39.513 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 14:33:39.513 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 14:33:39.514 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,084020 s
24/04/14 14:33:42.001 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:33:42.001 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:33:42.003 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:33:42.003 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:33:42.004 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 14:33:42.004 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 14:33:42.073 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 38.76 ms
24/04/14 14:33:42.099 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.2454 ms
24/04/14 14:33:42.115 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.8623 ms
24/04/14 14:47:54.443 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 14:47:54.444 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 14:47:54.468 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 14:47:54.492 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 14:47:54.515 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 14:47:54.516 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 14:47:54.527 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 14:47:54.530 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 14:47:54.535 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:47:54.536 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 14:47:54.536 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 14:47:54.537 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474
24/04/14 14:47:54.540 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:47:54.541 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-edc1b195-8086-4cd7-a225-9958ff1c357e
24/04/14 14:47:54.543 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342
24/04/14 14:47:54.545 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1d588a5f-f06b-4af3-aec5-5cef90013474\userFiles-e54e21ee-340f-4764-a475-4d2badf19342\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 14:48:06.296 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 14:48:06.481 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 14:48:06.499 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 14:48:06.554 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 14:48:06.625 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 14:48:06.626 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 14:48:06.626 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 14:48:06.627 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 14:48:06.651 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 14:48:06.663 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 14:48:06.664 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 14:48:06.717 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 14:48:06.717 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 14:48:06.718 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 14:48:06.718 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 14:48:06.718 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 14:48:06.821 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 60062.
24/04/14 14:48:06.847 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 14:48:06.883 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 14:48:06.905 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 14:48:06.906 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 14:48:06.909 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 14:48:06.932 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-59d10d38-ce90-4895-b3ee-2893d922100a
24/04/14 14:48:06.949 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 14:48:06.963 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 14:48:06.969 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 14:48:07.101 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 14:48:07.175 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 14:48:07.214 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:60062/jars/sparklyr-3.0-2.12.jar with timestamp 1713102486475
24/04/14 14:48:07.285 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 14:48:07.292 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 14:48:07.304 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:60062/jars/sparklyr-3.0-2.12.jar with timestamp 1713102486475
24/04/14 14:48:07.345 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:60062 after 19 ms (0 ms spent in bootstraps)
24/04/14 14:48:07.349 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:60062/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f\fetchFileTemp524033940281270640.tmp
24/04/14 14:48:07.509 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-1333ccc7-38db-4156-869f-570ac0f6b9cd/userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f/sparklyr-3.0-2.12.jar to class loader
24/04/14 14:48:07.521 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60082.
24/04/14 14:48:07.521 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:60082
24/04/14 14:48:07.523 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 14:48:07.530 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60082, None)
24/04/14 14:48:07.533 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:60082 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 60082, None)
24/04/14 14:48:07.536 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60082, None)
24/04/14 14:48:07.537 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 60082, None)
24/04/14 14:48:07.794 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 14:48:07.802 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 14:48:11.191 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 14:48:11.334 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 14:48:11.524 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 14:48:11.730 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 14:48:11.731 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 14:48:11.732 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 14:48:11.774 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 14:48:11.894 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 14:48:11.894 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 14:48:12.779 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 14:48:14.061 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 14:48:14.063 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 14:48:14.117 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 14:48:14.117 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 14:48:14.140 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 14:48:14.260 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 14:48:14.262 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 14:48:14.296 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 14:48:14.374 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:48:14.376 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:48:14.394 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 14:48:14.394 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 14:48:14.395 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 14:48:14.395 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:48:14.395 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:48:14.398 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:48:14.398 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:48:14.400 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 14:48:14.401 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 14:48:15.081 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 167.7736 ms
24/04/14 14:48:15.189 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:48:15.195 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005144 s
24/04/14 14:48:19.309 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 14:48:20.673 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 6.9998 ms
24/04/14 14:48:20.741 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:48:20.758 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 14:48:20.759 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 14:48:20.759 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 14:48:20.760 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 14:48:20.765 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 14:48:20.830 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 14:48:20.882 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 14:48:20.885 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:60082 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:48:20.889 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 14:48:20.905 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 14:48:20.906 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 14:48:20.960 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 14:48:20.971 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 14:48:21.536 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 168.9441 ms
24/04/14 14:48:21.559 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/14 14:48:21.568 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 618 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 14:48:21.570 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 14:48:21.576 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,796 s
24/04/14 14:48:21.580 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 14:48:21.580 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 14:48:21.581 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,839880 s
24/04/14 14:48:21.897 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 212.7904 ms
24/04/14 14:48:25.075 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 14:48:25.076 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 14:48:25.077 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 14:48:25.077 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 14:48:25.077 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 14:48:25.078 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 14:48:25.082 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 14:48:25.084 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 14:48:25.085 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:60082 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:48:25.086 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 14:48:25.086 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 14:48:25.087 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 14:48:25.088 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 14:48:25.089 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 14:48:25.129 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/14 14:48:25.131 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 43 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 14:48:25.131 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 14:48:25.132 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,053 s
24/04/14 14:48:25.132 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 14:48:25.133 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 14:48:25.133 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,057359 s
24/04/14 14:48:25.209 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:60082 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:48:25.217 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:60082 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 14:48:27.571 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:48:27.571 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:48:27.573 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 14:48:27.573 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 14:48:27.574 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 14:48:27.574 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 14:48:27.632 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 31.4804 ms
24/04/14 14:48:27.658 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 5.9715 ms
24/04/14 14:48:27.676 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.7637 ms
24/04/14 15:01:49.843 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 15:01:49.844 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 15:01:49.891 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 15:01:49.920 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 15:01:49.935 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 15:01:49.935 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 15:01:49.940 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 15:01:49.946 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 15:01:49.956 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:01:49.959 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 15:01:49.959 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 15:01:49.960 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-90805326-c4be-4636-a05a-da950d17af5d
24/04/14 15:01:49.963 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f
24/04/14 15:01:49.968 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:01:49.968 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd
24/04/14 15:01:49.972 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1333ccc7-38db-4156-869f-570ac0f6b9cd\userFiles-111a8a0e-405c-4432-8eb1-fd2ef32a904f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:02:02.650 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 15:02:02.910 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 15:02:02.938 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 15:02:03.006 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 15:02:03.100 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 15:02:03.102 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 15:02:03.102 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 15:02:03.105 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 15:02:03.149 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 15:02:03.171 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 15:02:03.171 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 15:02:03.254 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 15:02:03.254 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 15:02:03.254 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 15:02:03.256 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 15:02:03.256 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 15:02:03.372 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 60605.
24/04/14 15:02:03.399 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 15:02:03.436 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 15:02:03.465 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 15:02:03.466 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 15:02:03.471 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 15:02:03.506 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-56710843-3e36-4ba0-b5ed-25cc4b31cb12
24/04/14 15:02:03.533 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 15:02:03.558 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 15:02:03.562 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 15:02:03.746 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 15:02:03.820 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 15:02:03.858 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:60605/jars/sparklyr-3.0-2.12.jar with timestamp 1713103322899
24/04/14 15:02:03.930 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 15:02:03.937 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 15:02:03.949 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:60605/jars/sparklyr-3.0-2.12.jar with timestamp 1713103322899
24/04/14 15:02:03.997 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:60605 after 21 ms (0 ms spent in bootstraps)
24/04/14 15:02:04.001 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:60605/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2\fetchFileTemp3498373631561362447.tmp
24/04/14 15:02:04.213 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b/userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2/sparklyr-3.0-2.12.jar to class loader
24/04/14 15:02:04.225 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60625.
24/04/14 15:02:04.225 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:60625
24/04/14 15:02:04.227 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 15:02:04.235 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60625, None)
24/04/14 15:02:04.238 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:60625 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 60625, None)
24/04/14 15:02:04.241 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60625, None)
24/04/14 15:02:04.242 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 60625, None)
24/04/14 15:02:04.511 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 15:02:04.519 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 15:02:08.129 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 15:02:08.271 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 15:02:08.466 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 15:02:08.674 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 15:02:08.674 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 15:02:08.675 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 15:02:08.714 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 15:02:08.840 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 15:02:08.841 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 15:02:10.071 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 15:02:11.789 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 15:02:11.794 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 15:02:11.871 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 15:02:11.871 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 15:02:11.909 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 15:02:12.084 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 15:02:12.085 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 15:02:12.144 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 15:02:12.251 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:02:12.253 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:02:12.277 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 15:02:12.278 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 15:02:12.279 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 15:02:12.279 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:02:12.279 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:02:12.281 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:02:12.281 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:02:12.283 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 15:02:12.283 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 15:02:13.080 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 181.8462 ms
24/04/14 15:02:13.203 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:02:13.210 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,006290 s
24/04/14 15:02:18.046 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 15:02:19.593 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 6.1643 ms
24/04/14 15:02:19.653 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:02:19.672 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:02:19.673 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 15:02:19.674 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:02:19.675 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:02:19.679 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 15:02:19.774 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 15:02:19.828 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 15:02:19.831 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:60625 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:02:19.836 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 15:02:19.850 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:02:19.851 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 15:02:19.896 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 15:02:19.907 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 15:02:20.422 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 166.7323 ms
24/04/14 15:02:20.446 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/14 15:02:20.455 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 567 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:02:20.457 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 15:02:20.463 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,758 s
24/04/14 15:02:20.467 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:02:20.467 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 15:02:20.469 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,814311 s
24/04/14 15:02:20.761 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 208.7729 ms
24/04/14 15:02:24.023 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:02:24.024 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:02:24.024 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 15:02:24.024 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:02:24.025 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:02:24.026 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 15:02:24.032 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 15:02:24.035 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 15:02:24.036 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:60625 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:02:24.036 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 15:02:24.037 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:02:24.037 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 15:02:24.039 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 15:02:24.039 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 15:02:24.091 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/14 15:02:24.093 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:02:24.093 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 15:02:24.094 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,067 s
24/04/14 15:02:24.095 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:02:24.095 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 15:02:24.095 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,071664 s
24/04/14 15:02:24.198 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:60625 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:02:24.205 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:60625 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:02:26.673 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:02:26.673 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:02:26.674 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:02:26.675 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:02:26.676 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 15:02:26.676 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 15:02:26.749 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 35.0843 ms
24/04/14 15:02:26.784 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.7619 ms
24/04/14 15:02:26.808 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 13.221 ms
24/04/14 15:04:17.475 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 15:04:17.476 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 15:04:17.495 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 15:04:17.523 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 15:04:17.542 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 15:04:17.542 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 15:04:17.547 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 15:04:17.554 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 15:04:17.564 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:04:17.566 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 15:04:17.567 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 15:04:17.570 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-86b5686c-e3f2-4ac7-84f0-9d3f6b3a70d3
24/04/14 15:04:17.575 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b
24/04/14 15:04:17.578 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:04:17.578 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2
24/04/14 15:04:17.581 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1211a3b0-cdcd-4542-ba1e-bbe186e8df2b\userFiles-bbcfedce-ab5a-4150-aa15-2424b2e7dbe2\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:04:29.479 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 15:04:29.670 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 15:04:29.690 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 15:04:29.740 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 15:04:29.837 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 15:04:29.838 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 15:04:29.839 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 15:04:29.840 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 15:04:29.871 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 15:04:29.883 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 15:04:29.884 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 15:04:29.936 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 15:04:29.937 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 15:04:29.937 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 15:04:29.937 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 15:04:29.938 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 15:04:30.033 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 60799.
24/04/14 15:04:30.064 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 15:04:30.102 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 15:04:30.126 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 15:04:30.127 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 15:04:30.130 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 15:04:30.153 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-10792bea-da9f-462f-b178-b6a013a3edf0
24/04/14 15:04:30.171 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 15:04:30.187 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 15:04:30.190 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 15:04:30.338 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 15:04:30.413 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 15:04:30.452 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:60799/jars/sparklyr-3.0-2.12.jar with timestamp 1713103469663
24/04/14 15:04:30.523 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 15:04:30.530 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 15:04:30.541 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:60799/jars/sparklyr-3.0-2.12.jar with timestamp 1713103469663
24/04/14 15:04:30.584 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:60799 after 21 ms (0 ms spent in bootstraps)
24/04/14 15:04:30.589 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:60799/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c\fetchFileTemp1295951992504846968.tmp
24/04/14 15:04:30.745 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-1a1d4042-a1ce-4cfb-b498-1675321939e0/userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c/sparklyr-3.0-2.12.jar to class loader
24/04/14 15:04:30.758 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60822.
24/04/14 15:04:30.758 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:60822
24/04/14 15:04:30.760 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 15:04:30.767 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60822, None)
24/04/14 15:04:30.770 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:60822 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 60822, None)
24/04/14 15:04:30.773 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 60822, None)
24/04/14 15:04:30.774 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 60822, None)
24/04/14 15:04:31.033 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 15:04:31.040 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 15:04:34.438 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 15:04:34.568 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 15:04:34.753 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 15:04:34.963 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 15:04:34.964 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 15:04:34.964 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 15:04:35.003 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 15:04:35.124 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 15:04:35.125 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 15:04:36.035 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 15:04:37.328 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 15:04:37.330 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 15:04:37.383 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 15:04:37.383 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 15:04:37.405 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 15:04:37.524 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 15:04:37.526 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 15:04:37.565 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 15:04:37.645 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:04:37.647 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:04:37.661 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 15:04:37.662 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 15:04:37.662 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 15:04:37.663 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:04:37.663 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:04:37.665 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:04:37.665 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:04:37.666 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 15:04:37.667 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 15:04:38.353 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 175.1931 ms
24/04/14 15:04:38.460 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:04:38.467 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,005183 s
24/04/14 15:04:42.289 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 15:04:43.666 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.2865 ms
24/04/14 15:04:43.711 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:04:43.725 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:04:43.725 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 15:04:43.726 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:04:43.728 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:04:43.731 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 15:04:43.793 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 15:04:43.851 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 15:04:43.853 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:60822 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:04:43.857 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 15:04:43.871 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:04:43.873 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 15:04:43.925 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 15:04:43.938 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 15:04:44.433 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 165.0963 ms
24/04/14 15:04:44.456 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1516 bytes result sent to driver
24/04/14 15:04:44.464 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 551 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:04:44.467 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 15:04:44.472 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,727 s
24/04/14 15:04:44.476 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:04:44.476 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 15:04:44.477 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,765311 s
24/04/14 15:04:44.771 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 195.7978 ms
24/04/14 15:04:47.937 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:04:47.938 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:04:47.938 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 15:04:47.938 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:04:47.939 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:04:47.939 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 15:04:47.945 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 15:04:47.947 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 15:04:47.948 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:60822 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:04:47.949 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 15:04:47.950 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:04:47.950 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 15:04:47.951 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 15:04:47.953 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 15:04:47.996 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/14 15:04:47.998 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 47 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:04:47.998 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 15:04:47.999 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,058 s
24/04/14 15:04:47.999 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:04:47.999 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 15:04:47.999 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,062669 s
24/04/14 15:04:48.069 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:60822 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 15:04:50.442 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:04:50.443 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:04:50.445 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 15:04:50.446 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 15:04:50.448 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 15:04:50.448 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 15:04:50.509 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 30.8229 ms
24/04/14 15:04:50.540 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.1319 ms
24/04/14 15:04:50.555 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.1857 ms
24/04/14 15:05:12.753 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:05:12.754 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:05:12.754 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/14 15:05:12.754 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:05:12.755 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:05:12.756 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at collect at utils.scala:26), which has no missing parents
24/04/14 15:05:12.769 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 56.4 KiB, free 912.2 MiB)
24/04/14 15:05:12.771 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 912.2 MiB)
24/04/14 15:05:12.772 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:60822 (size: 13.5 KiB, free: 912.3 MiB)
24/04/14 15:05:12.774 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/14 15:05:12.774 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:05:12.774 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/14 15:05:12.791 dispatcher-event-loop-0 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 15:05:12.791 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 15:05:12.792 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/14 15:05:12.947 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 109.5222 ms
24/04/14 15:05:12.965 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 10.4982 ms
24/04/14 15:05:12.969 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1467 bytes result sent to driver
24/04/14 15:05:12.970 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 195 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:05:12.971 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/14 15:05:12.971 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 0,214 s
24/04/14 15:05:12.972 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:05:12.972 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/14 15:05:12.972 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 0,218873 s
24/04/14 15:05:12.991 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 13.7495 ms
24/04/14 15:05:54.644 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 15:05:54.644 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (collect at utils.scala:26) with 1 output partitions
24/04/14 15:05:54.644 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:26)
24/04/14 15:05:54.644 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 15:05:54.645 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 15:05:54.646 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at collect at utils.scala:26), which has no missing parents
24/04/14 15:05:54.653 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 56.4 KiB, free 912.1 MiB)
24/04/14 15:05:54.655 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 912.1 MiB)
24/04/14 15:05:54.655 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-LH06ASP:60822 (size: 13.5 KiB, free: 912.3 MiB)
24/04/14 15:05:54.656 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
24/04/14 15:05:54.656 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 15:05:54.656 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/04/14 15:05:54.664 dispatcher-event-loop-0 WARN TaskSetManager: Stage 3 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 15:05:54.665 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 15:05:54.665 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/04/14 15:05:54.701 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1510 bytes result sent to driver
24/04/14 15:05:54.702 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 45 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 15:05:54.703 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/04/14 15:05:54.703 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at utils.scala:26) finished in 0,057 s
24/04/14 15:05:54.703 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 15:05:54.704 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/04/14 15:05:54.704 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 4 finished: collect at utils.scala:26, took 0,060685 s
24/04/14 15:13:03.369 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 15:13:03.370 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 15:13:03.385 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 15:13:03.419 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 15:13:03.437 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 15:13:03.437 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 15:13:03.440 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 15:13:03.443 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 15:13:03.448 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:13:03.450 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 15:13:03.451 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 15:13:03.451 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0
24/04/14 15:13:03.455 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:13:03.456 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c
24/04/14 15:13:03.459 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-1a1d4042-a1ce-4cfb-b498-1675321939e0\userFiles-2f5751bd-7552-4d15-836f-2856dbbb605c\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 15:13:03.460 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-b8156eb7-ae67-4cef-96c1-9d485db446b7
24/04/14 17:30:09.013 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 17:30:09.225 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/04/14 17:30:09.248 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/04/14 17:30:09.313 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/04/14 17:30:09.381 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 17:30:09.382 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/04/14 17:30:09.382 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/04/14 17:30:09.383 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/04/14 17:30:09.412 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/04/14 17:30:09.427 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/04/14 17:30:09.427 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/04/14 17:30:09.485 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/04/14 17:30:09.485 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/04/14 17:30:09.485 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/04/14 17:30:09.486 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/04/14 17:30:09.486 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/04/14 17:30:09.573 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 63522.
24/04/14 17:30:09.604 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/04/14 17:30:09.643 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/04/14 17:30:09.670 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/04/14 17:30:09.670 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/04/14 17:30:09.674 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/04/14 17:30:09.705 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-6cd9e65b-59f9-44f7-85e8-a8fa3f4c69f4
24/04/14 17:30:09.733 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/04/14 17:30:09.759 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/04/14 17:30:09.764 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/04/14 17:30:10.037 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/04/14 17:30:10.167 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/04/14 17:30:10.217 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:63522/jars/sparklyr-3.0-2.12.jar with timestamp 1713112209217
24/04/14 17:30:10.295 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/04/14 17:30:10.304 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/04/14 17:30:10.316 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:63522/jars/sparklyr-3.0-2.12.jar with timestamp 1713112209217
24/04/14 17:30:10.360 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:63522 after 21 ms (0 ms spent in bootstraps)
24/04/14 17:30:10.366 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:63522/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441\fetchFileTemp3766881699594006508.tmp
24/04/14 17:30:10.529 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-11ff8928-2268-4097-b738-954081439a84/userFiles-2358e5b1-f4b0-400f-9548-9acc46766441/sparklyr-3.0-2.12.jar to class loader
24/04/14 17:30:10.540 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63542.
24/04/14 17:30:10.541 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:63542
24/04/14 17:30:10.542 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/04/14 17:30:10.567 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 63542, None)
24/04/14 17:30:10.572 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:63542 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 63542, None)
24/04/14 17:30:10.576 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 63542, None)
24/04/14 17:30:10.578 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 63542, None)
24/04/14 17:30:10.936 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/04/14 17:30:10.945 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/04/14 17:30:15.086 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/04/14 17:30:15.219 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/04/14 17:30:15.553 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/04/14 17:30:15.741 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/04/14 17:30:15.742 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/04/14 17:30:15.742 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/04/14 17:30:15.790 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/04/14 17:30:15.946 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/04/14 17:30:15.947 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/04/14 17:30:16.972 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/04/14 17:30:18.411 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/04/14 17:30:18.414 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/04/14 17:30:18.492 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/04/14 17:30:18.492 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/04/14 17:30:18.518 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/04/14 17:30:18.643 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/04/14 17:30:18.645 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/04/14 17:30:18.682 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/04/14 17:30:18.801 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 17:30:18.803 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 17:30:18.820 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/04/14 17:30:18.820 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/04/14 17:30:18.821 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/04/14 17:30:18.822 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 17:30:18.822 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 17:30:18.824 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 17:30:18.824 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 17:30:18.825 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 17:30:18.826 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 17:30:19.847 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 232.2976 ms
24/04/14 17:30:20.000 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:30:20.008 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,006436 s
24/04/14 17:30:24.661 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/04/14 17:30:26.332 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.8823 ms
24/04/14 17:30:26.404 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:30:26.425 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:30:26.426 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/04/14 17:30:26.426 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:30:26.428 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:30:26.433 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26), which has no missing parents
24/04/14 17:30:26.529 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 17:30:26.619 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 17:30:26.626 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:30:26.633 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/04/14 17:30:26.656 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:30:26.658 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/04/14 17:30:26.733 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 17:30:26.753 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/04/14 17:30:27.341 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 204.2508 ms
24/04/14 17:30:27.366 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1559 bytes result sent to driver
24/04/14 17:30:27.377 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 658 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:30:27.379 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/04/14 17:30:27.385 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,928 s
24/04/14 17:30:27.388 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:30:27.388 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/04/14 17:30:27.389 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,985765 s
24/04/14 17:30:27.704 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 231.7126 ms
24/04/14 17:30:32.036 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:30:32.058 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:30:32.059 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:30:32.059 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/04/14 17:30:32.060 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:30:32.060 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:30:32.062 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26), which has no missing parents
24/04/14 17:30:32.071 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 17:30:32.074 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 17:30:32.075 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:30:32.075 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/04/14 17:30:32.076 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:30:32.077 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/04/14 17:30:32.078 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 17:30:32.079 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/04/14 17:30:32.144 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1430 bytes result sent to driver
24/04/14 17:30:32.149 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 71 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:30:32.149 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/04/14 17:30:32.150 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,087 s
24/04/14 17:30:32.151 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:30:32.151 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/04/14 17:30:32.151 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,093484 s
24/04/14 17:30:35.072 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:30:35.073 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:30:35.073 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/04/14 17:30:35.073 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:30:35.074 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:30:35.075 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at collect at utils.scala:26), which has no missing parents
24/04/14 17:30:35.086 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 56.4 KiB, free 912.2 MiB)
24/04/14 17:30:35.088 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 912.2 MiB)
24/04/14 17:30:35.089 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 13.5 KiB, free: 912.3 MiB)
24/04/14 17:30:35.089 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/04/14 17:30:35.089 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:30:35.090 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/04/14 17:30:35.103 dispatcher-event-loop-1 WARN TaskSetManager: Stage 2 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:30:35.103 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 17:30:35.104 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/04/14 17:30:35.333 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 165.4321 ms
24/04/14 17:30:35.375 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 22.1972 ms
24/04/14 17:30:35.394 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1467 bytes result sent to driver
24/04/14 17:30:35.402 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 312 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:30:35.404 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/04/14 17:30:35.405 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 0,329 s
24/04/14 17:30:35.406 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:30:35.406 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/04/14 17:30:35.407 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 0,333197 s
24/04/14 17:30:35.451 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 29.8792 ms
24/04/14 17:30:36.494 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 17:30:36.495 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 17:30:36.534 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/04/14 17:30:36.534 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/04/14 17:30:36.535 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/04/14 17:30:36.535 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/04/14 17:30:36.626 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 43.3707 ms
24/04/14 17:30:36.659 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 9.43 ms
24/04/14 17:30:36.673 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 6.8977 ms
24/04/14 17:33:10.165 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_2_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 13.5 KiB, free: 912.3 MiB)
24/04/14 17:33:10.170 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:49:17.139 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:49:17.141 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:49:17.141 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (collect at utils.scala:26)
24/04/14 17:49:17.141 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:49:17.142 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:49:17.143 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at collect at utils.scala:26), which has no missing parents
24/04/14 17:49:17.148 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 17:49:17.151 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 17:49:17.152 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:49:17.152 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
24/04/14 17:49:17.153 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:49:17.153 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/04/14 17:49:17.154 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 17:49:17.156 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/04/14 17:49:17.195 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1430 bytes result sent to driver
24/04/14 17:49:17.197 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 43 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:49:17.197 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/04/14 17:49:17.198 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (collect at utils.scala:26) finished in 0,054 s
24/04/14 17:49:17.198 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:49:17.199 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/04/14 17:49:17.199 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 4 finished: collect at utils.scala:26, took 0,059214 s
24/04/14 17:49:20.303 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:49:20.304 dag-scheduler-event-loop INFO DAGScheduler: Got job 5 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:49:20.304 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (collect at utils.scala:26)
24/04/14 17:49:20.304 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:49:20.304 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:49:20.307 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at utils.scala:26), which has no missing parents
24/04/14 17:49:20.311 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 62.4 KiB, free 912.2 MiB)
24/04/14 17:49:20.314 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 912.2 MiB)
24/04/14 17:49:20.315 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:49:20.315 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
24/04/14 17:49:20.316 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:49:20.316 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/04/14 17:49:20.317 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 17:49:20.317 Executor task launch worker for task 0.0 in stage 4.0 (TID 4) INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/04/14 17:49:20.352 Executor task launch worker for task 0.0 in stage 4.0 (TID 4) INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1430 bytes result sent to driver
24/04/14 17:49:20.353 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 36 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:49:20.353 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/04/14 17:49:20.354 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (collect at utils.scala:26) finished in 0,046 s
24/04/14 17:49:20.354 dag-scheduler-event-loop INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:49:20.354 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/04/14 17:49:20.355 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 5 finished: collect at utils.scala:26, took 0,051259 s
24/04/14 17:49:20.428 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_3_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:49:20.432 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_4_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 912.3 MiB)
24/04/14 17:49:27.619 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:49:27.619 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:49:27.620 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:26)
24/04/14 17:49:27.620 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:49:27.621 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:49:27.623 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:26), which has no missing parents
24/04/14 17:49:27.627 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 123.2 KiB, free 912.2 MiB)
24/04/14 17:49:27.628 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 912.1 MiB)
24/04/14 17:49:27.629 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 31.0 KiB, free: 912.3 MiB)
24/04/14 17:49:27.629 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
24/04/14 17:49:27.630 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[28] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:49:27.630 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/04/14 17:49:27.638 dispatcher-event-loop-0 WARN TaskSetManager: Stage 5 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:49:27.638 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 17:49:27.638 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/04/14 17:49:28.048 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO MemoryStore: Block rdd_22_0 stored as values in memory (estimated size 686.4 KiB, free 911.5 MiB)
24/04/14 17:49:28.050 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_22_0 in memory on DESKTOP-LH06ASP:63542 (size: 686.4 KiB, free: 911.6 MiB)
24/04/14 17:49:28.062 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO CodeGenerator: Code generated in 3.6095 ms
24/04/14 17:49:28.204 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO CodeGenerator: Code generated in 128.8522 ms
24/04/14 17:49:28.994 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO CodeGenerator: Code generated in 171.9864 ms
24/04/14 17:49:29.019 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO CodeGenerator: Code generated in 4.8892 ms
24/04/14 17:49:29.162 Executor task launch worker for task 0.0 in stage 5.0 (TID 5) INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 247548 bytes result sent to driver
24/04/14 17:49:29.163 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1533 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:49:29.164 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/04/14 17:49:29.164 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 5 (collect at utils.scala:26) finished in 1,541 s
24/04/14 17:49:29.165 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:49:29.165 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/04/14 17:49:29.165 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 6 finished: collect at utils.scala:26, took 1,546718 s
24/04/14 17:49:29.387 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 108.393 ms
24/04/14 17:49:31.972 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:49:31.973 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:49:31.973 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:26)
24/04/14 17:49:31.973 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:49:31.976 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:49:31.977 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[34] at collect at utils.scala:26), which has no missing parents
24/04/14 17:49:31.980 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 123.2 KiB, free 911.4 MiB)
24/04/14 17:49:31.982 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 911.3 MiB)
24/04/14 17:49:31.983 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 31.0 KiB, free: 911.6 MiB)
24/04/14 17:49:31.984 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
24/04/14 17:49:31.984 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[34] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:49:31.984 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/04/14 17:49:31.995 dispatcher-event-loop-0 WARN TaskSetManager: Stage 6 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:49:31.995 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 17:49:31.996 Executor task launch worker for task 0.0 in stage 6.0 (TID 6) INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
24/04/14 17:49:32.005 Executor task launch worker for task 0.0 in stage 6.0 (TID 6) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 17:49:32.098 Executor task launch worker for task 0.0 in stage 6.0 (TID 6) INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 135223 bytes result sent to driver
24/04/14 17:49:32.099 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 112 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:49:32.099 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/04/14 17:49:32.100 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 6 (collect at utils.scala:26) finished in 0,123 s
24/04/14 17:49:32.100 dag-scheduler-event-loop INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:49:32.101 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/04/14 17:49:32.101 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 7 finished: collect at utils.scala:26, took 0,128464 s
24/04/14 17:59:25.347 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_6_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 31.0 KiB, free: 911.6 MiB)
24/04/14 17:59:26.021 nioEventLoopGroup-2-2 INFO Instrumentation: [ddb82b0a] training finished
24/04/14 17:59:26.730 nioEventLoopGroup-2-2 INFO Instrumentation: [abe5ca82] training finished
24/04/14 17:59:27.361 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 54.8009 ms
24/04/14 17:59:27.439 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] Stage class: RandomForestRegressor
24/04/14 17:59:27.439 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] Stage uid: random_forest__b37e1725_f5c0_415f_bd08_a879e17b9ea5
24/04/14 17:59:27.439 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
24/04/14 17:59:27.454 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"subsamplingRate":1.0,"impurity":"variance","featuresCol":"features","maxDepth":5,"minInstancesPerNode":1,"featureSubsetStrategy":"auto","checkpointInterval":10,"minInfoGain":0.0,"cacheNodeIds":false,"labelCol":"label","predictionCol":"prediction","maxMemoryInMB":256,"maxBins":32,"numTrees":20}
24/04/14 17:59:27.533 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
24/04/14 17:59:27.533 dag-scheduler-event-loop INFO DAGScheduler: Got job 8 (take at DecisionTreeMetadata.scala:119) with 1 output partitions
24/04/14 17:59:27.533 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (take at DecisionTreeMetadata.scala:119)
24/04/14 17:59:27.533 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:59:27.533 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:59:27.533 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[47] at map at DecisionTreeMetadata.scala:119), which has no missing parents
24/04/14 17:59:27.548 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 398.0 KiB, free 911.1 MiB)
24/04/14 17:59:27.548 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 82.7 KiB, free 911.0 MiB)
24/04/14 17:59:27.548 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 82.7 KiB, free: 911.5 MiB)
24/04/14 17:59:27.548 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:27.548 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[47] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:27.548 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/04/14 17:59:27.548 dispatcher-event-loop-0 WARN TaskSetManager: Stage 7 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:27.548 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 17:59:27.548 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
24/04/14 17:59:27.666 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 17:59:27.901 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO CodeGenerator: Code generated in 107.7892 ms
24/04/14 17:59:27.916 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO CodeGenerator: Code generated in 5.8131 ms
24/04/14 17:59:27.932 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO CodeGenerator: Code generated in 6.9661 ms
24/04/14 17:59:27.948 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO CodeGenerator: Code generated in 5.3364 ms
24/04/14 17:59:27.948 Executor task launch worker for task 0.0 in stage 7.0 (TID 7) INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1657 bytes result sent to driver
24/04/14 17:59:27.948 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 400 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:27.948 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (take at DecisionTreeMetadata.scala:119) finished in 0,430 s
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/04/14 17:59:27.963 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 8 finished: take at DecisionTreeMetadata.scala:119, took 0,425690 s
24/04/14 17:59:27.963 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Got job 9 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 8 (aggregate at DecisionTreeMetadata.scala:125)
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:59:27.963 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[46] at retag at RandomForest.scala:274), which has no missing parents
24/04/14 17:59:27.979 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 398.0 KiB, free 910.6 MiB)
24/04/14 17:59:27.979 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 82.9 KiB, free 910.5 MiB)
24/04/14 17:59:27.979 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 82.9 KiB, free: 911.4 MiB)
24/04/14 17:59:27.979 dag-scheduler-event-loop INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:27.979 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[46] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:27.979 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/04/14 17:59:27.995 dispatcher-event-loop-1 WARN TaskSetManager: Stage 8 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:27.995 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 17:59:27.995 Executor task launch worker for task 0.0 in stage 8.0 (TID 8) INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
24/04/14 17:59:28.010 Executor task launch worker for task 0.0 in stage 8.0 (TID 8) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 17:59:28.423 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_7_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 82.7 KiB, free: 911.5 MiB)
24/04/14 17:59:28.423 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_5_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 31.0 KiB, free: 911.5 MiB)
24/04/14 17:59:28.516 Executor task launch worker for task 0.0 in stage 8.0 (TID 8) INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1772 bytes result sent to driver
24/04/14 17:59:28.516 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 537 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:28.516 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/04/14 17:59:28.516 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 8 (aggregate at DecisionTreeMetadata.scala:125) finished in 0,537 s
24/04/14 17:59:28.516 dag-scheduler-event-loop INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:28.516 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/04/14 17:59:28.516 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 9 finished: aggregate at DecisionTreeMetadata.scala:125, took 0,542233 s
24/04/14 17:59:28.579 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1054
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 49 (flatMap at RandomForest.scala:1039) as input to shuffle 0
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Got job 10 (collectAsMap at RandomForest.scala:1054) with 1 output partitions
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 10 (collectAsMap at RandomForest.scala:1054)
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
24/04/14 17:59:28.608 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[49] at flatMap at RandomForest.scala:1039), which has no missing parents
24/04/14 17:59:28.623 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 407.0 KiB, free 910.8 MiB)
24/04/14 17:59:28.623 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 86.5 KiB, free 910.7 MiB)
24/04/14 17:59:28.623 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 86.5 KiB, free: 911.5 MiB)
24/04/14 17:59:28.623 dag-scheduler-event-loop INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:28.623 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[49] at flatMap at RandomForest.scala:1039) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:28.633 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/04/14 17:59:28.642 dispatcher-event-loop-0 WARN TaskSetManager: Stage 9 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:28.642 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:28.642 Executor task launch worker for task 0.0 in stage 9.0 (TID 9) INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
24/04/14 17:59:28.697 Executor task launch worker for task 0.0 in stage 9.0 (TID 9) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 17:59:29.214 Executor task launch worker for task 0.0 in stage 9.0 (TID 9) INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2040 bytes result sent to driver
24/04/14 17:59:29.214 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 581 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:29.214 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/04/14 17:59:29.214 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 9 (flatMap at RandomForest.scala:1039) finished in 0,606 s
24/04/14 17:59:29.230 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:29.230 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:29.230 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 10)
24/04/14 17:59:29.230 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:29.234 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[51] at map at RandomForest.scala:1054), which has no missing parents
24/04/14 17:59:29.240 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 11.9 KiB, free 910.7 MiB)
24/04/14 17:59:29.241 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 910.7 MiB)
24/04/14 17:59:29.242 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 4.5 KiB, free: 911.5 MiB)
24/04/14 17:59:29.243 dag-scheduler-event-loop INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:29.244 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at map at RandomForest.scala:1054) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:29.244 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
24/04/14 17:59:29.246 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:29.246 Executor task launch worker for task 0.0 in stage 10.0 (TID 10) INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
24/04/14 17:59:29.266 Executor task launch worker for task 0.0 in stage 10.0 (TID 10) INFO ShuffleBlockFetcherIterator: Getting 1 (42.2 KiB) non-empty blocks including 1 (42.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:29.282 Executor task launch worker for task 0.0 in stage 10.0 (TID 10) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/04/14 17:59:29.344 Executor task launch worker for task 0.0 in stage 10.0 (TID 10) INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 48798 bytes result sent to driver
24/04/14 17:59:29.344 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 99 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:29.344 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/04/14 17:59:29.344 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 10 (collectAsMap at RandomForest.scala:1054) finished in 0,106 s
24/04/14 17:59:29.344 dag-scheduler-event-loop INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:29.344 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
24/04/14 17:59:29.344 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 10 finished: collectAsMap at RandomForest.scala:1054, took 0,771069 s
24/04/14 17:59:29.360 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 59.3 KiB, free 910.6 MiB)
24/04/14 17:59:29.360 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 910.6 MiB)
24/04/14 17:59:29.360 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 10.1 KiB, free: 911.5 MiB)
24/04/14 17:59:29.360 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 11 from broadcast at RandomForest.scala:293
24/04/14 17:59:29.375 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"numFeatures":545}
24/04/14 17:59:29.375 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"numClasses":0}
24/04/14 17:59:29.375 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"numExamples":1439}
24/04/14 17:59:29.375 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"sumOfWeights":1439.0}
24/04/14 17:59:29.395 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 16.1 KiB, free 910.6 MiB)
24/04/14 17:59:29.398 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 910.6 MiB)
24/04/14 17:59:29.399 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 10.3 KiB, free: 911.4 MiB)
24/04/14 17:59:29.399 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 12 from broadcast at RandomForest.scala:622
24/04/14 17:59:29.428 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 54 (mapPartitions at RandomForest.scala:644) as input to shuffle 1
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Got job 11 (collectAsMap at RandomForest.scala:663) with 1 output partitions
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 12 (collectAsMap at RandomForest.scala:663)
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)
24/04/14 17:59:29.428 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[54] at mapPartitions at RandomForest.scala:644), which has no missing parents
24/04/14 17:59:29.443 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 441.6 KiB, free 910.1 MiB)
24/04/14 17:59:29.443 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 99.2 KiB, free 910.0 MiB)
24/04/14 17:59:29.443 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 99.2 KiB, free: 911.3 MiB)
24/04/14 17:59:29.443 dag-scheduler-event-loop INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:29.443 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[54] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:29.443 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/04/14 17:59:29.443 dispatcher-event-loop-1 WARN TaskSetManager: Stage 11 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:29.459 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:29.459 Executor task launch worker for task 0.0 in stage 11.0 (TID 11) INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
24/04/14 17:59:29.475 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_9_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 86.5 KiB, free: 911.4 MiB)
24/04/14 17:59:29.484 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_10_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 4.5 KiB, free: 911.4 MiB)
24/04/14 17:59:29.484 Executor task launch worker for task 0.0 in stage 11.0 (TID 11) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 17:59:29.791 Executor task launch worker for task 0.0 in stage 11.0 (TID 11) INFO MemoryStore: Block rdd_53_0 stored as values in memory (estimated size 3.2 MiB, free 907.4 MiB)
24/04/14 17:59:29.791 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_53_0 in memory on DESKTOP-LH06ASP:63542 (size: 3.2 MiB, free: 908.3 MiB)
24/04/14 17:59:29.838 Executor task launch worker for task 0.0 in stage 11.0 (TID 11) INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1954 bytes result sent to driver
24/04/14 17:59:29.838 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 395 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:29.838 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 11 (mapPartitions at RandomForest.scala:644) finished in 0,410 s
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 12)
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[56] at map at RandomForest.scala:663), which has no missing parents
24/04/14 17:59:29.838 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 7.4 KiB, free 907.4 MiB)
24/04/14 17:59:29.853 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 907.4 MiB)
24/04/14 17:59:29.853 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 3.8 KiB, free: 908.3 MiB)
24/04/14 17:59:29.853 dag-scheduler-event-loop INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:29.853 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[56] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:29.853 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
24/04/14 17:59:29.853 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:29.853 Executor task launch worker for task 0.0 in stage 12.0 (TID 12) INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
24/04/14 17:59:29.853 Executor task launch worker for task 0.0 in stage 12.0 (TID 12) INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:29.853 Executor task launch worker for task 0.0 in stage 12.0 (TID 12) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/04/14 17:59:29.963 Executor task launch worker for task 0.0 in stage 12.0 (TID 12) INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 6380 bytes result sent to driver
24/04/14 17:59:29.963 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 110 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:29.963 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/04/14 17:59:29.963 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 12 (collectAsMap at RandomForest.scala:663) finished in 0,125 s
24/04/14 17:59:29.963 dag-scheduler-event-loop INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:29.963 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
24/04/14 17:59:29.963 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 11 finished: collectAsMap at RandomForest.scala:663, took 0,544045 s
24/04/14 17:59:29.963 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(12) (from destroy at RandomForest.scala:674)
24/04/14 17:59:29.963 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_12_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 10.3 KiB, free: 908.3 MiB)
24/04/14 17:59:29.983 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 32.6 KiB, free 907.4 MiB)
24/04/14 17:59:29.983 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 907.3 MiB)
24/04/14 17:59:29.983 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 20.3 KiB, free: 908.3 MiB)
24/04/14 17:59:29.983 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 15 from broadcast at RandomForest.scala:622
24/04/14 17:59:29.998 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 57 (mapPartitions at RandomForest.scala:644) as input to shuffle 2
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Got job 12 (collectAsMap at RandomForest.scala:663) with 1 output partitions
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 14 (collectAsMap at RandomForest.scala:663)
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)
24/04/14 17:59:29.998 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[57] at mapPartitions at RandomForest.scala:644), which has no missing parents
24/04/14 17:59:30.014 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 462.6 KiB, free 906.9 MiB)
24/04/14 17:59:30.014 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 112.4 KiB, free 906.8 MiB)
24/04/14 17:59:30.014 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 112.4 KiB, free: 908.2 MiB)
24/04/14 17:59:30.014 dag-scheduler-event-loop INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.014 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[57] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.014 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
24/04/14 17:59:30.029 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_14_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 3.8 KiB, free: 908.2 MiB)
24/04/14 17:59:30.029 dispatcher-event-loop-1 WARN TaskSetManager: Stage 13 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:30.029 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:30.029 Executor task launch worker for task 0.0 in stage 13.0 (TID 13) INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
24/04/14 17:59:30.029 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_13_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 99.2 KiB, free: 908.3 MiB)
24/04/14 17:59:30.045 Executor task launch worker for task 0.0 in stage 13.0 (TID 13) INFO BlockManager: Found block rdd_53_0 locally
24/04/14 17:59:30.092 Executor task launch worker for task 0.0 in stage 13.0 (TID 13) INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 1911 bytes result sent to driver
24/04/14 17:59:30.092 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 78 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.092 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 13 (mapPartitions at RandomForest.scala:644) finished in 0,094 s
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 14)
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at map at RandomForest.scala:663), which has no missing parents
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 11.1 KiB, free 907.3 MiB)
24/04/14 17:59:30.092 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 907.3 MiB)
24/04/14 17:59:30.092 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 5.2 KiB, free: 908.2 MiB)
24/04/14 17:59:30.108 dag-scheduler-event-loop INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.108 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.108 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/04/14 17:59:30.108 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:30.108 Executor task launch worker for task 0.0 in stage 14.0 (TID 14) INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
24/04/14 17:59:30.108 Executor task launch worker for task 0.0 in stage 14.0 (TID 14) INFO ShuffleBlockFetcherIterator: Getting 1 (312.6 KiB) non-empty blocks including 1 (312.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:30.108 Executor task launch worker for task 0.0 in stage 14.0 (TID 14) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/04/14 17:59:30.133 Executor task launch worker for task 0.0 in stage 14.0 (TID 14) INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 10467 bytes result sent to driver
24/04/14 17:59:30.133 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 25 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.133 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.133 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 14 (collectAsMap at RandomForest.scala:663) finished in 0,041 s
24/04/14 17:59:30.133 dag-scheduler-event-loop INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:30.133 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/04/14 17:59:30.133 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 12 finished: collectAsMap at RandomForest.scala:663, took 0,135089 s
24/04/14 17:59:30.133 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(15) (from destroy at RandomForest.scala:674)
24/04/14 17:59:30.133 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_15_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 20.3 KiB, free: 908.3 MiB)
24/04/14 17:59:30.148 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 57.2 KiB, free 907.3 MiB)
24/04/14 17:59:30.148 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 35.2 KiB, free 907.3 MiB)
24/04/14 17:59:30.148 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 35.2 KiB, free: 908.2 MiB)
24/04/14 17:59:30.148 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 18 from broadcast at RandomForest.scala:622
24/04/14 17:59:30.165 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
24/04/14 17:59:30.166 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 60 (mapPartitions at RandomForest.scala:644) as input to shuffle 3
24/04/14 17:59:30.166 dag-scheduler-event-loop INFO DAGScheduler: Got job 13 (collectAsMap at RandomForest.scala:663) with 1 output partitions
24/04/14 17:59:30.167 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 16 (collectAsMap at RandomForest.scala:663)
24/04/14 17:59:30.167 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
24/04/14 17:59:30.167 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
24/04/14 17:59:30.168 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[60] at mapPartitions at RandomForest.scala:644), which has no missing parents
24/04/14 17:59:30.179 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 495.5 KiB, free 906.8 MiB)
24/04/14 17:59:30.181 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 131.2 KiB, free 906.7 MiB)
24/04/14 17:59:30.181 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 131.2 KiB, free: 908.1 MiB)
24/04/14 17:59:30.182 dag-scheduler-event-loop INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.182 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[60] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.182 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
24/04/14 17:59:30.187 dispatcher-event-loop-1 WARN TaskSetManager: Stage 15 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:30.187 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:30.187 Executor task launch worker for task 0.0 in stage 15.0 (TID 15) INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
24/04/14 17:59:30.187 Executor task launch worker for task 0.0 in stage 15.0 (TID 15) INFO BlockManager: Found block rdd_53_0 locally
24/04/14 17:59:30.250 Executor task launch worker for task 0.0 in stage 15.0 (TID 15) INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 1868 bytes result sent to driver
24/04/14 17:59:30.251 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 68 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.251 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 15 (mapPartitions at RandomForest.scala:644) finished in 0,083 s
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 16)
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:30.252 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[62] at map at RandomForest.scala:663), which has no missing parents
24/04/14 17:59:30.253 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 14.0 KiB, free 906.6 MiB)
24/04/14 17:59:30.255 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 906.6 MiB)
24/04/14 17:59:30.255 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 6.0 KiB, free: 908.1 MiB)
24/04/14 17:59:30.256 dag-scheduler-event-loop INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.256 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[62] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.256 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
24/04/14 17:59:30.257 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:30.257 Executor task launch worker for task 0.0 in stage 16.0 (TID 16) INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
24/04/14 17:59:30.259 Executor task launch worker for task 0.0 in stage 16.0 (TID 16) INFO ShuffleBlockFetcherIterator: Getting 1 (457.6 KiB) non-empty blocks including 1 (457.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:30.259 Executor task launch worker for task 0.0 in stage 16.0 (TID 16) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/04/14 17:59:30.281 Executor task launch worker for task 0.0 in stage 16.0 (TID 16) INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 16724 bytes result sent to driver
24/04/14 17:59:30.281 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 24 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.281 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.281 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 16 (collectAsMap at RandomForest.scala:663) finished in 0,029 s
24/04/14 17:59:30.281 dag-scheduler-event-loop INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:30.281 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
24/04/14 17:59:30.281 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 13 finished: collectAsMap at RandomForest.scala:663, took 0,118004 s
24/04/14 17:59:30.281 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(18) (from destroy at RandomForest.scala:674)
24/04/14 17:59:30.281 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_18_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 35.2 KiB, free: 908.1 MiB)
24/04/14 17:59:30.281 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 77.4 KiB, free 906.7 MiB)
24/04/14 17:59:30.281 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 906.6 MiB)
24/04/14 17:59:30.281 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 48.2 KiB, free: 908.1 MiB)
24/04/14 17:59:30.281 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 21 from broadcast at RandomForest.scala:622
24/04/14 17:59:30.297 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 63 (mapPartitions at RandomForest.scala:644) as input to shuffle 4
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Got job 14 (collectAsMap at RandomForest.scala:663) with 1 output partitions
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 18 (collectAsMap at RandomForest.scala:663)
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 17)
24/04/14 17:59:30.297 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[63] at mapPartitions at RandomForest.scala:644), which has no missing parents
24/04/14 17:59:30.312 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 532.4 KiB, free 906.1 MiB)
24/04/14 17:59:30.312 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 149.4 KiB, free 905.9 MiB)
24/04/14 17:59:30.312 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 149.4 KiB, free: 907.9 MiB)
24/04/14 17:59:30.312 dag-scheduler-event-loop INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.312 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[63] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.312 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
24/04/14 17:59:30.328 dispatcher-event-loop-1 WARN TaskSetManager: Stage 17 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:30.328 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:30.328 Executor task launch worker for task 0.0 in stage 17.0 (TID 17) INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
24/04/14 17:59:30.328 Executor task launch worker for task 0.0 in stage 17.0 (TID 17) INFO BlockManager: Found block rdd_53_0 locally
24/04/14 17:59:30.359 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_17_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 5.2 KiB, free: 907.9 MiB)
24/04/14 17:59:30.359 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_16_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 112.4 KiB, free: 908.1 MiB)
24/04/14 17:59:30.375 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_19_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 131.2 KiB, free: 908.2 MiB)
24/04/14 17:59:30.375 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_20_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 6.0 KiB, free: 908.2 MiB)
24/04/14 17:59:30.406 Executor task launch worker for task 0.0 in stage 17.0 (TID 17) INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 1954 bytes result sent to driver
24/04/14 17:59:30.406 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 94 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.406 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 17 (mapPartitions at RandomForest.scala:644) finished in 0,109 s
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 18)
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[65] at map at RandomForest.scala:663), which has no missing parents
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 16.5 KiB, free 907.1 MiB)
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 907.1 MiB)
24/04/14 17:59:30.422 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 6.6 KiB, free: 908.2 MiB)
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[65] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.422 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
24/04/14 17:59:30.422 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:30.422 Executor task launch worker for task 0.0 in stage 18.0 (TID 18) INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
24/04/14 17:59:30.422 Executor task launch worker for task 0.0 in stage 18.0 (TID 18) INFO ShuffleBlockFetcherIterator: Getting 1 (503.4 KiB) non-empty blocks including 1 (503.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:30.422 Executor task launch worker for task 0.0 in stage 18.0 (TID 18) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/04/14 17:59:30.453 Executor task launch worker for task 0.0 in stage 18.0 (TID 18) INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 21950 bytes result sent to driver
24/04/14 17:59:30.453 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 31 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.453 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.453 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 18 (collectAsMap at RandomForest.scala:663) finished in 0,031 s
24/04/14 17:59:30.453 dag-scheduler-event-loop INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:30.453 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
24/04/14 17:59:30.453 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 14 finished: collectAsMap at RandomForest.scala:663, took 0,152434 s
24/04/14 17:59:30.453 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(21) (from destroy at RandomForest.scala:674)
24/04/14 17:59:30.453 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_21_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 48.2 KiB, free: 908.2 MiB)
24/04/14 17:59:30.453 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 71.7 KiB, free 907.2 MiB)
24/04/14 17:59:30.468 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 44.5 KiB, free 907.1 MiB)
24/04/14 17:59:30.468 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 44.5 KiB, free: 908.2 MiB)
24/04/14 17:59:30.468 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 24 from broadcast at RandomForest.scala:622
24/04/14 17:59:30.468 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
24/04/14 17:59:30.468 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 66 (mapPartitions at RandomForest.scala:644) as input to shuffle 5
24/04/14 17:59:30.468 dag-scheduler-event-loop INFO DAGScheduler: Got job 15 (collectAsMap at RandomForest.scala:663) with 1 output partitions
24/04/14 17:59:30.468 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 20 (collectAsMap at RandomForest.scala:663)
24/04/14 17:59:30.468 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
24/04/14 17:59:30.468 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List(ShuffleMapStage 19)
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[66] at mapPartitions at RandomForest.scala:644), which has no missing parents
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 552.2 KiB, free 906.6 MiB)
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 152.3 KiB, free 906.4 MiB)
24/04/14 17:59:30.484 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 152.3 KiB, free: 908.0 MiB)
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[66] at mapPartitions at RandomForest.scala:644) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.484 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
24/04/14 17:59:30.500 dispatcher-event-loop-1 WARN TaskSetManager: Stage 19 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 17:59:30.500 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818691 bytes) 
24/04/14 17:59:30.500 Executor task launch worker for task 0.0 in stage 19.0 (TID 19) INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
24/04/14 17:59:30.516 Executor task launch worker for task 0.0 in stage 19.0 (TID 19) INFO BlockManager: Found block rdd_53_0 locally
24/04/14 17:59:30.563 Executor task launch worker for task 0.0 in stage 19.0 (TID 19) INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1911 bytes result sent to driver
24/04/14 17:59:30.563 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 79 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.563 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 19 (mapPartitions at RandomForest.scala:644) finished in 0,079 s
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set(ResultStage 20)
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[68] at map at RandomForest.scala:663), which has no missing parents
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 15.8 KiB, free 906.4 MiB)
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 906.4 MiB)
24/04/14 17:59:30.563 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 6.3 KiB, free: 908.0 MiB)
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[68] at map at RandomForest.scala:663) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:30.563 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
24/04/14 17:59:30.563 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7181 bytes) 
24/04/14 17:59:30.563 Executor task launch worker for task 0.0 in stage 20.0 (TID 20) INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
24/04/14 17:59:30.563 Executor task launch worker for task 0.0 in stage 20.0 (TID 20) INFO ShuffleBlockFetcherIterator: Getting 1 (457.6 KiB) non-empty blocks including 1 (457.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/04/14 17:59:30.563 Executor task launch worker for task 0.0 in stage 20.0 (TID 20) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/04/14 17:59:30.594 Executor task launch worker for task 0.0 in stage 20.0 (TID 20) INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 20483 bytes result sent to driver
24/04/14 17:59:30.594 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 31 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:30.594 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
24/04/14 17:59:30.594 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 20 (collectAsMap at RandomForest.scala:663) finished in 0,031 s
24/04/14 17:59:30.594 dag-scheduler-event-loop INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:30.594 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
24/04/14 17:59:30.594 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 15 finished: collectAsMap at RandomForest.scala:663, took 0,116703 s
24/04/14 17:59:30.594 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(24) (from destroy at RandomForest.scala:674)
24/04/14 17:59:30.594 nioEventLoopGroup-2-2 INFO RandomForest: Internal timing for DecisionTree:
24/04/14 17:59:30.594 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_24_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 44.5 KiB, free: 908.1 MiB)
24/04/14 17:59:30.594 nioEventLoopGroup-2-2 INFO RandomForest:   init: 0.0039227
  total: 1.2203433
  findBestSplits: 1.1840316
  chooseSplits: 1.1761299
24/04/14 17:59:30.594 nioEventLoopGroup-2-2 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
24/04/14 17:59:30.610 nioEventLoopGroup-2-2 INFO TorrentBroadcast: Destroying Broadcast(11) (from destroy at RandomForest.scala:305)
24/04/14 17:59:30.610 block-manager-storage-async-thread-pool-84 INFO BlockManager: Removing RDD 53
24/04/14 17:59:30.610 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_11_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 10.1 KiB, free: 911.2 MiB)
24/04/14 17:59:30.610 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] {"numFeatures":545}
24/04/14 17:59:30.625 nioEventLoopGroup-2-2 INFO Instrumentation: [ca1d4a7f] training finished
24/04/14 17:59:30.625 nioEventLoopGroup-2-2 INFO Instrumentation: [4f30f4b8] training finished
24/04/14 17:59:31.078 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_26_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 6.3 KiB, free: 911.2 MiB)
24/04/14 17:59:31.078 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_25_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 152.3 KiB, free: 911.4 MiB)
24/04/14 17:59:31.078 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_23_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 6.6 KiB, free: 911.4 MiB)
24/04/14 17:59:31.140 nioEventLoopGroup-2-2 INFO Instrumentation: [ad9dccfd] training finished
24/04/14 17:59:32.256 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 17:59:32.257 dag-scheduler-event-loop INFO DAGScheduler: Got job 16 (collect at utils.scala:26) with 1 output partitions
24/04/14 17:59:32.257 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 21 (collect at utils.scala:26)
24/04/14 17:59:32.257 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 17:59:32.257 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 17:59:32.258 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[71] at collect at utils.scala:26), which has no missing parents
24/04/14 17:59:32.261 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 62.5 KiB, free 910.4 MiB)
24/04/14 17:59:32.262 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 910.4 MiB)
24/04/14 17:59:32.262 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 17:59:32.263 dag-scheduler-event-loop INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
24/04/14 17:59:32.263 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[71] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 17:59:32.263 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
24/04/14 17:59:32.264 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 17:59:32.264 Executor task launch worker for task 0.0 in stage 21.0 (TID 21) INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
24/04/14 17:59:32.295 Executor task launch worker for task 0.0 in stage 21.0 (TID 21) INFO CodeGenerator: Code generated in 20.2196 ms
24/04/14 17:59:32.311 Executor task launch worker for task 0.0 in stage 21.0 (TID 21) INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 1430 bytes result sent to driver
24/04/14 17:59:32.311 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 47 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 17:59:32.311 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
24/04/14 17:59:32.311 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 21 (collect at utils.scala:26) finished in 0,052 s
24/04/14 17:59:32.311 dag-scheduler-event-loop INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 17:59:32.311 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
24/04/14 17:59:32.311 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 16 finished: collect at utils.scala:26, took 0,058209 s
24/04/14 17:59:32.436 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_27_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 17:59:32.436 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 105.3228 ms
24/04/14 18:00:11.149 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_22_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 149.4 KiB, free: 911.5 MiB)
24/04/14 18:00:11.164 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_8_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 82.9 KiB, free: 911.6 MiB)
24/04/14 18:00:11.164 block-manager-storage-async-thread-pool-20 INFO BlockManager: Removing RDD 53
24/04/14 18:15:50.777 nioEventLoopGroup-2-2 INFO Instrumentation: [37f18b88] training finished
24/04/14 18:15:50.839 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 477.8 KiB, free 911.2 MiB)
24/04/14 18:15:50.855 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 911.1 MiB)
24/04/14 18:15:50.855 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 37.4 KiB, free: 911.6 MiB)
24/04/14 18:15:50.855 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 28 from broadcast at RandomForestRegressor.scala:238
24/04/14 18:15:50.902 nioEventLoopGroup-2-2 INFO Instrumentation: [784f0b1e] training finished
24/04/14 18:15:51.901 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Got job 17 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 22 (collect at utils.scala:26)
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[74] at collect at utils.scala:26), which has no missing parents
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 62.6 KiB, free 911.1 MiB)
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 911.1 MiB)
24/04/14 18:15:51.901 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.6 MiB)
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[74] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:15:51.901 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
24/04/14 18:15:51.917 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 18:15:51.917 Executor task launch worker for task 0.0 in stage 22.0 (TID 22) INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
24/04/14 18:15:51.948 Executor task launch worker for task 0.0 in stage 22.0 (TID 22) INFO CodeGenerator: Code generated in 19.0768 ms
24/04/14 18:15:51.948 Executor task launch worker for task 0.0 in stage 22.0 (TID 22) INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 1430 bytes result sent to driver
24/04/14 18:15:51.964 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 63 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:15:51.964 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/04/14 18:15:51.964 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 22 (collect at utils.scala:26) finished in 0,063 s
24/04/14 18:15:51.964 dag-scheduler-event-loop INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:15:51.964 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
24/04/14 18:15:51.964 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 17 finished: collect at utils.scala:26, took 0,055151 s
24/04/14 18:15:52.058 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 55.8078 ms
24/04/14 18:15:56.562 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:15:56.562 dag-scheduler-event-loop INFO DAGScheduler: Got job 18 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:15:56.562 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 23 (collect at utils.scala:26)
24/04/14 18:15:56.562 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:15:56.562 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:15:56.562 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[82] at collect at utils.scala:26), which has no missing parents
24/04/14 18:15:56.577 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 452.7 KiB, free 910.6 MiB)
24/04/14 18:15:56.577 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 910.5 MiB)
24/04/14 18:15:56.577 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.5 KiB, free: 911.5 MiB)
24/04/14 18:15:56.577 dag-scheduler-event-loop INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535
24/04/14 18:15:56.577 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[82] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:15:56.577 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
24/04/14 18:15:56.609 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_29_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.5 MiB)
24/04/14 18:15:56.609 dispatcher-event-loop-1 WARN TaskSetManager: Stage 23 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:15:56.609 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:15:56.609 Executor task launch worker for task 0.0 in stage 23.0 (TID 23) INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
24/04/14 18:15:56.624 Executor task launch worker for task 0.0 in stage 23.0 (TID 23) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:15:56.884 Executor task launch worker for task 0.0 in stage 23.0 (TID 23) INFO CodeGenerator: Code generated in 99.5138 ms
24/04/14 18:15:56.993 Executor task launch worker for task 0.0 in stage 23.0 (TID 23) INFO CodeGenerator: Code generated in 72.6996 ms
24/04/14 18:15:57.118 Executor task launch worker for task 0.0 in stage 23.0 (TID 23) INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 138396 bytes result sent to driver
24/04/14 18:15:57.118 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 541 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:15:57.118 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/04/14 18:15:57.118 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 23 (collect at utils.scala:26) finished in 0,556 s
24/04/14 18:15:57.118 dag-scheduler-event-loop INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:15:57.118 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
24/04/14 18:15:57.118 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 18 finished: collect at utils.scala:26, took 0,560069 s
24/04/14 18:15:57.259 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 71.1064 ms
24/04/14 18:16:58.324 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:16:58.324 dag-scheduler-event-loop INFO DAGScheduler: Got job 19 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:16:58.324 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 24 (collect at utils.scala:26)
24/04/14 18:16:58.324 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:16:58.324 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[90] at collect at utils.scala:26), which has no missing parents
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 452.7 KiB, free 910.2 MiB)
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 910.1 MiB)
24/04/14 18:16:58.339 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.5 KiB, free: 911.4 MiB)
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[90] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:16:58.339 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
24/04/14 18:16:58.371 dispatcher-event-loop-0 WARN TaskSetManager: Stage 24 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:16:58.371 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:16:58.371 Executor task launch worker for task 0.0 in stage 24.0 (TID 24) INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
24/04/14 18:16:58.371 Executor task launch worker for task 0.0 in stage 24.0 (TID 24) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:16:58.692 Executor task launch worker for task 0.0 in stage 24.0 (TID 24) INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 138353 bytes result sent to driver
24/04/14 18:16:58.692 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 353 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:16:58.692 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
24/04/14 18:16:58.692 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 24 (collect at utils.scala:26) finished in 0,353 s
24/04/14 18:16:58.692 dag-scheduler-event-loop INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:16:58.692 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
24/04/14 18:16:58.692 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 19 finished: collect at utils.scala:26, took 0,356532 s
24/04/14 18:17:26.428 nioEventLoopGroup-2-2 INFO Instrumentation: [661f9b75] training finished
24/04/14 18:17:26.484 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 477.8 KiB, free 909.6 MiB)
24/04/14 18:17:26.486 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 909.6 MiB)
24/04/14 18:17:26.501 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 37.4 KiB, free: 911.4 MiB)
24/04/14 18:17:26.501 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 32 from broadcast at RandomForestRegressor.scala:238
24/04/14 18:17:26.548 nioEventLoopGroup-2-2 INFO Instrumentation: [fad32b85] training finished
24/04/14 18:17:27.500 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:17:27.500 dag-scheduler-event-loop INFO DAGScheduler: Got job 20 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:17:27.500 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 25 (collect at utils.scala:26)
24/04/14 18:17:27.500 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:17:27.500 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:17:27.500 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[93] at collect at utils.scala:26), which has no missing parents
24/04/14 18:17:27.516 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 62.6 KiB, free 909.5 MiB)
24/04/14 18:17:27.521 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 909.5 MiB)
24/04/14 18:17:27.521 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 18:17:27.521 dag-scheduler-event-loop INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535
24/04/14 18:17:27.521 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[93] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:17:27.521 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
24/04/14 18:17:27.521 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 18:17:27.521 Executor task launch worker for task 0.0 in stage 25.0 (TID 25) INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
24/04/14 18:17:27.532 Executor task launch worker for task 0.0 in stage 25.0 (TID 25) INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 1430 bytes result sent to driver
24/04/14 18:17:27.547 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 26 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:17:27.547 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/04/14 18:17:27.547 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 25 (collect at utils.scala:26) finished in 0,047 s
24/04/14 18:17:27.547 dag-scheduler-event-loop INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:17:27.547 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
24/04/14 18:17:27.547 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 20 finished: collect at utils.scala:26, took 0,035309 s
24/04/14 18:17:27.594 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_33_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 18:17:33.665 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:17:33.666 dag-scheduler-event-loop INFO DAGScheduler: Got job 21 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:17:33.666 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 26 (collect at utils.scala:26)
24/04/14 18:17:33.666 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:17:33.666 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:17:33.668 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[101] at collect at utils.scala:26), which has no missing parents
24/04/14 18:17:33.694 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 452.9 KiB, free 909.1 MiB)
24/04/14 18:17:33.696 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 909.0 MiB)
24/04/14 18:17:33.696 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.6 KiB, free: 911.3 MiB)
24/04/14 18:17:33.696 dag-scheduler-event-loop INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535
24/04/14 18:17:33.697 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[101] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:17:33.697 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
24/04/14 18:17:33.704 dispatcher-event-loop-0 WARN TaskSetManager: Stage 26 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:17:33.704 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:17:33.705 Executor task launch worker for task 0.0 in stage 26.0 (TID 26) INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
24/04/14 18:17:33.726 Executor task launch worker for task 0.0 in stage 26.0 (TID 26) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:17:33.992 Executor task launch worker for task 0.0 in stage 26.0 (TID 26) INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 2307 bytes result sent to driver
24/04/14 18:17:33.992 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 295 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:17:33.992 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/04/14 18:17:33.992 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 26 (collect at utils.scala:26) finished in 0,324 s
24/04/14 18:17:33.992 dag-scheduler-event-loop INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:17:33.992 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
24/04/14 18:17:33.992 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 21 finished: collect at utils.scala:26, took 0,331467 s
24/04/14 18:21:34.600 nioEventLoopGroup-2-2 INFO Instrumentation: [b9fbdfa7] training finished
24/04/14 18:21:34.647 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 477.8 KiB, free 908.6 MiB)
24/04/14 18:21:34.663 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 908.5 MiB)
24/04/14 18:21:34.663 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 37.4 KiB, free: 911.3 MiB)
24/04/14 18:21:34.663 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 35 from broadcast at RandomForestRegressor.scala:238
24/04/14 18:21:34.738 nioEventLoopGroup-2-2 INFO Instrumentation: [725d9c8a] training finished
24/04/14 18:21:35.925 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:21:35.925 dag-scheduler-event-loop INFO DAGScheduler: Got job 22 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:21:35.925 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 27 (collect at utils.scala:26)
24/04/14 18:21:35.925 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:21:35.925 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:21:35.925 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[104] at collect at utils.scala:26), which has no missing parents
24/04/14 18:21:35.940 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 62.6 KiB, free 908.5 MiB)
24/04/14 18:21:35.940 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 908.5 MiB)
24/04/14 18:21:35.940 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.2 MiB)
24/04/14 18:21:35.940 dag-scheduler-event-loop INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535
24/04/14 18:21:35.940 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[104] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:21:35.940 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
24/04/14 18:21:35.940 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 18:21:35.940 Executor task launch worker for task 0.0 in stage 27.0 (TID 27) INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
24/04/14 18:21:35.956 Executor task launch worker for task 0.0 in stage 27.0 (TID 27) INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 1430 bytes result sent to driver
24/04/14 18:21:35.956 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 16 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:21:35.956 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/04/14 18:21:35.956 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 27 (collect at utils.scala:26) finished in 0,031 s
24/04/14 18:21:35.956 dag-scheduler-event-loop INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:21:35.956 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
24/04/14 18:21:35.956 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 22 finished: collect at utils.scala:26, took 0,031304 s
24/04/14 18:21:42.350 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_36_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.3 MiB)
24/04/14 18:21:42.350 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:21:42.350 dag-scheduler-event-loop INFO DAGScheduler: Got job 23 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:21:42.350 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 28 (collect at utils.scala:26)
24/04/14 18:21:42.350 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:21:42.350 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:21:42.350 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[112] at collect at utils.scala:26), which has no missing parents
24/04/14 18:21:42.381 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 452.9 KiB, free 908.1 MiB)
24/04/14 18:21:42.381 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 908.0 MiB)
24/04/14 18:21:42.381 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.6 KiB, free: 911.2 MiB)
24/04/14 18:21:42.381 dag-scheduler-event-loop INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535
24/04/14 18:21:42.381 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[112] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:21:42.381 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
24/04/14 18:21:42.397 dispatcher-event-loop-1 WARN TaskSetManager: Stage 28 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:21:42.397 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:21:42.397 Executor task launch worker for task 0.0 in stage 28.0 (TID 28) INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
24/04/14 18:21:42.413 Executor task launch worker for task 0.0 in stage 28.0 (TID 28) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:21:42.600 Executor task launch worker for task 0.0 in stage 28.0 (TID 28) INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 2221 bytes result sent to driver
24/04/14 18:21:42.600 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 203 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:21:42.600 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/04/14 18:21:42.600 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 28 (collect at utils.scala:26) finished in 0,250 s
24/04/14 18:21:42.600 dag-scheduler-event-loop INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:21:42.600 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
24/04/14 18:21:42.600 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 23 finished: collect at utils.scala:26, took 0,245092 s
24/04/14 18:21:46.537 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_37_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.6 KiB, free: 911.3 MiB)
24/04/14 18:21:46.677 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:21:46.677 dag-scheduler-event-loop INFO DAGScheduler: Got job 24 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:21:46.678 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 29 (collect at utils.scala:26)
24/04/14 18:21:46.678 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:21:46.678 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:21:46.678 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[120] at collect at utils.scala:26), which has no missing parents
24/04/14 18:21:46.686 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 452.7 KiB, free 908.1 MiB)
24/04/14 18:21:46.688 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 908.0 MiB)
24/04/14 18:21:46.689 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.5 KiB, free: 911.2 MiB)
24/04/14 18:21:46.689 dag-scheduler-event-loop INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535
24/04/14 18:21:46.689 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[120] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:21:46.689 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
24/04/14 18:21:46.696 dispatcher-event-loop-0 WARN TaskSetManager: Stage 29 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:21:46.696 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:21:46.697 Executor task launch worker for task 0.0 in stage 29.0 (TID 29) INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
24/04/14 18:21:46.701 Executor task launch worker for task 0.0 in stage 29.0 (TID 29) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:21:47.080 Executor task launch worker for task 0.0 in stage 29.0 (TID 29) INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 138396 bytes result sent to driver
24/04/14 18:21:47.110 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 420 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:21:47.110 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/04/14 18:21:47.110 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 29 (collect at utils.scala:26) finished in 0,431 s
24/04/14 18:21:47.110 dag-scheduler-event-loop INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:21:47.110 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
24/04/14 18:21:47.110 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 24 finished: collect at utils.scala:26, took 0,434419 s
24/04/14 18:21:47.151 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_38_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.5 KiB, free: 911.3 MiB)
24/04/14 18:29:56.037 nioEventLoopGroup-2-2 INFO Instrumentation: [0a52126d] training finished
24/04/14 18:29:56.101 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 477.8 KiB, free 908.1 MiB)
24/04/14 18:29:56.104 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 908.0 MiB)
24/04/14 18:29:56.104 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 37.4 KiB, free: 911.2 MiB)
24/04/14 18:29:56.105 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 39 from broadcast at RandomForestRegressor.scala:238
24/04/14 18:29:56.161 nioEventLoopGroup-2-2 INFO Instrumentation: [e9dce56d] training finished
24/04/14 18:29:57.372 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:29:57.372 dag-scheduler-event-loop INFO DAGScheduler: Got job 25 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:29:57.373 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 30 (collect at utils.scala:26)
24/04/14 18:29:57.373 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:29:57.373 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:29:57.376 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[123] at collect at utils.scala:26), which has no missing parents
24/04/14 18:29:57.379 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 62.6 KiB, free 908.0 MiB)
24/04/14 18:29:57.382 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 908.0 MiB)
24/04/14 18:29:57.384 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.2 MiB)
24/04/14 18:29:57.384 dag-scheduler-event-loop INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535
24/04/14 18:29:57.385 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[123] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:29:57.385 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
24/04/14 18:29:57.386 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 18:29:57.387 Executor task launch worker for task 0.0 in stage 30.0 (TID 30) INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
24/04/14 18:29:57.425 Executor task launch worker for task 0.0 in stage 30.0 (TID 30) INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 1430 bytes result sent to driver
24/04/14 18:29:57.426 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 41 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:29:57.426 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/04/14 18:29:57.426 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 30 (collect at utils.scala:26) finished in 0,049 s
24/04/14 18:29:57.427 dag-scheduler-event-loop INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:29:57.427 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
24/04/14 18:29:57.427 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 25 finished: collect at utils.scala:26, took 0,054696 s
24/04/14 18:29:57.519 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_40_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.2 MiB)
24/04/14 18:30:08.358 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:30:08.358 dag-scheduler-event-loop INFO DAGScheduler: Got job 26 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:30:08.360 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 31 (collect at utils.scala:26)
24/04/14 18:30:08.360 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:30:08.360 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:30:08.362 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[131] at collect at utils.scala:26), which has no missing parents
24/04/14 18:30:08.418 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 452.9 KiB, free 907.6 MiB)
24/04/14 18:30:08.420 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 907.5 MiB)
24/04/14 18:30:08.420 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.6 KiB, free: 911.1 MiB)
24/04/14 18:30:08.420 dag-scheduler-event-loop INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535
24/04/14 18:30:08.421 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[131] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:30:08.421 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
24/04/14 18:30:08.434 dispatcher-event-loop-0 WARN TaskSetManager: Stage 31 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:30:08.434 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:30:08.435 Executor task launch worker for task 0.0 in stage 31.0 (TID 31) INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
24/04/14 18:30:08.447 Executor task launch worker for task 0.0 in stage 31.0 (TID 31) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:30:08.725 Executor task launch worker for task 0.0 in stage 31.0 (TID 31) INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 2264 bytes result sent to driver
24/04/14 18:30:08.726 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 305 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:30:08.726 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/04/14 18:30:08.726 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 31 (collect at utils.scala:26) finished in 0,363 s
24/04/14 18:30:08.727 dag-scheduler-event-loop INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:30:08.727 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
24/04/14 18:30:08.727 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 26 finished: collect at utils.scala:26, took 0,367943 s
24/04/14 18:30:11.267 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_30_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.5 KiB, free: 911.2 MiB)
24/04/14 18:30:11.270 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_41_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.6 KiB, free: 911.3 MiB)
24/04/14 18:30:11.273 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_34_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.6 KiB, free: 911.4 MiB)
24/04/14 18:30:11.277 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_31_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.5 KiB, free: 911.5 MiB)
24/04/14 18:30:14.629 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:30:14.629 dag-scheduler-event-loop INFO DAGScheduler: Got job 27 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:30:14.629 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 32 (collect at utils.scala:26)
24/04/14 18:30:14.629 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:30:14.630 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:30:14.631 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[139] at collect at utils.scala:26), which has no missing parents
24/04/14 18:30:14.638 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 452.7 KiB, free 909.2 MiB)
24/04/14 18:30:14.639 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 909.1 MiB)
24/04/14 18:30:14.640 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.5 KiB, free: 911.4 MiB)
24/04/14 18:30:14.640 dag-scheduler-event-loop INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535
24/04/14 18:30:14.641 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[139] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:30:14.641 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
24/04/14 18:30:14.649 dispatcher-event-loop-0 WARN TaskSetManager: Stage 32 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:30:14.649 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:30:14.650 Executor task launch worker for task 0.0 in stage 32.0 (TID 32) INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
24/04/14 18:30:14.660 Executor task launch worker for task 0.0 in stage 32.0 (TID 32) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:30:15.723 Executor task launch worker for task 0.0 in stage 32.0 (TID 32) INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 138396 bytes result sent to driver
24/04/14 18:30:15.726 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 1085 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:30:15.726 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/04/14 18:30:15.727 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 32 (collect at utils.scala:26) finished in 1,096 s
24/04/14 18:30:15.727 dag-scheduler-event-loop INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:30:15.728 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
24/04/14 18:30:15.728 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 27 finished: collect at utils.scala:26, took 1,099030 s
24/04/14 18:32:19.627 nioEventLoopGroup-2-2 INFO Instrumentation: [e9e7ddf1] training finished
24/04/14 18:32:19.674 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 477.8 KiB, free 908.6 MiB)
24/04/14 18:32:19.674 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 37.4 KiB, free 908.6 MiB)
24/04/14 18:32:19.674 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 37.4 KiB, free: 911.4 MiB)
24/04/14 18:32:19.674 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 43 from broadcast at RandomForestRegressor.scala:238
24/04/14 18:32:19.721 nioEventLoopGroup-2-2 INFO Instrumentation: [f688d7eb] training finished
24/04/14 18:32:19.846 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_42_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.5 KiB, free: 911.4 MiB)
24/04/14 18:32:20.998 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:32:20.999 dag-scheduler-event-loop INFO DAGScheduler: Got job 28 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:32:20.999 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 33 (collect at utils.scala:26)
24/04/14 18:32:20.999 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:32:20.999 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:32:21.000 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[142] at collect at utils.scala:26), which has no missing parents
24/04/14 18:32:21.004 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 62.6 KiB, free 909.1 MiB)
24/04/14 18:32:21.008 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 909.0 MiB)
24/04/14 18:32:21.008 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 18:32:21.009 dag-scheduler-event-loop INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535
24/04/14 18:32:21.009 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[142] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:32:21.010 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
24/04/14 18:32:21.011 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/04/14 18:32:21.012 Executor task launch worker for task 0.0 in stage 33.0 (TID 33) INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
24/04/14 18:32:21.053 Executor task launch worker for task 0.0 in stage 33.0 (TID 33) INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 1430 bytes result sent to driver
24/04/14 18:32:21.053 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 42 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:32:21.053 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
24/04/14 18:32:21.053 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 33 (collect at utils.scala:26) finished in 0,052 s
24/04/14 18:32:21.055 dag-scheduler-event-loop INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:32:21.055 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
24/04/14 18:32:21.055 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 28 finished: collect at utils.scala:26, took 0,057076 s
24/04/14 18:32:30.626 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:32:30.627 dag-scheduler-event-loop INFO DAGScheduler: Got job 29 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:32:30.627 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 34 (collect at utils.scala:26)
24/04/14 18:32:30.627 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:32:30.627 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:32:30.628 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[150] at collect at utils.scala:26), which has no missing parents
24/04/14 18:32:30.887 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 452.9 KiB, free 908.6 MiB)
24/04/14 18:32:30.889 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_44_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 12.7 KiB, free: 911.4 MiB)
24/04/14 18:32:30.890 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 908.6 MiB)
24/04/14 18:32:30.891 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.6 KiB, free: 911.4 MiB)
24/04/14 18:32:30.891 dag-scheduler-event-loop INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535
24/04/14 18:32:30.892 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[150] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:32:30.892 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
24/04/14 18:32:30.916 dispatcher-event-loop-0 WARN TaskSetManager: Stage 34 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:32:30.916 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:32:30.917 Executor task launch worker for task 0.0 in stage 34.0 (TID 34) INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
24/04/14 18:32:30.942 Executor task launch worker for task 0.0 in stage 34.0 (TID 34) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:32:31.242 Executor task launch worker for task 0.0 in stage 34.0 (TID 34) INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 2264 bytes result sent to driver
24/04/14 18:32:31.243 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 349 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:32:31.243 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/04/14 18:32:31.243 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 34 (collect at utils.scala:26) finished in 0,613 s
24/04/14 18:32:31.243 dag-scheduler-event-loop INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:32:31.243 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
24/04/14 18:32:31.244 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 29 finished: collect at utils.scala:26, took 0,617088 s
24/04/14 18:32:31.366 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_45_piece0 on DESKTOP-LH06ASP:63542 in memory (size: 90.6 KiB, free: 911.4 MiB)
24/04/14 18:32:41.139 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:32:41.140 dag-scheduler-event-loop INFO DAGScheduler: Got job 30 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:32:41.140 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 35 (collect at utils.scala:26)
24/04/14 18:32:41.140 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:32:41.140 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:32:41.143 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[158] at collect at utils.scala:26), which has no missing parents
24/04/14 18:32:41.158 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 452.7 KiB, free 908.7 MiB)
24/04/14 18:32:41.162 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 90.5 KiB, free 908.6 MiB)
24/04/14 18:32:41.163 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 90.5 KiB, free: 911.4 MiB)
24/04/14 18:32:41.165 dag-scheduler-event-loop INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535
24/04/14 18:32:41.166 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[158] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:32:41.167 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
24/04/14 18:32:41.190 dispatcher-event-loop-0 WARN TaskSetManager: Stage 35 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:32:41.190 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:32:41.191 Executor task launch worker for task 0.0 in stage 35.0 (TID 35) INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
24/04/14 18:32:41.218 Executor task launch worker for task 0.0 in stage 35.0 (TID 35) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:32:42.941 Executor task launch worker for task 0.0 in stage 35.0 (TID 35) INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 138439 bytes result sent to driver
24/04/14 18:32:42.942 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 1773 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:32:42.942 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/04/14 18:32:42.943 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 35 (collect at utils.scala:26) finished in 1,799 s
24/04/14 18:32:42.944 dag-scheduler-event-loop INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:32:42.944 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
24/04/14 18:32:42.944 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 30 finished: collect at utils.scala:26, took 1,805962 s
24/04/14 18:32:47.449 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/04/14 18:32:47.451 dag-scheduler-event-loop INFO DAGScheduler: Got job 31 (collect at utils.scala:26) with 1 output partitions
24/04/14 18:32:47.451 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 36 (collect at utils.scala:26)
24/04/14 18:32:47.451 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/04/14 18:32:47.452 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/04/14 18:32:47.453 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[164] at collect at utils.scala:26), which has no missing parents
24/04/14 18:32:47.456 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 123.2 KiB, free 908.5 MiB)
24/04/14 18:32:47.456 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 31.1 KiB, free 908.4 MiB)
24/04/14 18:32:47.456 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on DESKTOP-LH06ASP:63542 (size: 31.1 KiB, free: 911.3 MiB)
24/04/14 18:32:47.456 dag-scheduler-event-loop INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535
24/04/14 18:32:47.462 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[164] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/04/14 18:32:47.462 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
24/04/14 18:32:47.481 dispatcher-event-loop-1 WARN TaskSetManager: Stage 36 contains a task of very large size (9588 KiB). The maximum recommended task size is 1000 KiB.
24/04/14 18:32:47.481 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 9818702 bytes) 
24/04/14 18:32:47.486 Executor task launch worker for task 0.0 in stage 36.0 (TID 36) INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
24/04/14 18:32:47.500 Executor task launch worker for task 0.0 in stage 36.0 (TID 36) INFO BlockManager: Found block rdd_22_0 locally
24/04/14 18:32:47.705 Executor task launch worker for task 0.0 in stage 36.0 (TID 36) INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 135266 bytes result sent to driver
24/04/14 18:32:47.709 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 243 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/04/14 18:32:47.709 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/04/14 18:32:47.712 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 36 (collect at utils.scala:26) finished in 0,259 s
24/04/14 18:32:47.714 dag-scheduler-event-loop INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
24/04/14 18:32:47.714 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
24/04/14 18:32:47.716 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 31 finished: collect at utils.scala:26, took 0,264939 s
24/04/14 18:49:06.203 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/04/14 18:49:06.207 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/04/14 18:49:06.243 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/04/14 18:49:06.270 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/04/14 18:49:06.393 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/04/14 18:49:06.393 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/04/14 18:49:06.397 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/04/14 18:49:06.403 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/04/14 18:49:06.409 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 18:49:06.413 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/04/14 18:49:06.414 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/04/14 18:49:06.414 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84
24/04/14 18:49:06.418 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/04/14 18:49:06.419 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-37cc3b01-9e45-456e-b0d4-bc532b2340e8
24/04/14 18:49:06.421 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441
24/04/14 18:49:06.424 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-11ff8928-2268-4097-b738-954081439a84\userFiles-2358e5b1-f4b0-400f-9548-9acc46766441\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
