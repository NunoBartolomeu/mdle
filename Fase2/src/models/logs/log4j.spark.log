24/05/24 19:32:51.309 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/05/24 19:32:51.722 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/05/24 19:32:51.760 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/05/24 19:32:51.871 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/05/24 19:32:51.982 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/05/24 19:32:51.983 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/05/24 19:32:51.984 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/05/24 19:32:51.985 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/05/24 19:32:52.024 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/05/24 19:32:52.041 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/05/24 19:32:52.043 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/05/24 19:32:52.131 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/05/24 19:32:52.132 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/05/24 19:32:52.133 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/05/24 19:32:52.134 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/05/24 19:32:52.134 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/05/24 19:32:52.272 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 50756.
24/05/24 19:32:52.323 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/05/24 19:32:52.378 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/05/24 19:32:52.421 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/05/24 19:32:52.422 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/05/24 19:32:52.427 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/05/24 19:32:52.472 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-b6fdfaaa-214a-4e02-b7a9-b7685284aced
24/05/24 19:32:52.512 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/05/24 19:32:52.545 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/05/24 19:32:52.550 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/05/24 19:32:52.873 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/05/24 19:32:53.054 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/05/24 19:32:53.133 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:50756/jars/sparklyr-3.0-2.12.jar with timestamp 1716575571708
24/05/24 19:32:53.278 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/05/24 19:32:53.293 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/05/24 19:32:53.314 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:50756/jars/sparklyr-3.0-2.12.jar with timestamp 1716575571708
24/05/24 19:32:53.397 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:50756 after 40 ms (0 ms spent in bootstraps)
24/05/24 19:32:53.405 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:50756/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722\fetchFileTemp1177865411879383572.tmp
24/05/24 19:32:53.704 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7/userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722/sparklyr-3.0-2.12.jar to class loader
24/05/24 19:32:53.755 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50776.
24/05/24 19:32:53.756 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:50776
24/05/24 19:32:53.758 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/05/24 19:32:53.775 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 50776, None)
24/05/24 19:32:53.782 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:50776 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 50776, None)
24/05/24 19:32:53.788 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 50776, None)
24/05/24 19:32:53.790 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 50776, None)
24/05/24 19:32:54.310 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/05/24 19:32:54.323 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/05/24 19:32:59.933 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/05/24 19:33:00.158 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/05/24 19:33:00.656 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/05/24 19:33:01.017 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/05/24 19:33:01.018 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/05/24 19:33:01.018 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/05/24 19:33:01.089 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/05/24 19:33:01.327 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/05/24 19:33:01.329 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/05/24 19:33:03.061 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/05/24 19:33:05.172 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/05/24 19:33:05.180 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/05/24 19:33:05.296 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/05/24 19:33:05.297 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/05/24 19:33:05.346 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/05/24 19:33:05.561 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/05/24 19:33:05.564 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/05/24 19:33:05.646 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/05/24 19:33:05.800 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:33:05.803 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:33:05.828 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/05/24 19:33:05.830 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/05/24 19:33:05.830 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/05/24 19:33:05.831 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:33:05.832 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:33:05.834 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:33:05.834 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:33:05.837 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:33:05.837 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:33:06.338 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:33:06.338 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:33:06.340 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:33:06.340 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:33:06.341 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:33:06.342 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:37:26.595 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:26.595 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:26.600 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:26.600 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:26.604 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:37:26.604 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:37:27.917 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 400.7022 ms
24/05/24 19:37:28.246 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:37:28.265 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,016159 s
24/05/24 19:37:28.635 nioEventLoopGroup-2-2 INFO InMemoryFileIndex: It took 44 ms to list leaf files for 1 paths.
24/05/24 19:37:28.727 nioEventLoopGroup-2-2 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
24/05/24 19:37:29.006 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Pushed Filters: 
24/05/24 19:37:29.009 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#40, None)) > 0)
24/05/24 19:37:29.136 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 40.4206 ms
24/05/24 19:37:29.262 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 355.9 KiB, free 912.0 MiB)
24/05/24 19:37:29.517 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 911.9 MiB)
24/05/24 19:37:29.524 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 35.1 KiB, free: 912.3 MiB)
24/05/24 19:37:29.535 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
24/05/24 19:37:29.562 nioEventLoopGroup-2-2 INFO FileSourceScanExec: Planning scan with bin packing, max size: 51399661 bytes, open cost is considered as scanning 4194304 bytes.
24/05/24 19:37:29.701 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
24/05/24 19:37:29.732 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/05/24 19:37:29.733 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
24/05/24 19:37:29.734 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:29.736 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:29.745 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
24/05/24 19:37:29.809 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 911.9 MiB)
24/05/24 19:37:29.812 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 911.9 MiB)
24/05/24 19:37:29.813 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 6.0 KiB, free: 912.3 MiB)
24/05/24 19:37:29.814 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:29.835 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:29.838 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/05/24 19:37:29.911 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7975 bytes) 
24/05/24 19:37:29.926 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/05/24 19:37:30.303 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO FileScanRDD: Reading File path: file:///C:/Users/pedro/Desktop/Mestrado/MDLE/trabPratico/Fase2/mdle_data/out/merged_dataset.csv, range: 0-47205357, partition values: [empty row]
24/05/24 19:37:30.391 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO CodeGenerator: Code generated in 83.2622 ms
24/05/24 19:37:30.484 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2370 bytes result sent to driver
24/05/24 19:37:30.498 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 596 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:30.501 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/05/24 19:37:30.509 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0,730 s
24/05/24 19:37:30.514 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:30.515 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/05/24 19:37:30.516 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0,814315 s
24/05/24 19:37:30.548 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 13.5625 ms
24/05/24 19:37:30.650 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Pushed Filters: 
24/05/24 19:37:30.650 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Post-Scan Filters: 
24/05/24 19:37:30.668 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 355.9 KiB, free 911.6 MiB)
24/05/24 19:37:30.695 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 911.5 MiB)
24/05/24 19:37:30.698 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 35.1 KiB, free: 912.2 MiB)
24/05/24 19:37:30.699 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
24/05/24 19:37:30.700 nioEventLoopGroup-2-2 INFO FileSourceScanExec: Planning scan with bin packing, max size: 51399661 bytes, open cost is considered as scanning 4194304 bytes.
24/05/24 19:37:30.799 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
24/05/24 19:37:30.800 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/05/24 19:37:30.801 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
24/05/24 19:37:30.801 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:30.801 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:30.802 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
24/05/24 19:37:30.873 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.9 KiB, free 911.5 MiB)
24/05/24 19:37:30.876 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 911.5 MiB)
24/05/24 19:37:30.877 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 13.1 KiB, free: 912.2 MiB)
24/05/24 19:37:30.877 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:30.879 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[12] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:30.879 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/05/24 19:37:30.882 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7975 bytes) 
24/05/24 19:37:30.883 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/05/24 19:37:30.973 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO FileScanRDD: Reading File path: file:///C:/Users/pedro/Desktop/Mestrado/MDLE/trabPratico/Fase2/mdle_data/out/merged_dataset.csv, range: 0-47205357, partition values: [empty row]
24/05/24 19:37:31.539 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 6.0 KiB, free: 912.2 MiB)
24/05/24 19:37:31.562 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 35.1 KiB, free: 912.3 MiB)
24/05/24 19:37:34.310 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2058 bytes result sent to driver
24/05/24 19:37:34.314 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3433 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:34.314 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/05/24 19:37:34.316 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 3,512 s
24/05/24 19:37:34.316 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:34.317 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/05/24 19:37:34.319 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 3,520438 s
24/05/24 19:37:34.528 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:34.528 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:34.530 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:34.530 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:34.532 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:37:34.532 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:37:34.671 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/05/24 19:37:35.003 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Pushed Filters: 
24/05/24 19:37:35.004 nioEventLoopGroup-2-2 INFO FileSourceStrategy: Post-Scan Filters: 
24/05/24 19:37:35.464 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 15.48 ms
24/05/24 19:37:35.594 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 98.683 ms
24/05/24 19:37:35.603 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 355.8 KiB, free 911.5 MiB)
24/05/24 19:37:35.628 nioEventLoopGroup-2-2 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 911.5 MiB)
24/05/24 19:37:35.630 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 35.1 KiB, free: 912.2 MiB)
24/05/24 19:37:35.632 nioEventLoopGroup-2-2 INFO SparkContext: Created broadcast 4 from sql at NativeMethodAccessorImpl.java:0
24/05/24 19:37:35.650 nioEventLoopGroup-2-2 INFO FileSourceScanExec: Planning scan with bin packing, max size: 51399661 bytes, open cost is considered as scanning 4194304 bytes.
24/05/24 19:37:35.735 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 22 (sql at NativeMethodAccessorImpl.java:0) as input to shuffle 0
24/05/24 19:37:35.741 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 3 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/05/24 19:37:35.742 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:0)
24/05/24 19:37:35.742 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:35.744 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:35.746 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
24/05/24 19:37:35.788 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 54.4 KiB, free 911.4 MiB)
24/05/24 19:37:35.792 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 911.4 MiB)
24/05/24 19:37:35.793 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 18.7 KiB, free: 912.2 MiB)
24/05/24 19:37:35.794 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:35.796 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[22] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:35.796 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/05/24 19:37:35.800 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7964 bytes) 
24/05/24 19:37:35.801 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/05/24 19:37:35.856 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO FileScanRDD: Reading File path: file:///C:/Users/pedro/Desktop/Mestrado/MDLE/trabPratico/Fase2/mdle_data/out/merged_dataset.csv, range: 0-47205357, partition values: [empty row]
24/05/24 19:37:35.952 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 62.3789 ms
24/05/24 19:37:36.180 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_3_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 13.1 KiB, free: 912.2 MiB)
24/05/24 19:37:36.186 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_2_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 35.1 KiB, free: 912.2 MiB)
24/05/24 19:37:41.589 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_17_0 stored as values in memory (estimated size 27.3 MiB, free 884.5 MiB)
24/05/24 19:37:41.590 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_17_0 in memory on DESKTOP-LH06ASP:50776 (size: 27.3 MiB, free: 884.9 MiB)
24/05/24 19:37:41.605 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 5.389 ms
24/05/24 19:37:41.631 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 19.989 ms
24/05/24 19:37:41.741 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2215 bytes result sent to driver
24/05/24 19:37:41.747 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5949 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:41.748 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/05/24 19:37:41.752 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 2 (sql at NativeMethodAccessorImpl.java:0) finished in 6,002 s
24/05/24 19:37:41.753 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 19:37:41.754 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 19:37:41.755 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 19:37:41.755 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 19:37:41.838 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 25.2238 ms
24/05/24 19:37:41.874 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0
24/05/24 19:37:41.876 dag-scheduler-event-loop INFO DAGScheduler: Got job 4 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/05/24 19:37:41.876 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 4 (sql at NativeMethodAccessorImpl.java:0)
24/05/24 19:37:41.876 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
24/05/24 19:37:41.877 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:41.878 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[25] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents
24/05/24 19:37:41.889 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.1 KiB, free 884.5 MiB)
24/05/24 19:37:41.892 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 884.5 MiB)
24/05/24 19:37:41.893 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 5.8 KiB, free: 884.9 MiB)
24/05/24 19:37:41.895 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:41.895 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:41.896 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/05/24 19:37:41.903 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 19:37:41.903 Executor task launch worker for task 0.0 in stage 4.0 (TID 3) INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
24/05/24 19:37:41.951 Executor task launch worker for task 0.0 in stage 4.0 (TID 3) INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 19:37:41.954 Executor task launch worker for task 0.0 in stage 4.0 (TID 3) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms
24/05/24 19:37:41.983 Executor task launch worker for task 0.0 in stage 4.0 (TID 3) INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 3909 bytes result sent to driver
24/05/24 19:37:41.987 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 86 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:41.987 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/05/24 19:37:41.988 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 4 (sql at NativeMethodAccessorImpl.java:0) finished in 0,101 s
24/05/24 19:37:41.988 dag-scheduler-event-loop INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:41.989 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/05/24 19:37:41.989 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 4 finished: sql at NativeMethodAccessorImpl.java:0, took 0,114974 s
24/05/24 19:37:42.264 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 30 (collect at utils.scala:26) as input to shuffle 1
24/05/24 19:37:42.265 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 5 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:37:42.265 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at utils.scala:26)
24/05/24 19:37:42.265 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:42.266 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:42.267 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[30] at collect at utils.scala:26), which has no missing parents
24/05/24 19:37:42.283 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 54.4 KiB, free 884.5 MiB)
24/05/24 19:37:42.286 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 884.4 MiB)
24/05/24 19:37:42.287 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 18.8 KiB, free: 884.9 MiB)
24/05/24 19:37:42.288 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:42.288 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[30] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:42.289 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/05/24 19:37:42.291 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7964 bytes) 
24/05/24 19:37:42.292 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
24/05/24 19:37:42.300 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO BlockManager: Found block rdd_17_0 locally
24/05/24 19:37:42.331 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 2086 bytes result sent to driver
24/05/24 19:37:42.332 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 41 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:42.334 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:26) finished in 0,065 s
24/05/24 19:37:42.334 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 19:37:42.334 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 19:37:42.334 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 19:37:42.334 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 19:37:42.334 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/05/24 19:37:42.369 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:37:42.371 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:37:42.371 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:26)
24/05/24 19:37:42.372 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
24/05/24 19:37:42.372 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:42.373 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[33] at collect at utils.scala:26), which has no missing parents
24/05/24 19:37:42.376 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.1 KiB, free 884.4 MiB)
24/05/24 19:37:42.380 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 884.4 MiB)
24/05/24 19:37:42.381 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 5.8 KiB, free: 884.9 MiB)
24/05/24 19:37:42.382 dag-scheduler-event-loop INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:42.383 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[33] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:42.383 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/05/24 19:37:42.385 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 19:37:42.399 Executor task launch worker for task 0.0 in stage 7.0 (TID 5) INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
24/05/24 19:37:42.408 Executor task launch worker for task 0.0 in stage 7.0 (TID 5) INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 19:37:42.409 Executor task launch worker for task 0.0 in stage 7.0 (TID 5) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 19:37:42.415 Executor task launch worker for task 0.0 in stage 7.0 (TID 5) INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 3952 bytes result sent to driver
24/05/24 19:37:42.417 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 32 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:42.419 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (collect at utils.scala:26) finished in 0,045 s
24/05/24 19:37:42.419 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:42.417 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/05/24 19:37:42.420 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/05/24 19:37:42.420 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 6 finished: collect at utils.scala:26, took 0,050936 s
24/05/24 19:37:42.441 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 14.5733 ms
24/05/24 19:37:43.184 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 17.9351 ms
24/05/24 19:37:43.205 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:37:43.206 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:37:43.207 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:26)
24/05/24 19:37:43.207 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:43.207 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:43.208 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[36] at collect at utils.scala:26), which has no missing parents
24/05/24 19:37:43.212 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KiB, free 884.4 MiB)
24/05/24 19:37:43.218 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 884.4 MiB)
24/05/24 19:37:43.220 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 3.9 KiB, free: 884.9 MiB)
24/05/24 19:37:43.221 dag-scheduler-event-loop INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:43.222 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[36] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:43.222 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/05/24 19:37:43.225 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:37:43.226 Executor task launch worker for task 0.0 in stage 8.0 (TID 6) INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
24/05/24 19:37:43.234 Executor task launch worker for task 0.0 in stage 8.0 (TID 6) INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 1371 bytes result sent to driver
24/05/24 19:37:43.235 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 12 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:43.236 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/05/24 19:37:43.236 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 8 (collect at utils.scala:26) finished in 0,025 s
24/05/24 19:37:43.237 dag-scheduler-event-loop INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:43.237 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/05/24 19:37:43.237 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 7 finished: collect at utils.scala:26, took 0,031433 s
24/05/24 19:37:43.310 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 46.6451 ms
24/05/24 19:37:43.769 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:43.769 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:43.773 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:37:43.774 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:37:43.778 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:37:43.778 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:37:43.883 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 42.8759 ms
24/05/24 19:37:43.923 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.5368 ms
24/05/24 19:37:43.945 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.7801 ms
24/05/24 19:37:52.051 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:37:52.052 dag-scheduler-event-loop INFO DAGScheduler: Got job 8 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:37:52.053 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 9 (collect at utils.scala:26)
24/05/24 19:37:52.053 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:37:52.053 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:37:52.054 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[40] at collect at utils.scala:26), which has no missing parents
24/05/24 19:37:52.061 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 50.5 KiB, free 884.4 MiB)
24/05/24 19:37:52.063 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 884.4 MiB)
24/05/24 19:37:52.068 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 17.2 KiB, free: 884.9 MiB)
24/05/24 19:37:52.068 dag-scheduler-event-loop INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
24/05/24 19:37:52.069 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[40] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:37:52.069 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/05/24 19:37:52.071 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7975 bytes) 
24/05/24 19:37:52.071 Executor task launch worker for task 0.0 in stage 9.0 (TID 7) INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
24/05/24 19:37:52.079 Executor task launch worker for task 0.0 in stage 9.0 (TID 7) INFO BlockManager: Found block rdd_17_0 locally
24/05/24 19:37:52.140 Executor task launch worker for task 0.0 in stage 9.0 (TID 7) INFO CodeGenerator: Code generated in 47.997 ms
24/05/24 19:37:52.168 Executor task launch worker for task 0.0 in stage 9.0 (TID 7) INFO Executor: 1 block locks were not released by task 0.0 in stage 9.0 (TID 7)
[rdd_17_0]
24/05/24 19:37:52.170 Executor task launch worker for task 0.0 in stage 9.0 (TID 7) INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 2509 bytes result sent to driver
24/05/24 19:37:52.171 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 101 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:37:52.171 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/05/24 19:37:52.172 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 9 (collect at utils.scala:26) finished in 0,117 s
24/05/24 19:37:52.172 dag-scheduler-event-loop INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:37:52.173 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
24/05/24 19:37:52.173 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 8 finished: collect at utils.scala:26, took 0,121749 s
24/05/24 19:37:52.255 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 58.0258 ms
24/05/24 19:38:02.423 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_8_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 5.8 KiB, free: 884.9 MiB)
24/05/24 19:38:02.457 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_10_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 17.2 KiB, free: 884.9 MiB)
24/05/24 19:38:02.500 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_6_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 5.8 KiB, free: 884.9 MiB)
24/05/24 19:38:02.510 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_7_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 18.8 KiB, free: 884.9 MiB)
24/05/24 19:38:02.516 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_9_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 3.9 KiB, free: 884.9 MiB)
24/05/24 19:42:21.718 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:42:21.718 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:42:21.723 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:42:21.723 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:42:21.725 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:42:21.726 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:42:21.817 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:42:21.818 dag-scheduler-event-loop INFO DAGScheduler: Got job 9 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:42:21.818 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:26)
24/05/24 19:42:21.818 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:42:21.818 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:42:21.820 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at collect at utils.scala:26), which has no missing parents
24/05/24 19:42:21.822 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 7.1 KiB, free 884.5 MiB)
24/05/24 19:42:21.823 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 884.5 MiB)
24/05/24 19:42:21.824 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 3.7 KiB, free: 884.9 MiB)
24/05/24 19:42:21.825 dag-scheduler-event-loop INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
24/05/24 19:42:21.825 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:42:21.825 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
24/05/24 19:42:21.829 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) 
24/05/24 19:42:21.830 Executor task launch worker for task 0.0 in stage 10.0 (TID 8) INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
24/05/24 19:42:21.839 Executor task launch worker for task 0.0 in stage 10.0 (TID 8) INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1433 bytes result sent to driver
24/05/24 19:42:21.841 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 14 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:42:21.841 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/05/24 19:42:21.842 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 10 (collect at utils.scala:26) finished in 0,022 s
24/05/24 19:42:21.842 dag-scheduler-event-loop INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:42:21.842 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
24/05/24 19:42:21.843 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 9 finished: collect at utils.scala:26, took 0,024179 s
24/05/24 19:42:21.854 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.7778 ms
24/05/24 19:42:42.700 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_5_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 18.7 KiB, free: 885.0 MiB)
24/05/24 19:42:42.717 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_11_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 3.7 KiB, free: 885.0 MiB)
24/05/24 19:42:55.920 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:42:55.922 dag-scheduler-event-loop INFO DAGScheduler: Got job 10 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:42:55.922 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 11 (collect at utils.scala:26)
24/05/24 19:42:55.922 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:42:55.922 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:42:55.922 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[46] at collect at utils.scala:26), which has no missing parents
24/05/24 19:42:55.925 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 9.4 KiB, free 884.6 MiB)
24/05/24 19:42:55.929 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 884.6 MiB)
24/05/24 19:42:55.930 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 3.9 KiB, free: 885.0 MiB)
24/05/24 19:42:55.930 dag-scheduler-event-loop INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
24/05/24 19:42:55.931 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[46] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:42:55.931 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/05/24 19:42:55.932 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:42:55.933 Executor task launch worker for task 0.0 in stage 11.0 (TID 9) INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
24/05/24 19:42:55.938 Executor task launch worker for task 0.0 in stage 11.0 (TID 9) INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 1371 bytes result sent to driver
24/05/24 19:42:55.939 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 7 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:42:55.939 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/05/24 19:42:55.941 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 11 (collect at utils.scala:26) finished in 0,018 s
24/05/24 19:42:55.941 dag-scheduler-event-loop INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:42:55.941 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
24/05/24 19:42:55.941 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 10 finished: collect at utils.scala:26, took 0,020930 s
24/05/24 19:42:56.614 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:42:56.615 dag-scheduler-event-loop INFO DAGScheduler: Got job 11 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:42:56.615 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 12 (collect at utils.scala:26)
24/05/24 19:42:56.616 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:42:56.616 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:42:56.617 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[48] at collect at utils.scala:26), which has no missing parents
24/05/24 19:42:56.620 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.4 KiB, free 884.6 MiB)
24/05/24 19:42:56.623 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 884.6 MiB)
24/05/24 19:42:56.627 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 3.9 KiB, free: 884.9 MiB)
24/05/24 19:42:56.628 dag-scheduler-event-loop INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
24/05/24 19:42:56.628 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[48] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:42:56.629 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
24/05/24 19:42:56.631 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 10) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:42:56.632 Executor task launch worker for task 0.0 in stage 12.0 (TID 10) INFO Executor: Running task 0.0 in stage 12.0 (TID 10)
24/05/24 19:42:56.637 Executor task launch worker for task 0.0 in stage 12.0 (TID 10) INFO Executor: Finished task 0.0 in stage 12.0 (TID 10). 1371 bytes result sent to driver
24/05/24 19:42:56.639 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 10) in 9 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:42:56.639 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/05/24 19:42:56.640 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 12 (collect at utils.scala:26) finished in 0,022 s
24/05/24 19:42:56.641 dag-scheduler-event-loop INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:42:56.641 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
24/05/24 19:42:56.641 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 11 finished: collect at utils.scala:26, took 0,027170 s
24/05/24 19:42:57.238 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:42:57.238 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:42:57.252 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:42:57.253 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:42:57.257 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:42:57.258 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:47:24.726 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 32.1273 ms
24/05/24 19:47:24.775 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:47:24.776 dag-scheduler-event-loop INFO DAGScheduler: Got job 12 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:47:24.776 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:26)
24/05/24 19:47:24.776 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:47:24.777 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:47:24.777 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:26), which has no missing parents
24/05/24 19:47:24.783 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 34.5 KiB, free 884.5 MiB)
24/05/24 19:47:24.787 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 884.5 MiB)
24/05/24 19:47:24.787 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 11.8 KiB, free: 884.9 MiB)
24/05/24 19:47:24.788 dag-scheduler-event-loop INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
24/05/24 19:47:24.788 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:47:24.789 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
24/05/24 19:47:24.923 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_13_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 3.9 KiB, free: 884.9 MiB)
24/05/24 19:47:24.928 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_12_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 3.9 KiB, free: 884.9 MiB)
24/05/24 19:47:25.043 dispatcher-event-loop-0 WARN TaskSetManager: Stage 13 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:47:25.043 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:47:25.044 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
24/05/24 19:47:27.720 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO MemoryStore: Block rdd_51_0 stored as values in memory (estimated size 28.8 MiB, free 855.8 MiB)
24/05/24 19:47:27.720 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_51_0 in memory on DESKTOP-LH06ASP:50776 (size: 28.8 MiB, free: 856.1 MiB)
24/05/24 19:47:27.754 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO CodeGenerator: Code generated in 30.5234 ms
24/05/24 19:47:27.758 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO Executor: 1 block locks were not released by task 0.0 in stage 13.0 (TID 11)
[rdd_51_0]
24/05/24 19:47:27.759 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 2532 bytes result sent to driver
24/05/24 19:47:27.760 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 2970 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:47:27.761 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/05/24 19:47:27.761 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 13 (collect at utils.scala:26) finished in 2,983 s
24/05/24 19:47:27.762 dag-scheduler-event-loop INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:47:27.762 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
24/05/24 19:47:27.762 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 12 finished: collect at utils.scala:26, took 2,987587 s
24/05/24 19:47:27.815 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 40.0595 ms
24/05/24 19:47:30.599 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:47:30.600 dag-scheduler-event-loop INFO DAGScheduler: Got job 13 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:47:30.600 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:26)
24/05/24 19:47:30.600 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:47:30.601 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:47:30.602 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at collect at utils.scala:26), which has no missing parents
24/05/24 19:47:30.608 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 50.5 KiB, free 855.7 MiB)
24/05/24 19:47:30.609 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 855.7 MiB)
24/05/24 19:47:30.610 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 17.1 KiB, free: 856.1 MiB)
24/05/24 19:47:30.610 dag-scheduler-event-loop INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
24/05/24 19:47:30.611 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:47:30.611 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/05/24 19:47:30.612 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7975 bytes) 
24/05/24 19:47:30.613 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
24/05/24 19:47:30.621 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO BlockManager: Found block rdd_17_0 locally
24/05/24 19:47:30.625 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO Executor: 1 block locks were not released by task 0.0 in stage 14.0 (TID 12)
[rdd_17_0]
24/05/24 19:47:30.627 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 2466 bytes result sent to driver
24/05/24 19:47:30.629 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 16 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:47:30.629 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/05/24 19:47:30.629 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 14 (collect at utils.scala:26) finished in 0,026 s
24/05/24 19:47:30.630 dag-scheduler-event-loop INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:47:30.630 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/05/24 19:47:30.630 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 13 finished: collect at utils.scala:26, took 0,030941 s
24/05/24 19:47:32.706 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:47:32.707 dag-scheduler-event-loop INFO DAGScheduler: Got job 14 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:47:32.707 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 15 (collect at utils.scala:26)
24/05/24 19:47:32.707 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:47:32.707 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:47:32.708 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at collect at utils.scala:26), which has no missing parents
24/05/24 19:47:32.713 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 34.5 KiB, free 855.7 MiB)
24/05/24 19:47:32.714 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 855.6 MiB)
24/05/24 19:47:32.716 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on DESKTOP-LH06ASP:50776 (size: 11.8 KiB, free: 856.1 MiB)
24/05/24 19:47:32.716 dag-scheduler-event-loop INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
24/05/24 19:47:32.717 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:47:32.717 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
24/05/24 19:47:32.768 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_15_piece0 on DESKTOP-LH06ASP:50776 in memory (size: 17.1 KiB, free: 856.1 MiB)
24/05/24 19:47:32.900 dispatcher-event-loop-0 WARN TaskSetManager: Stage 15 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:47:32.900 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:47:32.946 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
24/05/24 19:47:33.018 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO BlockManager: Found block rdd_51_0 locally
24/05/24 19:47:33.023 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO Executor: 1 block locks were not released by task 0.0 in stage 15.0 (TID 13)
[rdd_51_0]
24/05/24 19:47:33.024 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 2575 bytes result sent to driver
24/05/24 19:47:33.025 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 307 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:47:33.025 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/05/24 19:47:33.027 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 15 (collect at utils.scala:26) finished in 0,317 s
24/05/24 19:47:33.027 dag-scheduler-event-loop INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:47:33.027 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
24/05/24 19:47:33.027 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 14 finished: collect at utils.scala:26, took 0,320994 s
24/05/24 19:48:17.217 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/05/24 19:48:17.218 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/05/24 19:48:17.249 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/05/24 19:48:17.292 dispatcher-event-loop-1 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/05/24 19:48:17.433 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/05/24 19:48:17.434 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/05/24 19:48:17.444 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/05/24 19:48:17.453 dispatcher-event-loop-0 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/05/24 19:48:17.461 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 19:48:17.463 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/05/24 19:48:17.464 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/05/24 19:48:17.465 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-8a1f046b-f8ba-4d65-b5ee-da792194d7bd
24/05/24 19:48:17.469 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7
24/05/24 19:48:17.477 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 19:48:17.478 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722
24/05/24 19:48:17.483 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-16e59c6c-fc60-4cb9-8291-c4b0585d2dd7\userFiles-6e4d2d50-b6f6-4c75-97ba-6f7e69c77722\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 19:48:32.720 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/05/24 19:48:33.131 nioEventLoopGroup-2-2 INFO SparkContext: Running Spark version 3.4.2
24/05/24 19:48:33.198 nioEventLoopGroup-2-2 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/05/24 19:48:33.359 nioEventLoopGroup-2-2 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
24/05/24 19:48:33.508 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/05/24 19:48:33.509 nioEventLoopGroup-2-2 INFO ResourceUtils: No custom resources configured for spark.driver.
24/05/24 19:48:33.509 nioEventLoopGroup-2-2 INFO ResourceUtils: ==============================================================
24/05/24 19:48:33.511 nioEventLoopGroup-2-2 INFO SparkContext: Submitted application: sparklyr
24/05/24 19:48:33.576 nioEventLoopGroup-2-2 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/05/24 19:48:33.602 nioEventLoopGroup-2-2 INFO ResourceProfile: Limiting resource is cpu
24/05/24 19:48:33.603 nioEventLoopGroup-2-2 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/05/24 19:48:33.718 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls to: pedro
24/05/24 19:48:33.719 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls to: pedro
24/05/24 19:48:33.720 nioEventLoopGroup-2-2 INFO SecurityManager: Changing view acls groups to: 
24/05/24 19:48:33.721 nioEventLoopGroup-2-2 INFO SecurityManager: Changing modify acls groups to: 
24/05/24 19:48:33.722 nioEventLoopGroup-2-2 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: pedro; groups with view permissions: EMPTY; users with modify permissions: pedro; groups with modify permissions: EMPTY
24/05/24 19:48:33.908 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'sparkDriver' on port 51444.
24/05/24 19:48:33.958 nioEventLoopGroup-2-2 INFO SparkEnv: Registering MapOutputTracker
24/05/24 19:48:34.027 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMaster
24/05/24 19:48:34.091 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/05/24 19:48:34.093 nioEventLoopGroup-2-2 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/05/24 19:48:34.102 nioEventLoopGroup-2-2 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/05/24 19:48:34.154 nioEventLoopGroup-2-2 INFO DiskBlockManager: Created local directory at C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\blockmgr-e87bd28a-4a1a-46ca-b247-d5fa9f0e2e98
24/05/24 19:48:34.186 nioEventLoopGroup-2-2 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
24/05/24 19:48:34.213 nioEventLoopGroup-2-2 INFO SparkEnv: Registering OutputCommitCoordinator
24/05/24 19:48:34.218 nioEventLoopGroup-2-2 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local]. Please check your configured local directories.
24/05/24 19:48:34.715 nioEventLoopGroup-2-2 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/05/24 19:48:34.970 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/05/24 19:48:35.079 nioEventLoopGroup-2-2 INFO SparkContext: Added JAR file:/C:/Users/pedro/AppData/Local/R/win-library/4.3/sparklyr/java/sparklyr-3.0-2.12.jar at spark://DESKTOP-LH06ASP:51444/jars/sparklyr-3.0-2.12.jar with timestamp 1716576513108
24/05/24 19:48:35.188 nioEventLoopGroup-2-2 INFO Executor: Starting executor ID driver on host DESKTOP-LH06ASP
24/05/24 19:48:35.200 nioEventLoopGroup-2-2 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/05/24 19:48:35.218 nioEventLoopGroup-2-2 INFO Executor: Fetching spark://DESKTOP-LH06ASP:51444/jars/sparklyr-3.0-2.12.jar with timestamp 1716576513108
24/05/24 19:48:35.274 nioEventLoopGroup-2-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51444 after 27 ms (0 ms spent in bootstraps)
24/05/24 19:48:35.291 nioEventLoopGroup-2-2 INFO Utils: Fetching spark://DESKTOP-LH06ASP:51444/jars/sparklyr-3.0-2.12.jar to C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68\fetchFileTemp2146375109462054833.tmp
24/05/24 19:48:35.715 nioEventLoopGroup-2-2 INFO Executor: Adding file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/local/spark-13a2f283-95c4-48d5-8486-b2607096219e/userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68/sparklyr-3.0-2.12.jar to class loader
24/05/24 19:48:35.746 nioEventLoopGroup-2-2 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51464.
24/05/24 19:48:35.746 nioEventLoopGroup-2-2 INFO NettyBlockTransferService: Server created on DESKTOP-LH06ASP:51464
24/05/24 19:48:35.750 nioEventLoopGroup-2-2 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/05/24 19:48:35.770 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51464, None)
24/05/24 19:48:35.779 dispatcher-BlockManagerMaster INFO BlockManagerMasterEndpoint: Registering block manager DESKTOP-LH06ASP:51464 with 912.3 MiB RAM, BlockManagerId(driver, DESKTOP-LH06ASP, 51464, None)
24/05/24 19:48:35.787 nioEventLoopGroup-2-2 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, DESKTOP-LH06ASP, 51464, None)
24/05/24 19:48:35.790 nioEventLoopGroup-2-2 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, DESKTOP-LH06ASP, 51464, None)
24/05/24 19:48:36.262 nioEventLoopGroup-2-2 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive') to the value of spark.sql.warehouse.dir.
24/05/24 19:48:36.281 nioEventLoopGroup-2-2 INFO SharedState: Warehouse path is 'file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive'.
24/05/24 19:48:44.385 nioEventLoopGroup-2-2 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/05/24 19:48:44.707 nioEventLoopGroup-2-2 INFO HiveConf: Found configuration file file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/conf/hive-site.xml
24/05/24 19:48:45.316 nioEventLoopGroup-2-2 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/C:/Users/pedro/AppData/Local/spark/spark-3.4.2-bin-hadoop3/tmp/hive
24/05/24 19:48:45.867 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/05/24 19:48:45.868 nioEventLoopGroup-2-2 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/05/24 19:48:45.869 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/05/24 19:48:46.024 nioEventLoopGroup-2-2 INFO ObjectStore: ObjectStore, initialize called
24/05/24 19:48:46.268 nioEventLoopGroup-2-2 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/05/24 19:48:46.270 nioEventLoopGroup-2-2 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/05/24 19:48:49.237 nioEventLoopGroup-2-2 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/05/24 19:48:51.284 nioEventLoopGroup-2-2 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/05/24 19:48:51.288 nioEventLoopGroup-2-2 INFO ObjectStore: Initialized ObjectStore
24/05/24 19:48:51.444 nioEventLoopGroup-2-2 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/05/24 19:48:51.444 nioEventLoopGroup-2-2 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.59.1
24/05/24 19:48:51.536 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/05/24 19:48:51.751 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added admin role in metastore
24/05/24 19:48:51.754 nioEventLoopGroup-2-2 INFO HiveMetaStore: Added public role in metastore
24/05/24 19:48:51.898 nioEventLoopGroup-2-2 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/05/24 19:48:52.399 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:48:52.404 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:48:52.461 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: global_temp
24/05/24 19:48:52.461 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/05/24 19:48:52.464 nioEventLoopGroup-2-2 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/05/24 19:48:52.465 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:48:52.466 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:48:52.472 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:48:52.472 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:48:52.477 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:48:52.478 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:50:10.358 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:50:10.359 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:50:10.364 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:50:10.365 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:50:10.368 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:50:10.369 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:50:11.849 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 468.7415 ms
24/05/24 19:50:12.176 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:50:12.192 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 0 finished: collect at utils.scala:26, took 0,013546 s
24/05/24 19:50:26.120 nioEventLoopGroup-2-2 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
24/05/24 19:50:26.684 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 34.1842 ms
24/05/24 19:50:26.707 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:50:26.730 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:50:26.731 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:26)
24/05/24 19:50:26.731 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:50:26.732 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:50:26.737 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at collect at utils.scala:26), which has no missing parents
24/05/24 19:50:26.831 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 9.4 KiB, free 912.3 MiB)
24/05/24 19:50:26.933 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 912.3 MiB)
24/05/24 19:50:26.938 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 912.3 MiB)
24/05/24 19:50:26.945 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/05/24 19:50:26.965 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:50:26.967 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/05/24 19:50:27.059 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:50:27.081 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/05/24 19:50:27.483 Executor task launch worker for task 0.0 in stage 0.0 (TID 0) INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1543 bytes result sent to driver
24/05/24 19:50:27.494 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 463 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:50:27.497 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/05/24 19:50:27.504 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (collect at utils.scala:26) finished in 0,744 s
24/05/24 19:50:27.509 dag-scheduler-event-loop INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:50:27.510 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/05/24 19:50:27.511 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 1 finished: collect at utils.scala:26, took 0,803326 s
24/05/24 19:50:27.647 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 82.8733 ms
24/05/24 19:50:28.880 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:50:28.882 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:50:28.882 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (collect at utils.scala:26)
24/05/24 19:50:28.882 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:50:28.882 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:50:28.884 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at collect at utils.scala:26), which has no missing parents
24/05/24 19:50:28.921 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.4 KiB, free 912.3 MiB)
24/05/24 19:50:28.926 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 912.3 MiB)
24/05/24 19:50:28.928 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 912.3 MiB)
24/05/24 19:50:28.929 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/05/24 19:50:28.931 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:50:28.932 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/05/24 19:50:28.936 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:50:28.938 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/05/24 19:50:28.955 Executor task launch worker for task 0.0 in stage 1.0 (TID 1) INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1371 bytes result sent to driver
24/05/24 19:50:28.962 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 27 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:50:28.964 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/05/24 19:50:28.966 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (collect at utils.scala:26) finished in 0,079 s
24/05/24 19:50:28.967 dag-scheduler-event-loop INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:50:28.967 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/05/24 19:50:28.969 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 2 finished: collect at utils.scala:26, took 0,087211 s
24/05/24 19:50:29.738 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:50:29.739 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:50:29.742 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:50:29.743 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:50:29.746 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:50:29.747 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:50:29.849 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 55.7852 ms
24/05/24 19:50:29.894 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 11.1057 ms
24/05/24 19:50:29.913 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.1853 ms
24/05/24 19:51:04.117 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:51:04.117 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:51:04.120 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:51:04.122 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:51:04.123 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:51:04.124 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:51:30.977 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 48.4614 ms
24/05/24 19:51:31.027 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:51:31.029 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:51:31.029 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:26)
24/05/24 19:51:31.029 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:51:31.035 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:51:31.037 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at collect at utils.scala:26), which has no missing parents
24/05/24 19:51:31.069 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 34.5 KiB, free 912.2 MiB)
24/05/24 19:51:31.071 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.7 KiB, free 912.2 MiB)
24/05/24 19:51:31.073 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 11.7 KiB, free: 912.3 MiB)
24/05/24 19:51:31.074 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/05/24 19:51:31.075 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:51:31.075 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/05/24 19:51:31.463 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_1_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 912.3 MiB)
24/05/24 19:51:31.471 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_0_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 912.3 MiB)
24/05/24 19:51:31.483 dispatcher-event-loop-1 WARN TaskSetManager: Stage 2 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:51:31.484 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:51:31.485 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/05/24 19:51:37.628 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO MemoryStore: Block rdd_11_0 stored as values in memory (estimated size 28.8 MiB, free 883.5 MiB)
24/05/24 19:51:37.633 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_11_0 in memory on DESKTOP-LH06ASP:51464 (size: 28.8 MiB, free: 883.5 MiB)
24/05/24 19:51:37.799 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 145.3009 ms
24/05/24 19:51:37.968 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO CodeGenerator: Code generated in 144.1986 ms
24/05/24 19:51:38.011 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: 1 block locks were not released by task 0.0 in stage 2.0 (TID 2)
[rdd_11_0]
24/05/24 19:51:38.017 Executor task launch worker for task 0.0 in stage 2.0 (TID 2) INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2575 bytes result sent to driver
24/05/24 19:51:38.020 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 6945 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:51:38.021 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/05/24 19:51:38.023 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (collect at utils.scala:26) finished in 6,985 s
24/05/24 19:51:38.024 dag-scheduler-event-loop INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:51:38.024 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/05/24 19:51:38.025 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 3 finished: collect at utils.scala:26, took 6,997120 s
24/05/24 19:51:38.169 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 103.5954 ms
24/05/24 19:51:58.838 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 19.8045 ms
24/05/24 19:51:58.905 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 17 (collect at utils.scala:26) as input to shuffle 0
24/05/24 19:51:58.917 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 4 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:51:58.918 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 3 (collect at utils.scala:26)
24/05/24 19:51:58.919 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:51:58.920 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:51:58.923 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at collect at utils.scala:26), which has no missing parents
24/05/24 19:51:58.968 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.9 KiB, free 883.4 MiB)
24/05/24 19:51:58.992 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.4 MiB)
24/05/24 19:51:58.996 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 19:51:58.996 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
24/05/24 19:51:59.001 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:51:59.002 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/05/24 19:51:59.831 dispatcher-event-loop-1 WARN TaskSetManager: Stage 3 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:51:59.832 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 19:51:59.833 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/05/24 19:52:00.069 Executor task launch worker for task 0.0 in stage 3.0 (TID 3) INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1888 bytes result sent to driver
24/05/24 19:52:00.075 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1069 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:52:00.075 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/05/24 19:52:00.079 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 3 (collect at utils.scala:26) finished in 1,149 s
24/05/24 19:52:00.080 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 19:52:00.081 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 19:52:00.082 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 19:52:00.083 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 19:52:00.179 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 25.4392 ms
24/05/24 19:52:00.263 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:52:00.265 dag-scheduler-event-loop INFO DAGScheduler: Got job 5 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:52:00.266 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 5 (collect at utils.scala:26)
24/05/24 19:52:00.266 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
24/05/24 19:52:00.266 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:52:00.267 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at collect at utils.scala:26), which has no missing parents
24/05/24 19:52:00.280 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.7 KiB, free 883.4 MiB)
24/05/24 19:52:00.283 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.4 MiB)
24/05/24 19:52:00.284 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 19:52:00.285 dag-scheduler-event-loop INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
24/05/24 19:52:00.286 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:52:00.286 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/05/24 19:52:00.293 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 19:52:00.294 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
24/05/24 19:52:00.340 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 19:52:00.342 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
24/05/24 19:52:00.368 Executor task launch worker for task 0.0 in stage 5.0 (TID 4) INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 3952 bytes result sent to driver
24/05/24 19:52:00.370 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 79 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:52:00.370 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/05/24 19:52:00.371 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 5 (collect at utils.scala:26) finished in 0,094 s
24/05/24 19:52:00.371 dag-scheduler-event-loop INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:52:00.371 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/05/24 19:52:00.372 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 5 finished: collect at utils.scala:26, took 0,108751 s
24/05/24 19:52:00.383 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 7.1222 ms
24/05/24 19:52:37.411 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:52:37.412 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:52:37.420 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:52:37.421 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:52:37.423 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:52:37.424 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:52:39.263 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:52:39.265 dag-scheduler-event-loop INFO DAGScheduler: Got job 6 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:52:39.265 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:26)
24/05/24 19:52:39.265 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:52:39.266 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:52:39.268 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:26), which has no missing parents
24/05/24 19:52:39.277 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 34.5 KiB, free 883.4 MiB)
24/05/24 19:52:39.279 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 11.7 KiB, free 883.4 MiB)
24/05/24 19:52:39.281 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 11.7 KiB, free: 883.5 MiB)
24/05/24 19:52:39.281 dag-scheduler-event-loop INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
24/05/24 19:52:39.282 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[24] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:52:39.282 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/05/24 19:52:39.518 dispatcher-event-loop-1 WARN TaskSetManager: Stage 6 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:52:39.518 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:52:39.519 Executor task launch worker for task 0.0 in stage 6.0 (TID 5) INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
24/05/24 19:52:39.575 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_4_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 19:52:39.615 Executor task launch worker for task 0.0 in stage 6.0 (TID 5) INFO BlockManager: Found block rdd_11_0 locally
24/05/24 19:52:39.620 Executor task launch worker for task 0.0 in stage 6.0 (TID 5) INFO Executor: 1 block locks were not released by task 0.0 in stage 6.0 (TID 5)
[rdd_11_0]
24/05/24 19:52:39.621 Executor task launch worker for task 0.0 in stage 6.0 (TID 5) INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 2618 bytes result sent to driver
24/05/24 19:52:39.623 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 338 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:52:39.623 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/05/24 19:52:39.624 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 6 (collect at utils.scala:26) finished in 0,355 s
24/05/24 19:52:39.624 dag-scheduler-event-loop INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:52:39.624 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/05/24 19:52:39.624 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 6 finished: collect at utils.scala:26, took 0,361119 s
24/05/24 19:53:03.328 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 41.1741 ms
24/05/24 19:53:03.343 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:53:03.344 dag-scheduler-event-loop INFO DAGScheduler: Got job 7 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:53:03.345 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:26)
24/05/24 19:53:03.345 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:53:03.346 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:53:03.348 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[26] at collect at utils.scala:26), which has no missing parents
24/05/24 19:53:03.351 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.3 KiB, free 883.4 MiB)
24/05/24 19:53:03.354 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.4 MiB)
24/05/24 19:53:03.355 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 19:53:03.356 dag-scheduler-event-loop INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
24/05/24 19:53:03.357 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[26] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:53:03.357 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/05/24 19:53:03.539 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_5_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 11.7 KiB, free: 883.5 MiB)
24/05/24 19:53:03.643 dispatcher-event-loop-0 WARN TaskSetManager: Stage 7 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:53:03.644 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:53:03.645 Executor task launch worker for task 0.0 in stage 7.0 (TID 6) INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
24/05/24 19:53:04.732 Executor task launch worker for task 0.0 in stage 7.0 (TID 6) INFO MemoryStore: Block taskresult_6 stored as bytes in memory (estimated size 13.2 MiB, free 870.2 MiB)
24/05/24 19:53:04.734 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_6 in memory on DESKTOP-LH06ASP:51464 (size: 13.2 MiB, free: 870.3 MiB)
24/05/24 19:53:04.735 Executor task launch worker for task 0.0 in stage 7.0 (TID 6) INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 13828710 bytes result sent via BlockManager)
24/05/24 19:53:04.783 task-result-getter-2 INFO TransportClientFactory: Successfully created connection to DESKTOP-LH06ASP/192.168.59.1:51464 after 7 ms (0 ms spent in bootstraps)
24/05/24 19:53:04.948 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 1590 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:53:04.948 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/05/24 19:53:04.951 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 7 (collect at utils.scala:26) finished in 1,600 s
24/05/24 19:53:04.951 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_6 on DESKTOP-LH06ASP:51464 in memory (size: 13.2 MiB, free: 883.5 MiB)
24/05/24 19:53:04.951 dag-scheduler-event-loop INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:53:04.952 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/05/24 19:53:04.952 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 7 finished: collect at utils.scala:26, took 1,608254 s
24/05/24 19:53:05.130 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 48.6171 ms
24/05/24 19:53:08.548 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_2_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 11.7 KiB, free: 883.5 MiB)
24/05/24 19:53:08.564 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_6_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 19:53:08.575 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_3_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 19:55:15.546 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:55:15.546 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:55:15.551 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:55:15.552 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:55:15.553 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:55:15.554 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:55:15.645 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:55:15.646 dag-scheduler-event-loop INFO DAGScheduler: Got job 8 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:55:15.646 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:26)
24/05/24 19:55:15.646 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:55:15.647 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:55:15.648 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[29] at collect at utils.scala:26), which has no missing parents
24/05/24 19:55:15.651 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 7.1 KiB, free 883.5 MiB)
24/05/24 19:55:15.658 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 883.5 MiB)
24/05/24 19:55:15.659 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 19:55:15.660 dag-scheduler-event-loop INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
24/05/24 19:55:15.662 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[29] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:55:15.662 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/05/24 19:55:15.664 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 7) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7559 bytes) 
24/05/24 19:55:15.666 Executor task launch worker for task 0.0 in stage 8.0 (TID 7) INFO Executor: Running task 0.0 in stage 8.0 (TID 7)
24/05/24 19:55:15.675 Executor task launch worker for task 0.0 in stage 8.0 (TID 7) INFO Executor: Finished task 0.0 in stage 8.0 (TID 7). 1385 bytes result sent to driver
24/05/24 19:55:15.676 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 7) in 13 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:55:15.676 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/05/24 19:55:15.677 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 8 (collect at utils.scala:26) finished in 0,028 s
24/05/24 19:55:15.678 dag-scheduler-event-loop INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:55:15.678 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/05/24 19:55:15.678 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 8 finished: collect at utils.scala:26, took 0,033401 s
24/05/24 19:55:15.692 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 9.9461 ms
24/05/24 19:55:27.110 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_7_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 19:55:32.611 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:55:32.612 dag-scheduler-event-loop INFO DAGScheduler: Got job 9 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:55:32.612 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 9 (collect at utils.scala:26)
24/05/24 19:55:32.612 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:55:32.612 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:55:32.614 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[32] at collect at utils.scala:26), which has no missing parents
24/05/24 19:55:32.617 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 19:55:32.620 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 19:55:32.620 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 19:55:32.621 dag-scheduler-event-loop INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
24/05/24 19:55:32.621 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[32] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:55:32.622 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/05/24 19:55:32.623 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 8) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:55:32.624 Executor task launch worker for task 0.0 in stage 9.0 (TID 8) INFO Executor: Running task 0.0 in stage 9.0 (TID 8)
24/05/24 19:55:32.629 Executor task launch worker for task 0.0 in stage 9.0 (TID 8) INFO Executor: Finished task 0.0 in stage 9.0 (TID 8). 1371 bytes result sent to driver
24/05/24 19:55:32.631 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 8) in 8 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:55:32.631 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/05/24 19:55:32.632 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 9 (collect at utils.scala:26) finished in 0,017 s
24/05/24 19:55:32.633 dag-scheduler-event-loop INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:55:32.633 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
24/05/24 19:55:32.633 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 9 finished: collect at utils.scala:26, took 0,021745 s
24/05/24 19:55:33.294 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:55:33.295 dag-scheduler-event-loop INFO DAGScheduler: Got job 10 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:55:33.295 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 10 (collect at utils.scala:26)
24/05/24 19:55:33.295 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:55:33.295 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:55:33.298 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[34] at collect at utils.scala:26), which has no missing parents
24/05/24 19:55:33.301 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 19:55:33.305 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 19:55:33.306 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 19:55:33.307 dag-scheduler-event-loop INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
24/05/24 19:55:33.308 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[34] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:55:33.308 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
24/05/24 19:55:33.309 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 9) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 19:55:33.310 Executor task launch worker for task 0.0 in stage 10.0 (TID 9) INFO Executor: Running task 0.0 in stage 10.0 (TID 9)
24/05/24 19:55:33.317 Executor task launch worker for task 0.0 in stage 10.0 (TID 9) INFO Executor: Finished task 0.0 in stage 10.0 (TID 9). 1371 bytes result sent to driver
24/05/24 19:55:33.318 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 9) in 9 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:55:33.318 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/05/24 19:55:33.319 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 10 (collect at utils.scala:26) finished in 0,020 s
24/05/24 19:55:33.319 dag-scheduler-event-loop INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:55:33.320 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
24/05/24 19:55:33.320 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 10 finished: collect at utils.scala:26, took 0,026022 s
24/05/24 19:55:33.835 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:55:33.836 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:55:33.841 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 19:55:33.842 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 19:55:33.844 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 19:55:33.844 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 19:56:27.310 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 36 (collect at utils.scala:26) as input to shuffle 1
24/05/24 19:56:27.311 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 11 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:56:27.311 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 11 (collect at utils.scala:26)
24/05/24 19:56:27.312 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:56:27.312 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:56:27.316 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[36] at collect at utils.scala:26), which has no missing parents
24/05/24 19:56:27.324 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.9 KiB, free 883.5 MiB)
24/05/24 19:56:27.330 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.5 MiB)
24/05/24 19:56:27.332 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 19:56:27.334 dag-scheduler-event-loop INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
24/05/24 19:56:27.335 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[36] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:56:27.336 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/05/24 19:56:28.293 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_9_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 19:56:28.299 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_8_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 19:56:28.324 dispatcher-event-loop-1 WARN TaskSetManager: Stage 11 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:56:28.324 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 19:56:28.325 Executor task launch worker for task 0.0 in stage 11.0 (TID 10) INFO Executor: Running task 0.0 in stage 11.0 (TID 10)
24/05/24 19:56:28.438 Executor task launch worker for task 0.0 in stage 11.0 (TID 10) INFO Executor: Finished task 0.0 in stage 11.0 (TID 10). 1802 bytes result sent to driver
24/05/24 19:56:28.439 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 1101 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:56:28.439 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/05/24 19:56:28.441 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 11 (collect at utils.scala:26) finished in 1,124 s
24/05/24 19:56:28.441 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 19:56:28.442 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 19:56:28.442 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 19:56:28.442 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 19:56:28.470 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:56:28.472 dag-scheduler-event-loop INFO DAGScheduler: Got job 12 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:56:28.472 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 13 (collect at utils.scala:26)
24/05/24 19:56:28.472 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
24/05/24 19:56:28.472 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:56:28.473 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[39] at collect at utils.scala:26), which has no missing parents
24/05/24 19:56:28.477 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.7 KiB, free 883.5 MiB)
24/05/24 19:56:28.480 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.5 MiB)
24/05/24 19:56:28.482 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 19:56:28.483 dag-scheduler-event-loop INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
24/05/24 19:56:28.483 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[39] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:56:28.484 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
24/05/24 19:56:28.485 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 11) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 19:56:28.485 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO Executor: Running task 0.0 in stage 13.0 (TID 11)
24/05/24 19:56:28.492 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 19:56:28.492 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
24/05/24 19:56:28.496 Executor task launch worker for task 0.0 in stage 13.0 (TID 11) INFO Executor: Finished task 0.0 in stage 13.0 (TID 11). 3952 bytes result sent to driver
24/05/24 19:56:28.498 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 11) in 13 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:56:28.498 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/05/24 19:56:28.499 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 13 (collect at utils.scala:26) finished in 0,025 s
24/05/24 19:56:28.500 dag-scheduler-event-loop INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:56:28.500 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
24/05/24 19:56:28.500 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 12 finished: collect at utils.scala:26, took 0,029610 s
24/05/24 19:57:27.271 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 19:57:27.273 dag-scheduler-event-loop INFO DAGScheduler: Got job 13 (collect at utils.scala:26) with 1 output partitions
24/05/24 19:57:27.273 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 14 (collect at utils.scala:26)
24/05/24 19:57:27.273 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 19:57:27.273 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 19:57:27.274 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[41] at collect at utils.scala:26), which has no missing parents
24/05/24 19:57:27.277 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 20.2 KiB, free 883.4 MiB)
24/05/24 19:57:27.286 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.4 MiB)
24/05/24 19:57:27.288 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 19:57:27.288 dag-scheduler-event-loop INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
24/05/24 19:57:27.289 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[41] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 19:57:27.289 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/05/24 19:57:27.971 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_11_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 19:57:28.104 dispatcher-event-loop-1 WARN TaskSetManager: Stage 14 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 19:57:28.104 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 12) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 19:57:28.105 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO Executor: Running task 0.0 in stage 14.0 (TID 12)
24/05/24 19:57:28.980 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_10_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 19:57:29.012 Executor task launch worker for task 0.0 in stage 14.0 (TID 12) INFO Executor: Finished task 0.0 in stage 14.0 (TID 12). 5871 bytes result sent to driver
24/05/24 19:57:29.013 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 12) in 1723 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 19:57:29.014 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/05/24 19:57:29.014 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 14 (collect at utils.scala:26) finished in 1,740 s
24/05/24 19:57:29.014 dag-scheduler-event-loop INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 19:57:29.015 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/05/24 19:57:29.015 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 13 finished: collect at utils.scala:26, took 1,742983 s
24/05/24 20:05:39.383 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 22.8936 ms
24/05/24 20:05:39.407 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:05:39.408 dag-scheduler-event-loop INFO DAGScheduler: Got job 14 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:05:39.408 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 15 (collect at utils.scala:26)
24/05/24 20:05:39.408 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:05:39.409 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:05:39.410 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[43] at collect at utils.scala:26), which has no missing parents
24/05/24 20:05:39.413 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:05:39.417 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:05:39.418 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:05:39.418 dag-scheduler-event-loop INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
24/05/24 20:05:39.420 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[43] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:05:39.420 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
24/05/24 20:05:39.422 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:05:39.423 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO Executor: Running task 0.0 in stage 15.0 (TID 13)
24/05/24 20:05:39.432 Executor task launch worker for task 0.0 in stage 15.0 (TID 13) INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 1371 bytes result sent to driver
24/05/24 20:05:39.433 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 12 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:05:39.434 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/05/24 20:05:39.434 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 15 (collect at utils.scala:26) finished in 0,023 s
24/05/24 20:05:39.435 dag-scheduler-event-loop INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:05:39.435 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
24/05/24 20:05:39.435 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 14 finished: collect at utils.scala:26, took 0,027568 s
24/05/24 20:05:39.471 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 23.0392 ms
24/05/24 20:05:40.433 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:05:40.435 dag-scheduler-event-loop INFO DAGScheduler: Got job 15 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:05:40.435 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 16 (collect at utils.scala:26)
24/05/24 20:05:40.436 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:05:40.436 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:05:40.440 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[45] at collect at utils.scala:26), which has no missing parents
24/05/24 20:05:40.445 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:05:40.451 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.4 MiB)
24/05/24 20:05:40.452 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:05:40.454 dag-scheduler-event-loop INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
24/05/24 20:05:40.455 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[45] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:05:40.456 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
24/05/24 20:05:40.458 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:05:40.459 Executor task launch worker for task 0.0 in stage 16.0 (TID 14) INFO Executor: Running task 0.0 in stage 16.0 (TID 14)
24/05/24 20:05:40.469 Executor task launch worker for task 0.0 in stage 16.0 (TID 14) INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 1371 bytes result sent to driver
24/05/24 20:05:40.472 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 15 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:05:40.472 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
24/05/24 20:05:40.473 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 16 (collect at utils.scala:26) finished in 0,031 s
24/05/24 20:05:40.474 dag-scheduler-event-loop INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:05:40.474 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
24/05/24 20:05:40.474 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 15 finished: collect at utils.scala:26, took 0,040359 s
24/05/24 20:07:14.107 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 69.8956 ms
24/05/24 20:07:14.167 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 48 (collect at StringIndexer.scala:204) as input to shuffle 2
24/05/24 20:07:14.167 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 16 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:14.167 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 17 (collect at StringIndexer.scala:204)
24/05/24 20:07:14.167 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:07:14.168 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:14.169 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:14.234 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 52.5 KiB, free 883.4 MiB)
24/05/24 20:07:14.237 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.4 MiB)
24/05/24 20:07:14.237 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:07:14.238 dag-scheduler-event-loop INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:14.239 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[48] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:14.239 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
24/05/24 20:07:14.297 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_14_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:07:14.303 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_13_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:07:14.447 dispatcher-event-loop-1 WARN TaskSetManager: Stage 17 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:07:14.447 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 15) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:07:14.449 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO Executor: Running task 0.0 in stage 17.0 (TID 15)
24/05/24 20:07:14.667 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 113.5006 ms
24/05/24 20:07:14.699 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 13.7602 ms
24/05/24 20:07:15.730 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 5.1432 ms
24/05/24 20:07:15.748 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 8.1931 ms
24/05/24 20:07:15.764 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 10.0782 ms
24/05/24 20:07:15.789 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 7.348 ms
24/05/24 20:07:15.986 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO CodeGenerator: Code generated in 35.4571 ms
24/05/24 20:07:17.136 Executor task launch worker for task 0.0 in stage 17.0 (TID 15) INFO Executor: Finished task 0.0 in stage 17.0 (TID 15). 2342 bytes result sent to driver
24/05/24 20:07:17.139 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 15) in 2899 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:17.139 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/05/24 20:07:17.140 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 17 (collect at StringIndexer.scala:204) finished in 2,969 s
24/05/24 20:07:17.141 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:07:17.141 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:07:17.141 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:07:17.141 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:07:17.187 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:07:17.189 dag-scheduler-event-loop INFO DAGScheduler: Got job 17 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:17.189 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 19 (collect at StringIndexer.scala:204)
24/05/24 20:07:17.189 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
24/05/24 20:07:17.191 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:17.192 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[51] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:17.197 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:07:17.200 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.3 MiB)
24/05/24 20:07:17.202 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:07:17.203 dag-scheduler-event-loop INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:17.203 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[51] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:17.204 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
24/05/24 20:07:17.206 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:07:17.207 Executor task launch worker for task 0.0 in stage 19.0 (TID 16) INFO Executor: Running task 0.0 in stage 19.0 (TID 16)
24/05/24 20:07:17.232 Executor task launch worker for task 0.0 in stage 19.0 (TID 16) INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:07:17.233 Executor task launch worker for task 0.0 in stage 19.0 (TID 16) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 20:07:17.267 Executor task launch worker for task 0.0 in stage 19.0 (TID 16) INFO CodeGenerator: Code generated in 19.7236 ms
24/05/24 20:07:17.392 Executor task launch worker for task 0.0 in stage 19.0 (TID 16) INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 7870 bytes result sent to driver
24/05/24 20:07:17.394 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 187 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:17.394 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
24/05/24 20:07:17.395 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 19 (collect at StringIndexer.scala:204) finished in 0,200 s
24/05/24 20:07:17.395 dag-scheduler-event-loop INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:07:17.395 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
24/05/24 20:07:17.395 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 17 finished: collect at StringIndexer.scala:204, took 0,207288 s
24/05/24 20:07:17.680 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 30.2422 ms
24/05/24 20:07:17.688 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 54 (collect at StringIndexer.scala:204) as input to shuffle 3
24/05/24 20:07:17.688 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 18 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:17.688 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 20 (collect at StringIndexer.scala:204)
24/05/24 20:07:17.688 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:07:17.690 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:17.691 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:17.700 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 52.5 KiB, free 883.3 MiB)
24/05/24 20:07:17.702 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.3 MiB)
24/05/24 20:07:17.703 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:07:17.703 dag-scheduler-event-loop INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:17.703 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[54] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:17.705 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
24/05/24 20:07:17.853 dispatcher-event-loop-1 WARN TaskSetManager: Stage 20 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:07:17.853 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 17) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:07:17.854 Executor task launch worker for task 0.0 in stage 20.0 (TID 17) INFO Executor: Running task 0.0 in stage 20.0 (TID 17)
24/05/24 20:07:19.007 Executor task launch worker for task 0.0 in stage 20.0 (TID 17) INFO Executor: Finished task 0.0 in stage 20.0 (TID 17). 2342 bytes result sent to driver
24/05/24 20:07:19.008 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 17) in 1302 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:19.008 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
24/05/24 20:07:19.009 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 20 (collect at StringIndexer.scala:204) finished in 1,317 s
24/05/24 20:07:19.009 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:07:19.009 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:07:19.009 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:07:19.010 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:07:19.048 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:07:19.049 dag-scheduler-event-loop INFO DAGScheduler: Got job 19 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:19.049 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 22 (collect at StringIndexer.scala:204)
24/05/24 20:07:19.049 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
24/05/24 20:07:19.050 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:19.052 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[57] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:19.056 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 49.1 KiB, free 883.2 MiB)
24/05/24 20:07:19.058 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.2 MiB)
24/05/24 20:07:19.059 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:07:19.059 dag-scheduler-event-loop INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:19.061 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[57] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:19.062 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
24/05/24 20:07:19.063 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 18) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:07:19.064 Executor task launch worker for task 0.0 in stage 22.0 (TID 18) INFO Executor: Running task 0.0 in stage 22.0 (TID 18)
24/05/24 20:07:19.071 Executor task launch worker for task 0.0 in stage 22.0 (TID 18) INFO ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:07:19.072 Executor task launch worker for task 0.0 in stage 22.0 (TID 18) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:07:19.097 Executor task launch worker for task 0.0 in stage 22.0 (TID 18) INFO Executor: Finished task 0.0 in stage 22.0 (TID 18). 5140 bytes result sent to driver
24/05/24 20:07:19.098 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 18) in 36 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:19.099 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/05/24 20:07:19.100 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 22 (collect at StringIndexer.scala:204) finished in 0,046 s
24/05/24 20:07:19.100 dag-scheduler-event-loop INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:07:19.101 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
24/05/24 20:07:19.101 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 19 finished: collect at StringIndexer.scala:204, took 0,052782 s
24/05/24 20:07:19.298 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 28.684 ms
24/05/24 20:07:19.308 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 60 (collect at StringIndexer.scala:204) as input to shuffle 4
24/05/24 20:07:19.308 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 20 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:19.308 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 23 (collect at StringIndexer.scala:204)
24/05/24 20:07:19.308 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:07:19.308 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:19.312 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:19.322 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 52.5 KiB, free 883.1 MiB)
24/05/24 20:07:19.325 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.1 MiB)
24/05/24 20:07:19.327 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:07:19.328 dag-scheduler-event-loop INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:19.328 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[60] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:19.329 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
24/05/24 20:07:19.680 dispatcher-event-loop-1 WARN TaskSetManager: Stage 23 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:07:19.680 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 19) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:07:19.681 Executor task launch worker for task 0.0 in stage 23.0 (TID 19) INFO Executor: Running task 0.0 in stage 23.0 (TID 19)
24/05/24 20:07:19.981 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_15_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:07:19.987 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_18_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:07:19.991 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_12_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.4 MiB)
24/05/24 20:07:19.997 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_17_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:07:20.004 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_16_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:07:20.978 Executor task launch worker for task 0.0 in stage 23.0 (TID 19) INFO Executor: Finished task 0.0 in stage 23.0 (TID 19). 2342 bytes result sent to driver
24/05/24 20:07:20.981 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 19) in 1651 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:20.981 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/05/24 20:07:20.982 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 23 (collect at StringIndexer.scala:204) finished in 1,670 s
24/05/24 20:07:20.983 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:07:20.983 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:07:20.983 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:07:20.983 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:07:21.014 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:07:21.016 dag-scheduler-event-loop INFO DAGScheduler: Got job 21 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:07:21.016 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 25 (collect at StringIndexer.scala:204)
24/05/24 20:07:21.016 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
24/05/24 20:07:21.017 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:07:21.018 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:07:21.021 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:07:21.024 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.4 MiB)
24/05/24 20:07:21.025 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:07:21.026 dag-scheduler-event-loop INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
24/05/24 20:07:21.026 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[63] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:07:21.027 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
24/05/24 20:07:21.028 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 20) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:07:21.028 Executor task launch worker for task 0.0 in stage 25.0 (TID 20) INFO Executor: Running task 0.0 in stage 25.0 (TID 20)
24/05/24 20:07:21.034 Executor task launch worker for task 0.0 in stage 25.0 (TID 20) INFO ShuffleBlockFetcherIterator: Getting 1 (99.6 KiB) non-empty blocks including 1 (99.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:07:21.034 Executor task launch worker for task 0.0 in stage 25.0 (TID 20) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:07:21.086 Executor task launch worker for task 0.0 in stage 25.0 (TID 20) INFO Executor: Finished task 0.0 in stage 25.0 (TID 20). 101290 bytes result sent to driver
24/05/24 20:07:21.089 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 20) in 61 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:07:21.089 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
24/05/24 20:07:21.090 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 25 (collect at StringIndexer.scala:204) finished in 0,070 s
24/05/24 20:07:21.090 dag-scheduler-event-loop INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:07:21.090 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
24/05/24 20:07:21.091 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 21 finished: collect at StringIndexer.scala:204, took 0,076559 s
24/05/24 20:07:21.780 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Active`.`Energy`.`Class` cannot be resolved. Did you mean one of the following? [`datetime`, `Zip_Code`, `Date`, `Hour`, `Active_Energy_kWh_`, `N_EDIFICIOS_CLASSICOS`, `N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ`, `N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS`, `N_EDIFICIOS_EXCLUSIV_RESID`, `N_EDIFICIOS_1_OU_2_PISOS`, `N_EDIFICIOS_3_OU_MAIS_PISOS`, `N_EDIFICIOS_CONSTR_ANTES_1945`, `N_EDIFICIOS_CONSTR_1946_1980`, `N_EDIFICIOS_CONSTR_1981_2000`, `N_EDIFICIOS_CONSTR_2001_2010`, `N_EDIFICIOS_CONSTR_2011_2021`, `N_EDIFICIOS_COM_NECESSIDADES_REPARACAO`, `N_ALOJAMENTOS_TOTAL`, `N_ALOJAMENTOS_FAMILIARES`, `N_ALOJAMENTOS_FAM_CLASS_RHABITUAL`, `N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA`, `N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS`, `N_RHABITUAL_COM_ESTACIONAMENTO`, `N_RHABITUAL_PROP_OCUP`, `N_RHABITUAL_ARRENDADOS`, `N_AGREGADOS_DOMESTICOS_PRIVADOS`, `N_ADP_1_OU_2_PESSOAS`, `N_ADP_3_OU_MAIS_PESSOAS`, `N_NUCLEOS_FAMILIARES`, `N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25`, `N_INDIVIDUOS`, `N_INDIVIDUOS_H`, `N_INDIVIDUOS_M`, `N_INDIVIDUOS_0_14`, `N_INDIVIDUOS_15_24`, `N_INDIVIDUOS_25_64`, `N_INDIVIDUOS_65_OU_MAIS`, `temp`, `feelslike`, `dew`, `humidity`, `precip`, `precipprob`, `windgust`, `windspeed`, `winddir`, `sealevelpressure`, `cloudcover`, `visibility`, `solarradiation`, `solarenergy`, `uvindex`, `Active`.`Energy`.`Class`, `stridx_8dc94b97bbb7`, `stridx_8ca4e7162818`, `stridx_d8e2aa21813e`, `onehot_4922ae5dc3e5`, `onehot_8ec13a44dc5c`, `onehot_7cd022b8ce0c`, `features`].
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedColumnWithSuggestionError(QueryCompilationErrors.scala:2936)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:249)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.Dataset.resolve(Dataset.scala:249)
	at org.apache.spark.sql.Dataset.col(Dataset.scala:1474)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.$anonfun$transform$6(RFormula.scala:565)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.transform(RFormula.scala:565)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)
	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)
	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)
	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:302)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

24/05/24 20:07:21.781 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Active`.`Energy`.`Class` cannot be resolved. Did you mean one of the following? [`datetime`, `Zip_Code`, `Date`, `Hour`, `Active_Energy_kWh_`, `N_EDIFICIOS_CLASSICOS`, `N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ`, `N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS`, `N_EDIFICIOS_EXCLUSIV_RESID`, `N_EDIFICIOS_1_OU_2_PISOS`, `N_EDIFICIOS_3_OU_MAIS_PISOS`, `N_EDIFICIOS_CONSTR_ANTES_1945`, `N_EDIFICIOS_CONSTR_1946_1980`, `N_EDIFICIOS_CONSTR_1981_2000`, `N_EDIFICIOS_CONSTR_2001_2010`, `N_EDIFICIOS_CONSTR_2011_2021`, `N_EDIFICIOS_COM_NECESSIDADES_REPARACAO`, `N_ALOJAMENTOS_TOTAL`, `N_ALOJAMENTOS_FAMILIARES`, `N_ALOJAMENTOS_FAM_CLASS_RHABITUAL`, `N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA`, `N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS`, `N_RHABITUAL_COM_ESTACIONAMENTO`, `N_RHABITUAL_PROP_OCUP`, `N_RHABITUAL_ARRENDADOS`, `N_AGREGADOS_DOMESTICOS_PRIVADOS`, `N_ADP_1_OU_2_PESSOAS`, `N_ADP_3_OU_MAIS_PESSOAS`, `N_NUCLEOS_FAMILIARES`, `N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25`, `N_INDIVIDUOS`, `N_INDIVIDUOS_H`, `N_INDIVIDUOS_M`, `N_INDIVIDUOS_0_14`, `N_INDIVIDUOS_15_24`, `N_INDIVIDUOS_25_64`, `N_INDIVIDUOS_65_OU_MAIS`, `temp`, `feelslike`, `dew`, `humidity`, `precip`, `precipprob`, `windgust`, `windspeed`, `winddir`, `sealevelpressure`, `cloudcover`, `visibility`, `solarradiation`, `solarenergy`, `uvindex`, `Active`.`Energy`.`Class`, `stridx_8dc94b97bbb7`, `stridx_8ca4e7162818`, `stridx_d8e2aa21813e`, `onehot_4922ae5dc3e5`, `onehot_8ec13a44dc5c`, `onehot_7cd022b8ce0c`, `features`].
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedColumnWithSuggestionError(QueryCompilationErrors.scala:2936)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:249)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.Dataset.resolve(Dataset.scala:249)
	at org.apache.spark.sql.Dataset.col(Dataset.scala:1474)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.$anonfun$transform$6(RFormula.scala:565)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.transform(RFormula.scala:565)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)
	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)
	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)
	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:302)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

24/05/24 20:10:15.900 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_20_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:10:15.940 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 66 (collect at StringIndexer.scala:204) as input to shuffle 5
24/05/24 20:10:15.940 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 22 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:15.940 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 26 (collect at StringIndexer.scala:204)
24/05/24 20:10:15.940 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:10:15.940 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:15.942 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[66] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:15.952 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 52.5 KiB, free 883.4 MiB)
24/05/24 20:10:15.953 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.4 MiB)
24/05/24 20:10:15.955 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:10:15.955 dag-scheduler-event-loop INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:15.955 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[66] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:15.956 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
24/05/24 20:10:16.124 dispatcher-event-loop-1 WARN TaskSetManager: Stage 26 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:10:16.124 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 21) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:10:16.126 Executor task launch worker for task 0.0 in stage 26.0 (TID 21) INFO Executor: Running task 0.0 in stage 26.0 (TID 21)
24/05/24 20:10:17.221 Executor task launch worker for task 0.0 in stage 26.0 (TID 21) INFO Executor: Finished task 0.0 in stage 26.0 (TID 21). 2342 bytes result sent to driver
24/05/24 20:10:17.222 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 21) in 1266 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:17.222 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/05/24 20:10:17.223 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 26 (collect at StringIndexer.scala:204) finished in 1,280 s
24/05/24 20:10:17.223 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:10:17.223 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:10:17.223 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:10:17.223 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:10:17.245 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:10:17.246 dag-scheduler-event-loop INFO DAGScheduler: Got job 23 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:17.246 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 28 (collect at StringIndexer.scala:204)
24/05/24 20:10:17.246 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
24/05/24 20:10:17.246 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:17.247 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[69] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:17.252 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 49.1 KiB, free 883.3 MiB)
24/05/24 20:10:17.254 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.3 MiB)
24/05/24 20:10:17.254 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:10:17.255 dag-scheduler-event-loop INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:17.255 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[69] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:17.255 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
24/05/24 20:10:17.256 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:10:17.257 Executor task launch worker for task 0.0 in stage 28.0 (TID 22) INFO Executor: Running task 0.0 in stage 28.0 (TID 22)
24/05/24 20:10:17.261 Executor task launch worker for task 0.0 in stage 28.0 (TID 22) INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:10:17.262 Executor task launch worker for task 0.0 in stage 28.0 (TID 22) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:10:17.284 Executor task launch worker for task 0.0 in stage 28.0 (TID 22) INFO Executor: Finished task 0.0 in stage 28.0 (TID 22). 7784 bytes result sent to driver
24/05/24 20:10:17.285 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 29 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:17.286 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
24/05/24 20:10:17.287 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 28 (collect at StringIndexer.scala:204) finished in 0,037 s
24/05/24 20:10:17.287 dag-scheduler-event-loop INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:10:17.287 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
24/05/24 20:10:17.287 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 23 finished: collect at StringIndexer.scala:204, took 0,042193 s
24/05/24 20:10:17.487 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 72 (collect at StringIndexer.scala:204) as input to shuffle 6
24/05/24 20:10:17.487 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 24 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:17.488 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 29 (collect at StringIndexer.scala:204)
24/05/24 20:10:17.488 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:10:17.488 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:17.489 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[72] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:17.501 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:10:17.503 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.2 MiB)
24/05/24 20:10:17.504 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:10:17.504 dag-scheduler-event-loop INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:17.505 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[72] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:17.505 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
24/05/24 20:10:17.538 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_22_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:10:17.704 dispatcher-event-loop-1 WARN TaskSetManager: Stage 29 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:10:17.704 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:10:17.704 Executor task launch worker for task 0.0 in stage 29.0 (TID 23) INFO Executor: Running task 0.0 in stage 29.0 (TID 23)
24/05/24 20:10:18.980 Executor task launch worker for task 0.0 in stage 29.0 (TID 23) INFO Executor: Finished task 0.0 in stage 29.0 (TID 23). 2342 bytes result sent to driver
24/05/24 20:10:18.981 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 1475 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:18.981 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/05/24 20:10:18.982 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 29 (collect at StringIndexer.scala:204) finished in 1,492 s
24/05/24 20:10:18.982 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:10:18.982 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:10:18.983 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:10:18.983 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:10:19.006 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:10:19.007 dag-scheduler-event-loop INFO DAGScheduler: Got job 25 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:19.007 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 31 (collect at StringIndexer.scala:204)
24/05/24 20:10:19.007 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
24/05/24 20:10:19.007 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:19.008 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[75] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:19.013 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.1 KiB, free 883.2 MiB)
24/05/24 20:10:19.018 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.2 MiB)
24/05/24 20:10:19.019 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:10:19.020 dag-scheduler-event-loop INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:19.021 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[75] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:19.022 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
24/05/24 20:10:19.024 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:10:19.025 Executor task launch worker for task 0.0 in stage 31.0 (TID 24) INFO Executor: Running task 0.0 in stage 31.0 (TID 24)
24/05/24 20:10:19.037 Executor task launch worker for task 0.0 in stage 31.0 (TID 24) INFO ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:10:19.038 Executor task launch worker for task 0.0 in stage 31.0 (TID 24) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 20:10:19.080 Executor task launch worker for task 0.0 in stage 31.0 (TID 24) INFO Executor: Finished task 0.0 in stage 31.0 (TID 24). 5183 bytes result sent to driver
24/05/24 20:10:19.082 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 58 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:19.083 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/05/24 20:10:19.085 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 31 (collect at StringIndexer.scala:204) finished in 0,076 s
24/05/24 20:10:19.086 dag-scheduler-event-loop INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:10:19.086 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
24/05/24 20:10:19.087 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 25 finished: collect at StringIndexer.scala:204, took 0,080891 s
24/05/24 20:10:19.311 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 78 (collect at StringIndexer.scala:204) as input to shuffle 7
24/05/24 20:10:19.311 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 26 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:19.311 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 32 (collect at StringIndexer.scala:204)
24/05/24 20:10:19.311 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:10:19.312 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:19.313 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:19.322 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:10:19.325 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.1 MiB)
24/05/24 20:10:19.326 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:10:19.326 dag-scheduler-event-loop INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:19.327 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[78] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:19.327 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
24/05/24 20:10:19.395 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_24_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:10:19.557 dispatcher-event-loop-1 WARN TaskSetManager: Stage 32 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:10:19.557 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:10:19.558 Executor task launch worker for task 0.0 in stage 32.0 (TID 25) INFO Executor: Running task 0.0 in stage 32.0 (TID 25)
24/05/24 20:10:20.791 Executor task launch worker for task 0.0 in stage 32.0 (TID 25) INFO Executor: Finished task 0.0 in stage 32.0 (TID 25). 2342 bytes result sent to driver
24/05/24 20:10:20.808 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 1480 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:20.809 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/05/24 20:10:20.811 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 32 (collect at StringIndexer.scala:204) finished in 1,497 s
24/05/24 20:10:20.812 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:10:20.813 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:10:20.813 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:10:20.814 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:10:20.862 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:10:20.863 dag-scheduler-event-loop INFO DAGScheduler: Got job 27 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:10:20.864 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 34 (collect at StringIndexer.scala:204)
24/05/24 20:10:20.864 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
24/05/24 20:10:20.864 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:10:20.865 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[81] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:10:20.870 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 49.1 KiB, free 883.2 MiB)
24/05/24 20:10:20.871 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.1 MiB)
24/05/24 20:10:20.872 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:10:20.872 dag-scheduler-event-loop INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
24/05/24 20:10:20.873 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[81] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:10:20.873 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
24/05/24 20:10:20.874 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 26) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:10:20.875 Executor task launch worker for task 0.0 in stage 34.0 (TID 26) INFO Executor: Running task 0.0 in stage 34.0 (TID 26)
24/05/24 20:10:20.883 Executor task launch worker for task 0.0 in stage 34.0 (TID 26) INFO ShuffleBlockFetcherIterator: Getting 1 (99.6 KiB) non-empty blocks including 1 (99.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:10:20.884 Executor task launch worker for task 0.0 in stage 34.0 (TID 26) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 20:10:20.913 Executor task launch worker for task 0.0 in stage 34.0 (TID 26) INFO Executor: Finished task 0.0 in stage 34.0 (TID 26). 101290 bytes result sent to driver
24/05/24 20:10:20.914 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 26) in 40 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:10:20.914 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/05/24 20:10:20.915 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 34 (collect at StringIndexer.scala:204) finished in 0,049 s
24/05/24 20:10:20.915 dag-scheduler-event-loop INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:10:20.915 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
24/05/24 20:10:20.915 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 27 finished: collect at StringIndexer.scala:204, took 0,053320 s
24/05/24 20:10:21.474 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Active`.`Energy`.`Class` cannot be resolved. Did you mean one of the following? [`datetime`, `Zip_Code`, `Date`, `Hour`, `Active_Energy_kWh_`, `N_EDIFICIOS_CLASSICOS`, `N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ`, `N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS`, `N_EDIFICIOS_EXCLUSIV_RESID`, `N_EDIFICIOS_1_OU_2_PISOS`, `N_EDIFICIOS_3_OU_MAIS_PISOS`, `N_EDIFICIOS_CONSTR_ANTES_1945`, `N_EDIFICIOS_CONSTR_1946_1980`, `N_EDIFICIOS_CONSTR_1981_2000`, `N_EDIFICIOS_CONSTR_2001_2010`, `N_EDIFICIOS_CONSTR_2011_2021`, `N_EDIFICIOS_COM_NECESSIDADES_REPARACAO`, `N_ALOJAMENTOS_TOTAL`, `N_ALOJAMENTOS_FAMILIARES`, `N_ALOJAMENTOS_FAM_CLASS_RHABITUAL`, `N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA`, `N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS`, `N_RHABITUAL_COM_ESTACIONAMENTO`, `N_RHABITUAL_PROP_OCUP`, `N_RHABITUAL_ARRENDADOS`, `N_AGREGADOS_DOMESTICOS_PRIVADOS`, `N_ADP_1_OU_2_PESSOAS`, `N_ADP_3_OU_MAIS_PESSOAS`, `N_NUCLEOS_FAMILIARES`, `N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25`, `N_INDIVIDUOS`, `N_INDIVIDUOS_H`, `N_INDIVIDUOS_M`, `N_INDIVIDUOS_0_14`, `N_INDIVIDUOS_15_24`, `N_INDIVIDUOS_25_64`, `N_INDIVIDUOS_65_OU_MAIS`, `temp`, `feelslike`, `dew`, `humidity`, `precip`, `precipprob`, `windgust`, `windspeed`, `winddir`, `sealevelpressure`, `cloudcover`, `visibility`, `solarradiation`, `solarenergy`, `uvindex`, `Active`.`Energy`.`Class`, `stridx_f8db28721b26`, `stridx_4195b7bf26f7`, `stridx_95c66806a16c`, `onehot_b67e4ab69568`, `onehot_97af0fe7685d`, `onehot_921e7d777f84`, `features`].
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedColumnWithSuggestionError(QueryCompilationErrors.scala:2936)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:249)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.Dataset.resolve(Dataset.scala:249)
	at org.apache.spark.sql.Dataset.col(Dataset.scala:1474)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.$anonfun$transform$6(RFormula.scala:565)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.transform(RFormula.scala:565)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)
	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)
	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)
	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:302)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

24/05/24 20:10:21.475 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Active`.`Energy`.`Class` cannot be resolved. Did you mean one of the following? [`datetime`, `Zip_Code`, `Date`, `Hour`, `Active_Energy_kWh_`, `N_EDIFICIOS_CLASSICOS`, `N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ`, `N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS`, `N_EDIFICIOS_EXCLUSIV_RESID`, `N_EDIFICIOS_1_OU_2_PISOS`, `N_EDIFICIOS_3_OU_MAIS_PISOS`, `N_EDIFICIOS_CONSTR_ANTES_1945`, `N_EDIFICIOS_CONSTR_1946_1980`, `N_EDIFICIOS_CONSTR_1981_2000`, `N_EDIFICIOS_CONSTR_2001_2010`, `N_EDIFICIOS_CONSTR_2011_2021`, `N_EDIFICIOS_COM_NECESSIDADES_REPARACAO`, `N_ALOJAMENTOS_TOTAL`, `N_ALOJAMENTOS_FAMILIARES`, `N_ALOJAMENTOS_FAM_CLASS_RHABITUAL`, `N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA`, `N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS`, `N_RHABITUAL_COM_ESTACIONAMENTO`, `N_RHABITUAL_PROP_OCUP`, `N_RHABITUAL_ARRENDADOS`, `N_AGREGADOS_DOMESTICOS_PRIVADOS`, `N_ADP_1_OU_2_PESSOAS`, `N_ADP_3_OU_MAIS_PESSOAS`, `N_NUCLEOS_FAMILIARES`, `N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25`, `N_INDIVIDUOS`, `N_INDIVIDUOS_H`, `N_INDIVIDUOS_M`, `N_INDIVIDUOS_0_14`, `N_INDIVIDUOS_15_24`, `N_INDIVIDUOS_25_64`, `N_INDIVIDUOS_65_OU_MAIS`, `temp`, `feelslike`, `dew`, `humidity`, `precip`, `precipprob`, `windgust`, `windspeed`, `winddir`, `sealevelpressure`, `cloudcover`, `visibility`, `solarradiation`, `solarenergy`, `uvindex`, `Active`.`Energy`.`Class`, `stridx_f8db28721b26`, `stridx_4195b7bf26f7`, `stridx_95c66806a16c`, `onehot_b67e4ab69568`, `onehot_97af0fe7685d`, `onehot_921e7d777f84`, `features`].
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedColumnWithSuggestionError(QueryCompilationErrors.scala:2936)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:249)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.Dataset.resolve(Dataset.scala:249)
	at org.apache.spark.sql.Dataset.col(Dataset.scala:1474)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.$anonfun$transform$6(RFormula.scala:565)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.feature.VectorAttributeRewriter.transform(RFormula.scala:565)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$6(Pipeline.scala:160)
	at org.apache.spark.ml.MLEvents.withTransformEvent(events.scala:146)
	at org.apache.spark.ml.MLEvents.withTransformEvent$(events.scala:139)
	at org.apache.spark.ml.util.Instrumentation.withTransformEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:160)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:302)
	at org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

24/05/24 20:18:36.696 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_26_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:18:36.700 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_21_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:18:36.703 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_19_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:18:36.705 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_23_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:18:36.708 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_25_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:21:43.366 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:21:43.367 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:21:43.373 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:21:43.373 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:21:43.375 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:21:43.375 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:21:43.431 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:21:43.432 dag-scheduler-event-loop INFO DAGScheduler: Got job 28 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:21:43.432 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 35 (collect at utils.scala:26)
24/05/24 20:21:43.432 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:21:43.432 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:21:43.432 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[84] at collect at utils.scala:26), which has no missing parents
24/05/24 20:21:43.434 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 7.1 KiB, free 883.5 MiB)
24/05/24 20:21:43.436 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 883.5 MiB)
24/05/24 20:21:43.436 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 20:21:43.436 dag-scheduler-event-loop INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
24/05/24 20:21:43.437 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[84] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:21:43.437 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
24/05/24 20:21:43.438 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7769 bytes) 
24/05/24 20:21:43.439 Executor task launch worker for task 0.0 in stage 35.0 (TID 27) INFO Executor: Running task 0.0 in stage 35.0 (TID 27)
24/05/24 20:21:43.442 Executor task launch worker for task 0.0 in stage 35.0 (TID 27) INFO Executor: Finished task 0.0 in stage 35.0 (TID 27). 1450 bytes result sent to driver
24/05/24 20:21:43.442 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:21:43.442 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/05/24 20:21:43.443 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 35 (collect at utils.scala:26) finished in 0,010 s
24/05/24 20:21:43.443 dag-scheduler-event-loop INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:21:43.443 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
24/05/24 20:21:43.443 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 28 finished: collect at utils.scala:26, took 0,012016 s
24/05/24 20:21:59.071 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_27_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 20:22:04.653 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:22:04.656 dag-scheduler-event-loop INFO DAGScheduler: Got job 29 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:22:04.656 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 36 (collect at utils.scala:26)
24/05/24 20:22:04.656 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:22:04.657 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:22:04.658 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[87] at collect at utils.scala:26), which has no missing parents
24/05/24 20:22:04.665 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:22:04.667 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:22:04.668 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:22:04.668 dag-scheduler-event-loop INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535
24/05/24 20:22:04.669 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[87] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:22:04.669 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
24/05/24 20:22:04.673 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:22:04.674 Executor task launch worker for task 0.0 in stage 36.0 (TID 28) INFO Executor: Running task 0.0 in stage 36.0 (TID 28)
24/05/24 20:22:04.682 Executor task launch worker for task 0.0 in stage 36.0 (TID 28) INFO Executor: Finished task 0.0 in stage 36.0 (TID 28). 1371 bytes result sent to driver
24/05/24 20:22:04.684 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 12 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:22:04.684 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
24/05/24 20:22:04.685 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 36 (collect at utils.scala:26) finished in 0,023 s
24/05/24 20:22:04.685 dag-scheduler-event-loop INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:22:04.685 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
24/05/24 20:22:04.685 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 29 finished: collect at utils.scala:26, took 0,031448 s
24/05/24 20:22:05.237 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:22:05.237 dag-scheduler-event-loop INFO DAGScheduler: Got job 30 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:22:05.237 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 37 (collect at utils.scala:26)
24/05/24 20:22:05.237 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:22:05.237 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:22:05.238 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[89] at collect at utils.scala:26), which has no missing parents
24/05/24 20:22:05.240 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:22:05.242 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:22:05.242 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:22:05.242 dag-scheduler-event-loop INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535
24/05/24 20:22:05.243 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[89] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:22:05.243 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
24/05/24 20:22:05.244 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:22:05.244 Executor task launch worker for task 0.0 in stage 37.0 (TID 29) INFO Executor: Running task 0.0 in stage 37.0 (TID 29)
24/05/24 20:22:05.246 Executor task launch worker for task 0.0 in stage 37.0 (TID 29) INFO Executor: Finished task 0.0 in stage 37.0 (TID 29). 1285 bytes result sent to driver
24/05/24 20:22:05.247 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:22:05.247 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/05/24 20:22:05.247 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 37 (collect at utils.scala:26) finished in 0,008 s
24/05/24 20:22:05.248 dag-scheduler-event-loop INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:22:05.248 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
24/05/24 20:22:05.248 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 30 finished: collect at utils.scala:26, took 0,011260 s
24/05/24 20:22:05.644 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:22:05.644 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:22:05.656 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:22:05.656 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:22:05.657 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:22:05.657 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:22:33.679 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 91 (collect at utils.scala:26) as input to shuffle 8
24/05/24 20:22:33.679 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 31 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:22:33.679 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 38 (collect at utils.scala:26)
24/05/24 20:22:33.680 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:22:33.680 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:22:33.681 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[91] at collect at utils.scala:26), which has no missing parents
24/05/24 20:22:33.685 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 12.9 KiB, free 883.5 MiB)
24/05/24 20:22:33.687 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.5 MiB)
24/05/24 20:22:33.687 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:22:33.687 dag-scheduler-event-loop INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535
24/05/24 20:22:33.687 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[91] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:22:33.687 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
24/05/24 20:22:34.543 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_28_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:22:34.549 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_29_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:22:34.656 dispatcher-event-loop-1 WARN TaskSetManager: Stage 38 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:22:34.656 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:22:34.656 Executor task launch worker for task 0.0 in stage 38.0 (TID 30) INFO Executor: Running task 0.0 in stage 38.0 (TID 30)
24/05/24 20:22:34.800 Executor task launch worker for task 0.0 in stage 38.0 (TID 30) INFO Executor: Finished task 0.0 in stage 38.0 (TID 30). 1759 bytes result sent to driver
24/05/24 20:22:34.802 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 1114 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:22:34.802 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/05/24 20:22:34.804 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 38 (collect at utils.scala:26) finished in 1,122 s
24/05/24 20:22:34.804 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:22:34.804 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:22:34.804 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:22:34.804 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:22:35.417 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:22:35.418 dag-scheduler-event-loop INFO DAGScheduler: Got job 32 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:22:35.418 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 40 (collect at utils.scala:26)
24/05/24 20:22:35.418 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
24/05/24 20:22:35.418 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:22:35.419 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[94] at collect at utils.scala:26), which has no missing parents
24/05/24 20:22:35.421 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 13.7 KiB, free 883.5 MiB)
24/05/24 20:22:35.423 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.5 MiB)
24/05/24 20:22:35.423 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:22:35.424 dag-scheduler-event-loop INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
24/05/24 20:22:35.424 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[94] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:22:35.424 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
24/05/24 20:22:35.425 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:22:35.426 Executor task launch worker for task 0.0 in stage 40.0 (TID 31) INFO Executor: Running task 0.0 in stage 40.0 (TID 31)
24/05/24 20:22:35.429 Executor task launch worker for task 0.0 in stage 40.0 (TID 31) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:22:35.429 Executor task launch worker for task 0.0 in stage 40.0 (TID 31) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 20:22:35.433 Executor task launch worker for task 0.0 in stage 40.0 (TID 31) INFO Executor: Finished task 0.0 in stage 40.0 (TID 31). 3909 bytes result sent to driver
24/05/24 20:22:35.434 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 9 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:22:35.435 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/05/24 20:22:35.436 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 40 (collect at utils.scala:26) finished in 0,016 s
24/05/24 20:22:35.436 dag-scheduler-event-loop INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:22:35.436 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
24/05/24 20:22:35.436 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 32 finished: collect at utils.scala:26, took 0,018770 s
24/05/24 20:22:50.893 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:22:50.894 dag-scheduler-event-loop INFO DAGScheduler: Got job 33 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:22:50.894 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 41 (collect at utils.scala:26)
24/05/24 20:22:50.894 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:22:50.894 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:22:50.896 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[96] at collect at utils.scala:26), which has no missing parents
24/05/24 20:22:50.898 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 20.3 KiB, free 883.4 MiB)
24/05/24 20:22:50.900 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.4 MiB)
24/05/24 20:22:50.900 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:22:50.901 dag-scheduler-event-loop INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1535
24/05/24 20:22:50.902 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[96] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:22:50.902 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
24/05/24 20:22:51.033 dispatcher-event-loop-0 WARN TaskSetManager: Stage 41 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:22:51.033 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:22:51.034 Executor task launch worker for task 0.0 in stage 41.0 (TID 32) INFO Executor: Running task 0.0 in stage 41.0 (TID 32)
24/05/24 20:22:51.400 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_31_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:22:51.404 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_30_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:22:51.583 Executor task launch worker for task 0.0 in stage 41.0 (TID 32) INFO MemoryStore: Block taskresult_32 stored as bytes in memory (estimated size 13.2 MiB, free 870.3 MiB)
24/05/24 20:22:51.583 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_32 in memory on DESKTOP-LH06ASP:51464 (size: 13.2 MiB, free: 870.3 MiB)
24/05/24 20:22:51.584 Executor task launch worker for task 0.0 in stage 41.0 (TID 32) INFO Executor: Finished task 0.0 in stage 41.0 (TID 32). 13828710 bytes result sent via BlockManager)
24/05/24 20:22:51.704 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 801 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:22:51.705 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
24/05/24 20:22:51.706 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 41 (collect at utils.scala:26) finished in 0,808 s
24/05/24 20:22:51.706 dag-scheduler-event-loop INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:22:51.706 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_32 on DESKTOP-LH06ASP:51464 in memory (size: 13.2 MiB, free: 883.5 MiB)
24/05/24 20:22:51.706 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
24/05/24 20:22:51.706 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 33 finished: collect at utils.scala:26, took 0,813704 s
24/05/24 20:22:54.264 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_32_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:23:31.670 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:23:31.671 dag-scheduler-event-loop INFO DAGScheduler: Got job 34 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:23:31.671 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 42 (collect at utils.scala:26)
24/05/24 20:23:31.671 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:23:31.671 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:23:31.672 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[98] at collect at utils.scala:26), which has no missing parents
24/05/24 20:23:31.674 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 20.2 KiB, free 883.5 MiB)
24/05/24 20:23:31.677 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.5 MiB)
24/05/24 20:23:31.685 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:23:31.685 dag-scheduler-event-loop INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535
24/05/24 20:23:31.685 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[98] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:23:31.685 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
24/05/24 20:23:31.846 dispatcher-event-loop-1 WARN TaskSetManager: Stage 42 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:23:31.846 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:23:31.846 Executor task launch worker for task 0.0 in stage 42.0 (TID 33) INFO Executor: Running task 0.0 in stage 42.0 (TID 33)
24/05/24 20:23:31.945 Executor task launch worker for task 0.0 in stage 42.0 (TID 33) INFO Executor: Finished task 0.0 in stage 42.0 (TID 33). 2528 bytes result sent to driver
24/05/24 20:23:31.945 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 259 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:23:31.945 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/05/24 20:23:31.946 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 42 (collect at utils.scala:26) finished in 0,274 s
24/05/24 20:23:31.946 dag-scheduler-event-loop INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:23:31.946 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
24/05/24 20:23:31.946 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 34 finished: collect at utils.scala:26, took 0,275102 s
24/05/24 20:25:10.949 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:25:10.950 dag-scheduler-event-loop INFO DAGScheduler: Got job 35 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:25:10.950 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 43 (collect at utils.scala:26)
24/05/24 20:25:10.950 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:25:10.950 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:25:10.951 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[100] at collect at utils.scala:26), which has no missing parents
24/05/24 20:25:10.952 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:25:10.953 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:25:10.953 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:25:10.954 dag-scheduler-event-loop INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535
24/05/24 20:25:10.954 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[100] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:25:10.954 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
24/05/24 20:25:10.955 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:25:10.956 Executor task launch worker for task 0.0 in stage 43.0 (TID 34) INFO Executor: Running task 0.0 in stage 43.0 (TID 34)
24/05/24 20:25:10.958 Executor task launch worker for task 0.0 in stage 43.0 (TID 34) INFO Executor: Finished task 0.0 in stage 43.0 (TID 34). 1328 bytes result sent to driver
24/05/24 20:25:10.959 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:25:10.959 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
24/05/24 20:25:10.959 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 43 (collect at utils.scala:26) finished in 0,008 s
24/05/24 20:25:10.959 dag-scheduler-event-loop INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:25:10.959 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
24/05/24 20:25:10.959 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 35 finished: collect at utils.scala:26, took 0,010064 s
24/05/24 20:25:11.326 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:25:11.326 dag-scheduler-event-loop INFO DAGScheduler: Got job 36 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:25:11.327 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 44 (collect at utils.scala:26)
24/05/24 20:25:11.327 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:25:11.327 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:25:11.327 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[102] at collect at utils.scala:26), which has no missing parents
24/05/24 20:25:11.329 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:25:11.330 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.4 MiB)
24/05/24 20:25:11.330 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:25:11.331 dag-scheduler-event-loop INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535
24/05/24 20:25:11.331 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[102] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:25:11.331 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
24/05/24 20:25:11.331 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 35) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:25:11.332 Executor task launch worker for task 0.0 in stage 44.0 (TID 35) INFO Executor: Running task 0.0 in stage 44.0 (TID 35)
24/05/24 20:25:11.334 Executor task launch worker for task 0.0 in stage 44.0 (TID 35) INFO Executor: Finished task 0.0 in stage 44.0 (TID 35). 1328 bytes result sent to driver
24/05/24 20:25:11.334 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 35) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:25:11.334 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
24/05/24 20:25:11.335 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 44 (collect at utils.scala:26) finished in 0,007 s
24/05/24 20:25:11.335 dag-scheduler-event-loop INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:25:11.335 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
24/05/24 20:25:11.335 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 36 finished: collect at utils.scala:26, took 0,008571 s
24/05/24 20:29:18.320 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:29:18.321 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:29:18.325 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:29:18.325 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:29:18.326 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:29:18.326 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:29:18.381 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:29:18.382 dag-scheduler-event-loop INFO DAGScheduler: Got job 37 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:18.382 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 45 (collect at utils.scala:26)
24/05/24 20:29:18.382 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:29:18.382 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:18.383 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[105] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:18.384 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 7.1 KiB, free 883.4 MiB)
24/05/24 20:29:18.386 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 883.4 MiB)
24/05/24 20:29:18.388 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 20:29:18.389 dag-scheduler-event-loop INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:18.389 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[105] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:18.389 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
24/05/24 20:29:18.391 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 36) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7979 bytes) 
24/05/24 20:29:18.393 Executor task launch worker for task 0.0 in stage 45.0 (TID 36) INFO Executor: Running task 0.0 in stage 45.0 (TID 36)
24/05/24 20:29:18.396 Executor task launch worker for task 0.0 in stage 45.0 (TID 36) INFO Executor: Finished task 0.0 in stage 45.0 (TID 36). 1544 bytes result sent to driver
24/05/24 20:29:18.396 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 36) in 6 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:18.396 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
24/05/24 20:29:18.397 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 45 (collect at utils.scala:26) finished in 0,014 s
24/05/24 20:29:18.397 dag-scheduler-event-loop INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:29:18.397 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
24/05/24 20:29:18.397 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 37 finished: collect at utils.scala:26, took 0,015583 s
24/05/24 20:29:28.485 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_33_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:29:28.490 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_36_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 883.5 MiB)
24/05/24 20:29:28.493 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_35_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:28.497 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_34_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:32.632 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:29:32.634 dag-scheduler-event-loop INFO DAGScheduler: Got job 38 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:32.635 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 46 (collect at utils.scala:26)
24/05/24 20:29:32.635 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:29:32.635 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:32.635 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[108] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:32.639 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:29:32.641 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:29:32.642 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:32.643 dag-scheduler-event-loop INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:32.644 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[108] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:32.644 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
24/05/24 20:29:32.647 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 37) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:29:32.649 Executor task launch worker for task 0.0 in stage 46.0 (TID 37) INFO Executor: Running task 0.0 in stage 46.0 (TID 37)
24/05/24 20:29:32.656 Executor task launch worker for task 0.0 in stage 46.0 (TID 37) INFO Executor: Finished task 0.0 in stage 46.0 (TID 37). 1371 bytes result sent to driver
24/05/24 20:29:32.657 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 37) in 11 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:32.657 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
24/05/24 20:29:32.658 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 46 (collect at utils.scala:26) finished in 0,021 s
24/05/24 20:29:32.658 dag-scheduler-event-loop INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:29:32.658 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
24/05/24 20:29:32.658 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 38 finished: collect at utils.scala:26, took 0,026260 s
24/05/24 20:29:33.098 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:29:33.098 dag-scheduler-event-loop INFO DAGScheduler: Got job 39 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:33.098 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 47 (collect at utils.scala:26)
24/05/24 20:29:33.098 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:29:33.098 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:33.145 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[110] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:33.148 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:29:33.165 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:29:33.176 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:33.177 dag-scheduler-event-loop INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:33.177 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[110] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:33.177 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
24/05/24 20:29:33.178 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 38) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:29:33.178 Executor task launch worker for task 0.0 in stage 47.0 (TID 38) INFO Executor: Running task 0.0 in stage 47.0 (TID 38)
24/05/24 20:29:33.181 Executor task launch worker for task 0.0 in stage 47.0 (TID 38) INFO Executor: Finished task 0.0 in stage 47.0 (TID 38). 1328 bytes result sent to driver
24/05/24 20:29:33.181 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 38) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:33.182 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
24/05/24 20:29:33.182 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 47 (collect at utils.scala:26) finished in 0,036 s
24/05/24 20:29:33.182 dag-scheduler-event-loop INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:29:33.182 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
24/05/24 20:29:33.182 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 39 finished: collect at utils.scala:26, took 0,084472 s
24/05/24 20:29:33.560 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:29:33.560 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:29:33.579 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:29:33.579 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:29:33.581 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:29:33.581 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:29:35.357 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 112 (collect at utils.scala:26) as input to shuffle 9
24/05/24 20:29:35.358 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 40 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:35.358 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 48 (collect at utils.scala:26)
24/05/24 20:29:35.358 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:29:35.358 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:35.359 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 48 (MapPartitionsRDD[112] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:35.361 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 12.9 KiB, free 883.5 MiB)
24/05/24 20:29:35.362 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.5 MiB)
24/05/24 20:29:35.363 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:29:35.363 dag-scheduler-event-loop INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:35.363 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[112] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:35.363 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
24/05/24 20:29:35.534 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_37_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:35.538 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_38_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:29:35.610 dispatcher-event-loop-0 WARN TaskSetManager: Stage 48 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:29:35.610 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 39) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:29:35.610 Executor task launch worker for task 0.0 in stage 48.0 (TID 39) INFO Executor: Running task 0.0 in stage 48.0 (TID 39)
24/05/24 20:29:35.826 Executor task launch worker for task 0.0 in stage 48.0 (TID 39) INFO Executor: Finished task 0.0 in stage 48.0 (TID 39). 1845 bytes result sent to driver
24/05/24 20:29:35.827 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 39) in 463 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:35.827 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
24/05/24 20:29:35.828 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 48 (collect at utils.scala:26) finished in 0,468 s
24/05/24 20:29:35.828 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:29:35.828 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:29:35.828 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:29:35.828 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:29:35.848 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:29:35.849 dag-scheduler-event-loop INFO DAGScheduler: Got job 41 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:35.849 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 50 (collect at utils.scala:26)
24/05/24 20:29:35.849 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 49)
24/05/24 20:29:35.849 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:35.850 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[115] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:35.852 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 13.7 KiB, free 883.5 MiB)
24/05/24 20:29:35.853 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.5 MiB)
24/05/24 20:29:35.853 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:29:35.854 dag-scheduler-event-loop INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:35.855 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[115] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:35.855 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
24/05/24 20:29:35.857 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 40) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:29:35.857 Executor task launch worker for task 0.0 in stage 50.0 (TID 40) INFO Executor: Running task 0.0 in stage 50.0 (TID 40)
24/05/24 20:29:35.861 Executor task launch worker for task 0.0 in stage 50.0 (TID 40) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:29:35.862 Executor task launch worker for task 0.0 in stage 50.0 (TID 40) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/05/24 20:29:35.867 Executor task launch worker for task 0.0 in stage 50.0 (TID 40) INFO Executor: Finished task 0.0 in stage 50.0 (TID 40). 3909 bytes result sent to driver
24/05/24 20:29:35.869 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 40) in 12 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:35.869 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
24/05/24 20:29:35.869 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 50 (collect at utils.scala:26) finished in 0,018 s
24/05/24 20:29:35.870 dag-scheduler-event-loop INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:29:35.870 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
24/05/24 20:29:35.870 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 41 finished: collect at utils.scala:26, took 0,021766 s
24/05/24 20:29:37.946 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:29:37.947 dag-scheduler-event-loop INFO DAGScheduler: Got job 42 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:29:37.947 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 51 (collect at utils.scala:26)
24/05/24 20:29:37.947 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:29:37.947 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:29:37.948 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[117] at collect at utils.scala:26), which has no missing parents
24/05/24 20:29:37.950 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 20.3 KiB, free 883.4 MiB)
24/05/24 20:29:37.953 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.4 MiB)
24/05/24 20:29:37.953 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:29:37.954 dag-scheduler-event-loop INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535
24/05/24 20:29:37.954 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[117] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:29:37.954 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
24/05/24 20:29:38.251 dispatcher-event-loop-0 WARN TaskSetManager: Stage 51 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:29:38.251 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 41) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:29:38.251 Executor task launch worker for task 0.0 in stage 51.0 (TID 41) INFO Executor: Running task 0.0 in stage 51.0 (TID 41)
24/05/24 20:29:39.410 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_39_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:29:39.418 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_40_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:29:40.084 Executor task launch worker for task 0.0 in stage 51.0 (TID 41) INFO MemoryStore: Block taskresult_41 stored as bytes in memory (estimated size 13.2 MiB, free 870.3 MiB)
24/05/24 20:29:40.085 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_41 in memory on DESKTOP-LH06ASP:51464 (size: 13.2 MiB, free: 870.3 MiB)
24/05/24 20:29:40.085 Executor task launch worker for task 0.0 in stage 51.0 (TID 41) INFO Executor: Finished task 0.0 in stage 51.0 (TID 41). 13828710 bytes result sent via BlockManager)
24/05/24 20:29:40.212 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 41) in 2257 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:29:40.212 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
24/05/24 20:29:40.213 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 51 (collect at utils.scala:26) finished in 2,264 s
24/05/24 20:29:40.213 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_41 on DESKTOP-LH06ASP:51464 in memory (size: 13.2 MiB, free: 883.5 MiB)
24/05/24 20:29:40.213 dag-scheduler-event-loop INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:29:40.213 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
24/05/24 20:29:40.214 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 42 finished: collect at utils.scala:26, took 2,267469 s
24/05/24 20:29:43.399 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_41_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:30:21.704 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:30:21.705 dag-scheduler-event-loop INFO DAGScheduler: Got job 43 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:30:21.705 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 52 (collect at utils.scala:26)
24/05/24 20:30:21.705 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:21.705 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:21.706 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[119] at collect at utils.scala:26), which has no missing parents
24/05/24 20:30:21.710 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:30:21.711 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:30:21.711 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:30:21.712 dag-scheduler-event-loop INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:21.712 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[119] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:21.712 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
24/05/24 20:30:21.713 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 42) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:30:21.713 Executor task launch worker for task 0.0 in stage 52.0 (TID 42) INFO Executor: Running task 0.0 in stage 52.0 (TID 42)
24/05/24 20:30:21.718 Executor task launch worker for task 0.0 in stage 52.0 (TID 42) INFO Executor: Finished task 0.0 in stage 52.0 (TID 42). 1328 bytes result sent to driver
24/05/24 20:30:21.719 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 42) in 7 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:21.719 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
24/05/24 20:30:21.720 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 52 (collect at utils.scala:26) finished in 0,011 s
24/05/24 20:30:21.720 dag-scheduler-event-loop INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:21.720 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
24/05/24 20:30:21.720 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 43 finished: collect at utils.scala:26, took 0,014659 s
24/05/24 20:30:22.077 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:30:22.078 dag-scheduler-event-loop INFO DAGScheduler: Got job 44 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:30:22.078 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 53 (collect at utils.scala:26)
24/05/24 20:30:22.078 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:22.078 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:22.079 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[121] at collect at utils.scala:26), which has no missing parents
24/05/24 20:30:22.081 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:30:22.082 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:30:22.083 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:30:22.083 dag-scheduler-event-loop INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:22.084 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[121] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:22.084 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
24/05/24 20:30:22.085 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 43) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:30:22.086 Executor task launch worker for task 0.0 in stage 53.0 (TID 43) INFO Executor: Running task 0.0 in stage 53.0 (TID 43)
24/05/24 20:30:22.089 Executor task launch worker for task 0.0 in stage 53.0 (TID 43) INFO Executor: Finished task 0.0 in stage 53.0 (TID 43). 1328 bytes result sent to driver
24/05/24 20:30:22.090 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 43) in 6 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:22.090 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
24/05/24 20:30:22.090 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 53 (collect at utils.scala:26) finished in 0,010 s
24/05/24 20:30:22.090 dag-scheduler-event-loop INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:22.091 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
24/05/24 20:30:22.091 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 44 finished: collect at utils.scala:26, took 0,013394 s
24/05/24 20:30:26.708 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 124 (collect at StringIndexer.scala:204) as input to shuffle 10
24/05/24 20:30:26.708 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 45 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:26.708 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 54 (collect at StringIndexer.scala:204)
24/05/24 20:30:26.708 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:26.708 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:26.710 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 54 (MapPartitionsRDD[124] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:26.719 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 52.5 KiB, free 883.4 MiB)
24/05/24 20:30:26.720 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.4 MiB)
24/05/24 20:30:26.720 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:30:26.720 dag-scheduler-event-loop INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:26.720 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[124] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:26.721 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
24/05/24 20:30:26.773 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_42_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:30:26.777 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_43_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:30:26.879 dispatcher-event-loop-0 WARN TaskSetManager: Stage 54 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:26.879 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 44) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:30:26.879 Executor task launch worker for task 0.0 in stage 54.0 (TID 44) INFO Executor: Running task 0.0 in stage 54.0 (TID 44)
24/05/24 20:30:27.576 Executor task launch worker for task 0.0 in stage 54.0 (TID 44) INFO Executor: Finished task 0.0 in stage 54.0 (TID 44). 2342 bytes result sent to driver
24/05/24 20:30:27.577 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 44) in 856 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:27.577 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
24/05/24 20:30:27.578 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 54 (collect at StringIndexer.scala:204) finished in 0,867 s
24/05/24 20:30:27.578 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:30:27.578 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:30:27.578 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:30:27.578 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:30:27.609 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:30:27.609 dag-scheduler-event-loop INFO DAGScheduler: Got job 46 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:27.610 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 56 (collect at StringIndexer.scala:204)
24/05/24 20:30:27.610 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
24/05/24 20:30:27.610 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:27.610 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[127] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:27.613 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:30:27.614 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.4 MiB)
24/05/24 20:30:27.615 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:30:27.615 dag-scheduler-event-loop INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:27.616 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[127] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:27.616 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
24/05/24 20:30:27.616 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 45) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:30:27.617 Executor task launch worker for task 0.0 in stage 56.0 (TID 45) INFO Executor: Running task 0.0 in stage 56.0 (TID 45)
24/05/24 20:30:27.620 Executor task launch worker for task 0.0 in stage 56.0 (TID 45) INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:30:27.620 Executor task launch worker for task 0.0 in stage 56.0 (TID 45) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:30:27.638 Executor task launch worker for task 0.0 in stage 56.0 (TID 45) INFO Executor: Finished task 0.0 in stage 56.0 (TID 45). 7827 bytes result sent to driver
24/05/24 20:30:27.638 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 45) in 22 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:27.639 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
24/05/24 20:30:27.639 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 56 (collect at StringIndexer.scala:204) finished in 0,028 s
24/05/24 20:30:27.639 dag-scheduler-event-loop INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:27.639 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
24/05/24 20:30:27.639 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 46 finished: collect at StringIndexer.scala:204, took 0,030464 s
24/05/24 20:30:27.741 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 130 (collect at StringIndexer.scala:204) as input to shuffle 11
24/05/24 20:30:27.742 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 47 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:27.742 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 57 (collect at StringIndexer.scala:204)
24/05/24 20:30:27.742 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:27.742 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:27.742 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 57 (MapPartitionsRDD[130] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:27.750 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 52.5 KiB, free 883.3 MiB)
24/05/24 20:30:27.752 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.3 MiB)
24/05/24 20:30:27.753 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:30:27.753 dag-scheduler-event-loop INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:27.753 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 57 (MapPartitionsRDD[130] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:27.753 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
24/05/24 20:30:27.876 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_45_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:30:27.995 dispatcher-event-loop-0 WARN TaskSetManager: Stage 57 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:27.995 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 46) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:30:28.050 Executor task launch worker for task 0.0 in stage 57.0 (TID 46) INFO Executor: Running task 0.0 in stage 57.0 (TID 46)
24/05/24 20:30:28.284 Executor task launch worker for task 0.0 in stage 57.0 (TID 46) INFO Executor: Finished task 0.0 in stage 57.0 (TID 46). 2299 bytes result sent to driver
24/05/24 20:30:28.285 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 46) in 531 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:28.285 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
24/05/24 20:30:28.285 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 57 (collect at StringIndexer.scala:204) finished in 0,542 s
24/05/24 20:30:28.285 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:30:28.285 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:30:28.285 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:30:28.286 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:30:28.297 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:30:28.298 dag-scheduler-event-loop INFO DAGScheduler: Got job 48 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:28.298 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 59 (collect at StringIndexer.scala:204)
24/05/24 20:30:28.298 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)
24/05/24 20:30:28.298 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:28.299 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[133] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:28.301 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 49.1 KiB, free 883.3 MiB)
24/05/24 20:30:28.302 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.3 MiB)
24/05/24 20:30:28.303 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:30:28.303 dag-scheduler-event-loop INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:28.303 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[133] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:28.303 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
24/05/24 20:30:28.304 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 47) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:30:28.304 Executor task launch worker for task 0.0 in stage 59.0 (TID 47) INFO Executor: Running task 0.0 in stage 59.0 (TID 47)
24/05/24 20:30:28.307 Executor task launch worker for task 0.0 in stage 59.0 (TID 47) INFO ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:30:28.307 Executor task launch worker for task 0.0 in stage 59.0 (TID 47) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:30:28.319 Executor task launch worker for task 0.0 in stage 59.0 (TID 47) INFO Executor: Finished task 0.0 in stage 59.0 (TID 47). 5140 bytes result sent to driver
24/05/24 20:30:28.320 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 47) in 16 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:28.320 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
24/05/24 20:30:28.321 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 59 (collect at StringIndexer.scala:204) finished in 0,022 s
24/05/24 20:30:28.321 dag-scheduler-event-loop INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:28.321 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
24/05/24 20:30:28.321 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 48 finished: collect at StringIndexer.scala:204, took 0,023574 s
24/05/24 20:30:28.394 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 136 (collect at StringIndexer.scala:204) as input to shuffle 12
24/05/24 20:30:28.394 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 49 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:28.394 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 60 (collect at StringIndexer.scala:204)
24/05/24 20:30:28.394 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:28.394 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:28.395 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 60 (MapPartitionsRDD[136] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:28.401 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:30:28.403 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.2 MiB)
24/05/24 20:30:28.403 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:30:28.403 dag-scheduler-event-loop INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:28.403 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 60 (MapPartitionsRDD[136] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:28.403 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
24/05/24 20:30:28.458 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_47_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:30:28.563 dispatcher-event-loop-0 WARN TaskSetManager: Stage 60 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:28.563 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 48) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:30:28.565 Executor task launch worker for task 0.0 in stage 60.0 (TID 48) INFO Executor: Running task 0.0 in stage 60.0 (TID 48)
24/05/24 20:30:28.844 Executor task launch worker for task 0.0 in stage 60.0 (TID 48) INFO Executor: Finished task 0.0 in stage 60.0 (TID 48). 2342 bytes result sent to driver
24/05/24 20:30:28.845 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 48) in 441 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:28.845 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
24/05/24 20:30:28.845 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 60 (collect at StringIndexer.scala:204) finished in 0,450 s
24/05/24 20:30:28.845 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:30:28.845 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:30:28.845 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:30:28.846 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:30:28.858 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:30:28.859 dag-scheduler-event-loop INFO DAGScheduler: Got job 50 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:28.859 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 62 (collect at StringIndexer.scala:204)
24/05/24 20:30:28.859 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 61)
24/05/24 20:30:28.859 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:28.860 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[139] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:28.862 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 49.1 KiB, free 883.2 MiB)
24/05/24 20:30:28.863 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 883.2 MiB)
24/05/24 20:30:28.863 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.4 KiB, free: 883.4 MiB)
24/05/24 20:30:28.864 dag-scheduler-event-loop INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:28.864 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[139] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:28.864 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
24/05/24 20:30:28.864 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 49) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:30:28.865 Executor task launch worker for task 0.0 in stage 62.0 (TID 49) INFO Executor: Running task 0.0 in stage 62.0 (TID 49)
24/05/24 20:30:28.867 Executor task launch worker for task 0.0 in stage 62.0 (TID 49) INFO ShuffleBlockFetcherIterator: Getting 1 (99.6 KiB) non-empty blocks including 1 (99.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:30:28.867 Executor task launch worker for task 0.0 in stage 62.0 (TID 49) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:30:28.883 Executor task launch worker for task 0.0 in stage 62.0 (TID 49) INFO Executor: Finished task 0.0 in stage 62.0 (TID 49). 101247 bytes result sent to driver
24/05/24 20:30:28.884 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 49) in 20 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:28.884 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
24/05/24 20:30:28.884 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 62 (collect at StringIndexer.scala:204) finished in 0,023 s
24/05/24 20:30:28.884 dag-scheduler-event-loop INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:28.884 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
24/05/24 20:30:28.884 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 50 finished: collect at StringIndexer.scala:204, took 0,025826 s
24/05/24 20:30:29.124 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_49_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.4 KiB, free: 883.4 MiB)
24/05/24 20:30:29.186 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 24.9503 ms
24/05/24 20:30:29.191 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 142 (collect at StringIndexer.scala:204) as input to shuffle 13
24/05/24 20:30:29.191 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 51 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:29.191 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 63 (collect at StringIndexer.scala:204)
24/05/24 20:30:29.191 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:29.192 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:29.192 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 63 (MapPartitionsRDD[142] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:29.202 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:30:29.204 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.2 MiB)
24/05/24 20:30:29.205 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:30:29.205 dag-scheduler-event-loop INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:29.205 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 63 (MapPartitionsRDD[142] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:29.205 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
24/05/24 20:30:29.360 dispatcher-event-loop-0 WARN TaskSetManager: Stage 63 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:29.360 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 50) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:30:29.360 Executor task launch worker for task 0.0 in stage 63.0 (TID 50) INFO Executor: Running task 0.0 in stage 63.0 (TID 50)
24/05/24 20:30:30.041 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_44_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:30:30.043 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_48_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:30:30.049 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_46_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:30:30.211 Executor task launch worker for task 0.0 in stage 63.0 (TID 50) INFO Executor: Finished task 0.0 in stage 63.0 (TID 50). 2342 bytes result sent to driver
24/05/24 20:30:30.211 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 50) in 1005 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:30.211 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
24/05/24 20:30:30.212 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 63 (collect at StringIndexer.scala:204) finished in 1,019 s
24/05/24 20:30:30.212 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:30:30.212 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:30:30.212 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:30:30.212 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:30:30.234 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:30:30.235 dag-scheduler-event-loop INFO DAGScheduler: Got job 52 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:30:30.235 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 65 (collect at StringIndexer.scala:204)
24/05/24 20:30:30.235 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 64)
24/05/24 20:30:30.235 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:30.236 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[145] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:30:30.239 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:30:30.240 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.4 MiB)
24/05/24 20:30:30.240 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:30:30.241 dag-scheduler-event-loop INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:30.241 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[145] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:30.241 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
24/05/24 20:30:30.242 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 51) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:30:30.242 Executor task launch worker for task 0.0 in stage 65.0 (TID 51) INFO Executor: Running task 0.0 in stage 65.0 (TID 51)
24/05/24 20:30:30.245 Executor task launch worker for task 0.0 in stage 65.0 (TID 51) INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:30:30.245 Executor task launch worker for task 0.0 in stage 65.0 (TID 51) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:30:30.256 Executor task launch worker for task 0.0 in stage 65.0 (TID 51) INFO Executor: Finished task 0.0 in stage 65.0 (TID 51). 4920 bytes result sent to driver
24/05/24 20:30:30.257 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 51) in 16 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:30.257 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
24/05/24 20:30:30.258 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 65 (collect at StringIndexer.scala:204) finished in 0,021 s
24/05/24 20:30:30.258 dag-scheduler-event-loop INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:30.258 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
24/05/24 20:30:30.258 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 52 finished: collect at StringIndexer.scala:204, took 0,023742 s
24/05/24 20:30:30.275 nioEventLoopGroup-2-2 INFO Instrumentation: [fb985a17] training finished
24/05/24 20:30:30.493 nioEventLoopGroup-2-2 INFO Instrumentation: [dbfad6be] training finished
24/05/24 20:30:30.506 nioEventLoopGroup-2-2 INFO Instrumentation: [fbbe5a63] Stage class: RandomForestClassifier
24/05/24 20:30:30.506 nioEventLoopGroup-2-2 INFO Instrumentation: [fbbe5a63] Stage uid: random_forest__bb2b9192_8925_4df6_ab87_155bdac6f7c8
24/05/24 20:30:30.599 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_51_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:30:30.739 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 81.0899 ms
24/05/24 20:30:30.843 nioEventLoopGroup-2-2 INFO Instrumentation: [fbbe5a63] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
24/05/24 20:30:31.114 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 55.8121 ms
24/05/24 20:30:31.205 nioEventLoopGroup-2-2 INFO Instrumentation: [fbbe5a63] {"subsamplingRate":1.0,"impurity":"gini","featuresCol":"features","maxDepth":5,"minInstancesPerNode":1,"featureSubsetStrategy":"auto","checkpointInterval":10,"minInfoGain":0.0,"cacheNodeIds":false,"labelCol":"label","predictionCol":"prediction","maxMemoryInMB":256,"maxBins":32,"rawPredictionCol":"rawPrediction","probabilityCol":"probability","numTrees":10}
24/05/24 20:30:31.268 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
24/05/24 20:30:31.269 dag-scheduler-event-loop INFO DAGScheduler: Got job 53 (take at DecisionTreeMetadata.scala:119) with 1 output partitions
24/05/24 20:30:31.269 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 66 (take at DecisionTreeMetadata.scala:119)
24/05/24 20:30:31.269 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:31.269 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:31.270 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[156] at map at DecisionTreeMetadata.scala:119), which has no missing parents
24/05/24 20:30:31.319 dag-scheduler-event-loop WARN DAGScheduler: Broadcasting large task binary with size 1667.2 KiB
24/05/24 20:30:31.319 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 1667.3 KiB, free 881.8 MiB)
24/05/24 20:30:31.322 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 339.7 KiB, free 881.5 MiB)
24/05/24 20:30:31.323 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 339.7 KiB, free: 883.1 MiB)
24/05/24 20:30:31.323 dag-scheduler-event-loop INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:31.324 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[156] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:31.324 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0
24/05/24 20:30:31.433 dispatcher-event-loop-0 WARN TaskSetManager: Stage 66 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:31.434 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 52) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:30:31.434 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO Executor: Running task 0.0 in stage 66.0 (TID 52)
24/05/24 20:30:31.611 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO CodeGenerator: Code generated in 6.5159 ms
24/05/24 20:30:32.208 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO CodeGenerator: Code generated in 3.5112 ms
24/05/24 20:30:32.223 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO CodeGenerator: Code generated in 6.261 ms
24/05/24 20:30:32.239 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO CodeGenerator: Code generated in 3.8831 ms
24/05/24 20:30:32.243 Executor task launch worker for task 0.0 in stage 66.0 (TID 52) INFO Executor: Finished task 0.0 in stage 66.0 (TID 52). 1528 bytes result sent to driver
24/05/24 20:30:32.244 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 52) in 919 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:30:32.244 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
24/05/24 20:30:32.244 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 66 (take at DecisionTreeMetadata.scala:119) finished in 0,973 s
24/05/24 20:30:32.244 dag-scheduler-event-loop INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:30:32.244 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
24/05/24 20:30:32.244 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 53 finished: take at DecisionTreeMetadata.scala:119, took 0,976030 s
24/05/24 20:30:32.254 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
24/05/24 20:30:32.254 dag-scheduler-event-loop INFO DAGScheduler: Got job 54 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions
24/05/24 20:30:32.254 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 67 (aggregate at DecisionTreeMetadata.scala:125)
24/05/24 20:30:32.254 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:30:32.254 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:30:32.255 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[155] at retag at RandomForest.scala:274), which has no missing parents
24/05/24 20:30:32.288 dag-scheduler-event-loop WARN DAGScheduler: Broadcasting large task binary with size 1667.3 KiB
24/05/24 20:30:32.289 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 1667.4 KiB, free 879.8 MiB)
24/05/24 20:30:32.292 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 339.8 KiB, free 879.5 MiB)
24/05/24 20:30:32.293 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 339.8 KiB, free: 882.8 MiB)
24/05/24 20:30:32.293 dag-scheduler-event-loop INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1535
24/05/24 20:30:32.294 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[155] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))
24/05/24 20:30:32.294 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
24/05/24 20:30:32.403 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_52_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 339.7 KiB, free: 883.1 MiB)
24/05/24 20:30:32.475 dispatcher-event-loop-1 WARN TaskSetManager: Stage 67 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:30:32.475 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 53) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:30:32.475 Executor task launch worker for task 0.0 in stage 67.0 (TID 53) INFO Executor: Running task 0.0 in stage 67.0 (TID 53)
24/05/24 20:30:33.175 Executor task launch worker for task 0.0 in stage 67.0 (TID 53) ERROR Executor: Exception in task 0.0 in stage 67.0 (TID 53)
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more
24/05/24 20:30:33.200 task-result-getter-1 WARN TaskSetManager: Lost task 0.0 in stage 67.0 (TID 53) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:30:33.203 task-result-getter-1 ERROR TaskSetManager: Task 0 in stage 67.0 failed 1 times; aborting job
24/05/24 20:30:33.205 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
24/05/24 20:30:33.208 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 67
24/05/24 20:30:33.209 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage cancelled
24/05/24 20:30:33.210 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 67 (aggregate at DecisionTreeMetadata.scala:125) failed in 0,953 s due to Job aborted due to stage failure: Task 0 in stage 67.0 failed 1 times, most recent failure: Lost task 0.0 in stage 67.0 (TID 53) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
24/05/24 20:30:33.212 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 54 failed: aggregate at DecisionTreeMetadata.scala:125, took 0,957991 s
24/05/24 20:30:33.213 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 67.0 failed 1 times, most recent failure: Lost task 0.0 in stage 67.0 (TID 53) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:30:33.214 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 67.0 failed 1 times, most recent failure: Lost task 0.0 in stage 67.0 (TID 53) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_b666b7630c10:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,onehot_d4e77e22b15e:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_964004638232:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_TOTAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_PROP_OCUP_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_RHABITUAL_ARRENDADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_H_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_M_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_0_14_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_15_24_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_25_64_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double,solarenergy:double,uvindex_double_r_formula__0eb30a6b_a2ee_4965_b183_6cb64945e522:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:31:45.906 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:31:45.906 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:31:45.910 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:31:45.910 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:31:45.911 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:31:45.911 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:31:45.952 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:31:45.953 dag-scheduler-event-loop INFO DAGScheduler: Got job 55 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:31:45.953 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 68 (collect at utils.scala:26)
24/05/24 20:31:45.953 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:31:45.953 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:31:45.954 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[159] at collect at utils.scala:26), which has no missing parents
24/05/24 20:31:45.955 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 7.1 KiB, free 881.5 MiB)
24/05/24 20:31:45.957 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 881.5 MiB)
24/05/24 20:31:45.958 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 883.1 MiB)
24/05/24 20:31:45.958 dag-scheduler-event-loop INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1535
24/05/24 20:31:45.959 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[159] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:31:45.959 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0
24/05/24 20:31:45.959 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 54) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 8189 bytes) 
24/05/24 20:31:45.960 Executor task launch worker for task 0.0 in stage 68.0 (TID 54) INFO Executor: Running task 0.0 in stage 68.0 (TID 54)
24/05/24 20:31:45.962 Executor task launch worker for task 0.0 in stage 68.0 (TID 54) INFO Executor: Finished task 0.0 in stage 68.0 (TID 54). 1590 bytes result sent to driver
24/05/24 20:31:45.963 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 54) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:31:45.963 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
24/05/24 20:31:45.963 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 68 (collect at utils.scala:26) finished in 0,009 s
24/05/24 20:31:45.963 dag-scheduler-event-loop INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:31:45.963 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
24/05/24 20:31:45.964 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 55 finished: collect at utils.scala:26, took 0,011082 s
24/05/24 20:31:59.316 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_54_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 883.1 MiB)
24/05/24 20:31:59.332 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_50_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.2 MiB)
24/05/24 20:31:59.341 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_53_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 339.8 KiB, free: 883.5 MiB)
24/05/24 20:32:02.689 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:02.690 dag-scheduler-event-loop INFO DAGScheduler: Got job 56 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:02.690 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 69 (collect at utils.scala:26)
24/05/24 20:32:02.690 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:02.690 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:02.690 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[162] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:02.691 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:32:02.692 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:32:02.692 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:02.692 dag-scheduler-event-loop INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:02.693 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[162] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:02.693 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
24/05/24 20:32:02.693 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 55) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:32:02.693 Executor task launch worker for task 0.0 in stage 69.0 (TID 55) INFO Executor: Running task 0.0 in stage 69.0 (TID 55)
24/05/24 20:32:02.695 Executor task launch worker for task 0.0 in stage 69.0 (TID 55) INFO Executor: Finished task 0.0 in stage 69.0 (TID 55). 1285 bytes result sent to driver
24/05/24 20:32:02.695 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 55) in 2 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:02.696 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
24/05/24 20:32:02.696 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 69 (collect at utils.scala:26) finished in 0,006 s
24/05/24 20:32:02.696 dag-scheduler-event-loop INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:02.696 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
24/05/24 20:32:02.697 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 56 finished: collect at utils.scala:26, took 0,007197 s
24/05/24 20:32:03.334 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:03.334 dag-scheduler-event-loop INFO DAGScheduler: Got job 57 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:03.334 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 70 (collect at utils.scala:26)
24/05/24 20:32:03.334 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:03.334 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:03.335 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[164] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:03.336 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:32:03.337 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:32:03.337 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:03.337 dag-scheduler-event-loop INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:03.337 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[164] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:03.337 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0
24/05/24 20:32:03.338 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 56) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:32:03.338 Executor task launch worker for task 0.0 in stage 70.0 (TID 56) INFO Executor: Running task 0.0 in stage 70.0 (TID 56)
24/05/24 20:32:03.340 Executor task launch worker for task 0.0 in stage 70.0 (TID 56) INFO Executor: Finished task 0.0 in stage 70.0 (TID 56). 1285 bytes result sent to driver
24/05/24 20:32:03.341 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 56) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:03.341 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
24/05/24 20:32:03.341 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 70 (collect at utils.scala:26) finished in 0,006 s
24/05/24 20:32:03.342 dag-scheduler-event-loop INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:03.342 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
24/05/24 20:32:03.342 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 57 finished: collect at utils.scala:26, took 0,007938 s
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 166 (collect at utils.scala:26) as input to shuffle 14
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 58 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 71 (collect at utils.scala:26)
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:05.084 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 71 (MapPartitionsRDD[166] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:05.087 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 12.9 KiB, free 883.5 MiB)
24/05/24 20:32:05.089 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.5 MiB)
24/05/24 20:32:05.089 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:32:05.089 dag-scheduler-event-loop INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:05.090 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 71 (MapPartitionsRDD[166] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:05.090 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
24/05/24 20:32:05.173 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_55_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:05.176 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_56_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:05.262 dispatcher-event-loop-1 WARN TaskSetManager: Stage 71 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:05.262 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 57) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:32:05.263 Executor task launch worker for task 0.0 in stage 71.0 (TID 57) INFO Executor: Running task 0.0 in stage 71.0 (TID 57)
24/05/24 20:32:05.386 Executor task launch worker for task 0.0 in stage 71.0 (TID 57) INFO Executor: Finished task 0.0 in stage 71.0 (TID 57). 1888 bytes result sent to driver
24/05/24 20:32:05.387 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 57) in 297 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:05.387 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
24/05/24 20:32:05.388 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 71 (collect at utils.scala:26) finished in 0,303 s
24/05/24 20:32:05.388 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:32:05.388 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:32:05.388 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:32:05.388 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:32:05.403 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:05.404 dag-scheduler-event-loop INFO DAGScheduler: Got job 59 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:05.404 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 73 (collect at utils.scala:26)
24/05/24 20:32:05.404 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)
24/05/24 20:32:05.404 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:05.404 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[169] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:05.406 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 13.7 KiB, free 883.5 MiB)
24/05/24 20:32:05.408 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.5 MiB)
24/05/24 20:32:05.408 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:32:05.409 dag-scheduler-event-loop INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:05.409 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[169] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:05.409 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0
24/05/24 20:32:05.410 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 58) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:32:05.410 Executor task launch worker for task 0.0 in stage 73.0 (TID 58) INFO Executor: Running task 0.0 in stage 73.0 (TID 58)
24/05/24 20:32:05.412 Executor task launch worker for task 0.0 in stage 73.0 (TID 58) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:32:05.412 Executor task launch worker for task 0.0 in stage 73.0 (TID 58) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:32:05.415 Executor task launch worker for task 0.0 in stage 73.0 (TID 58) INFO Executor: Finished task 0.0 in stage 73.0 (TID 58). 3909 bytes result sent to driver
24/05/24 20:32:05.415 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 58) in 5 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:05.415 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
24/05/24 20:32:05.416 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 73 (collect at utils.scala:26) finished in 0,011 s
24/05/24 20:32:05.416 dag-scheduler-event-loop INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:05.416 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
24/05/24 20:32:05.416 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 59 finished: collect at utils.scala:26, took 0,013245 s
24/05/24 20:32:05.632 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:05.635 dag-scheduler-event-loop INFO DAGScheduler: Got job 60 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:05.635 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 74 (collect at utils.scala:26)
24/05/24 20:32:05.635 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:05.635 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:05.635 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[171] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:05.636 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 20.3 KiB, free 883.4 MiB)
24/05/24 20:32:05.637 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 883.4 MiB)
24/05/24 20:32:05.638 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:32:05.638 dag-scheduler-event-loop INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:05.639 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[171] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:05.639 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0
24/05/24 20:32:05.802 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_58_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:32:05.843 dispatcher-event-loop-1 WARN TaskSetManager: Stage 74 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:05.844 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 59) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:32:05.844 Executor task launch worker for task 0.0 in stage 74.0 (TID 59) INFO Executor: Running task 0.0 in stage 74.0 (TID 59)
24/05/24 20:32:06.517 Executor task launch worker for task 0.0 in stage 74.0 (TID 59) INFO MemoryStore: Block taskresult_59 stored as bytes in memory (estimated size 13.2 MiB, free 870.3 MiB)
24/05/24 20:32:06.517 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_59 in memory on DESKTOP-LH06ASP:51464 (size: 13.2 MiB, free: 870.3 MiB)
24/05/24 20:32:06.518 Executor task launch worker for task 0.0 in stage 74.0 (TID 59) INFO Executor: Finished task 0.0 in stage 74.0 (TID 59). 13828710 bytes result sent via BlockManager)
24/05/24 20:32:06.555 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 59) in 916 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:06.555 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
24/05/24 20:32:06.556 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 74 (collect at utils.scala:26) finished in 0,921 s
24/05/24 20:32:06.557 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_59 on DESKTOP-LH06ASP:51464 in memory (size: 13.2 MiB, free: 883.5 MiB)
24/05/24 20:32:06.557 dag-scheduler-event-loop INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:06.557 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
24/05/24 20:32:06.557 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 60 finished: collect at utils.scala:26, took 0,924212 s
24/05/24 20:32:08.611 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_57_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:32:08.614 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_59_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.6 KiB, free: 883.5 MiB)
24/05/24 20:32:11.839 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:11.840 dag-scheduler-event-loop INFO DAGScheduler: Got job 61 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:11.840 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 75 (collect at utils.scala:26)
24/05/24 20:32:11.840 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:11.840 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:11.840 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[173] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:11.841 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:32:11.843 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:32:11.843 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:11.844 dag-scheduler-event-loop INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:11.844 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[173] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:11.844 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
24/05/24 20:32:11.845 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 60) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:32:11.845 Executor task launch worker for task 0.0 in stage 75.0 (TID 60) INFO Executor: Running task 0.0 in stage 75.0 (TID 60)
24/05/24 20:32:11.847 Executor task launch worker for task 0.0 in stage 75.0 (TID 60) INFO Executor: Finished task 0.0 in stage 75.0 (TID 60). 1328 bytes result sent to driver
24/05/24 20:32:11.848 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 60) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:11.848 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
24/05/24 20:32:11.849 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 75 (collect at utils.scala:26) finished in 0,007 s
24/05/24 20:32:11.849 dag-scheduler-event-loop INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:11.849 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
24/05/24 20:32:11.849 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 61 finished: collect at utils.scala:26, took 0,009702 s
24/05/24 20:32:12.208 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:32:12.208 dag-scheduler-event-loop INFO DAGScheduler: Got job 62 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:32:12.209 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 76 (collect at utils.scala:26)
24/05/24 20:32:12.209 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:12.209 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:12.209 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[175] at collect at utils.scala:26), which has no missing parents
24/05/24 20:32:12.210 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:32:12.212 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:32:12.212 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:12.212 dag-scheduler-event-loop INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:12.214 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[175] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:12.214 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0
24/05/24 20:32:12.215 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 61) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:32:12.215 Executor task launch worker for task 0.0 in stage 76.0 (TID 61) INFO Executor: Running task 0.0 in stage 76.0 (TID 61)
24/05/24 20:32:12.217 Executor task launch worker for task 0.0 in stage 76.0 (TID 61) INFO Executor: Finished task 0.0 in stage 76.0 (TID 61). 1285 bytes result sent to driver
24/05/24 20:32:12.219 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 61) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:12.219 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
24/05/24 20:32:12.219 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 76 (collect at utils.scala:26) finished in 0,010 s
24/05/24 20:32:12.220 dag-scheduler-event-loop INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:12.220 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished
24/05/24 20:32:12.220 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 62 finished: collect at utils.scala:26, took 0,011743 s
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 178 (collect at StringIndexer.scala:204) as input to shuffle 15
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 63 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 77 (collect at StringIndexer.scala:204)
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:13.225 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 77 (MapPartitionsRDD[178] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:13.231 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 52.5 KiB, free 883.4 MiB)
24/05/24 20:32:13.232 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.4 MiB)
24/05/24 20:32:13.232 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:32:13.233 dag-scheduler-event-loop INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:13.233 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 77 (MapPartitionsRDD[178] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:13.233 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0
24/05/24 20:32:13.317 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_61_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:13.319 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_60_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:32:13.384 dispatcher-event-loop-0 WARN TaskSetManager: Stage 77 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:13.384 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 62) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:32:13.384 Executor task launch worker for task 0.0 in stage 77.0 (TID 62) INFO Executor: Running task 0.0 in stage 77.0 (TID 62)
24/05/24 20:32:14.358 Executor task launch worker for task 0.0 in stage 77.0 (TID 62) INFO Executor: Finished task 0.0 in stage 77.0 (TID 62). 2299 bytes result sent to driver
24/05/24 20:32:14.359 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 62) in 1125 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:14.359 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
24/05/24 20:32:14.359 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 77 (collect at StringIndexer.scala:204) finished in 1,133 s
24/05/24 20:32:14.359 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:32:14.359 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:32:14.359 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:32:14.359 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:32:14.395 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:32:14.396 dag-scheduler-event-loop INFO DAGScheduler: Got job 64 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:14.396 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 79 (collect at StringIndexer.scala:204)
24/05/24 20:32:14.396 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 78)
24/05/24 20:32:14.396 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:14.396 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 79 (MapPartitionsRDD[181] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:14.398 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:32:14.399 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.4 MiB)
24/05/24 20:32:14.399 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:32:14.400 dag-scheduler-event-loop INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:14.400 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 79 (MapPartitionsRDD[181] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:14.400 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0
24/05/24 20:32:14.401 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 63) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:32:14.401 Executor task launch worker for task 0.0 in stage 79.0 (TID 63) INFO Executor: Running task 0.0 in stage 79.0 (TID 63)
24/05/24 20:32:14.405 Executor task launch worker for task 0.0 in stage 79.0 (TID 63) INFO ShuffleBlockFetcherIterator: Getting 1 (3.2 KiB) non-empty blocks including 1 (3.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:32:14.405 Executor task launch worker for task 0.0 in stage 79.0 (TID 63) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:32:14.417 Executor task launch worker for task 0.0 in stage 79.0 (TID 63) INFO Executor: Finished task 0.0 in stage 79.0 (TID 63). 7784 bytes result sent to driver
24/05/24 20:32:14.418 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 63) in 17 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:14.418 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
24/05/24 20:32:14.418 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 79 (collect at StringIndexer.scala:204) finished in 0,021 s
24/05/24 20:32:14.418 dag-scheduler-event-loop INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:14.419 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 79: Stage finished
24/05/24 20:32:14.419 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 64 finished: collect at StringIndexer.scala:204, took 0,023568 s
24/05/24 20:32:14.497 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 184 (collect at StringIndexer.scala:204) as input to shuffle 16
24/05/24 20:32:14.498 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 65 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:14.498 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 80 (collect at StringIndexer.scala:204)
24/05/24 20:32:14.498 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:14.498 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:14.498 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[184] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:14.503 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 52.5 KiB, free 883.3 MiB)
24/05/24 20:32:14.504 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.3 MiB)
24/05/24 20:32:14.504 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:32:14.505 dag-scheduler-event-loop INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:14.505 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[184] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:14.505 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks resource profile 0
24/05/24 20:32:14.607 dispatcher-event-loop-1 WARN TaskSetManager: Stage 80 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:14.607 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 64) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:32:14.608 Executor task launch worker for task 0.0 in stage 80.0 (TID 64) INFO Executor: Running task 0.0 in stage 80.0 (TID 64)
24/05/24 20:32:14.661 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_63_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:32:15.403 Executor task launch worker for task 0.0 in stage 80.0 (TID 64) INFO Executor: Finished task 0.0 in stage 80.0 (TID 64). 2342 bytes result sent to driver
24/05/24 20:32:15.403 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 64) in 898 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:15.403 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
24/05/24 20:32:15.404 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 80 (collect at StringIndexer.scala:204) finished in 0,906 s
24/05/24 20:32:15.404 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:32:15.404 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:32:15.404 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:32:15.404 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:32:15.416 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:32:15.417 dag-scheduler-event-loop INFO DAGScheduler: Got job 66 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:15.417 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 82 (collect at StringIndexer.scala:204)
24/05/24 20:32:15.417 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 81)
24/05/24 20:32:15.417 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:15.417 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[187] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:15.419 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 49.1 KiB, free 883.3 MiB)
24/05/24 20:32:15.426 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.3 MiB)
24/05/24 20:32:15.427 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:32:15.427 dag-scheduler-event-loop INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:15.427 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[187] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:15.427 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0
24/05/24 20:32:15.428 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 65) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:32:15.428 Executor task launch worker for task 0.0 in stage 82.0 (TID 65) INFO Executor: Running task 0.0 in stage 82.0 (TID 65)
24/05/24 20:32:15.430 Executor task launch worker for task 0.0 in stage 82.0 (TID 65) INFO ShuffleBlockFetcherIterator: Getting 1 (539.0 B) non-empty blocks including 1 (539.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:32:15.430 Executor task launch worker for task 0.0 in stage 82.0 (TID 65) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:32:15.441 Executor task launch worker for task 0.0 in stage 82.0 (TID 65) INFO Executor: Finished task 0.0 in stage 82.0 (TID 65). 5140 bytes result sent to driver
24/05/24 20:32:15.442 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 65) in 14 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:15.442 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
24/05/24 20:32:15.443 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 82 (collect at StringIndexer.scala:204) finished in 0,025 s
24/05/24 20:32:15.443 dag-scheduler-event-loop INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:15.443 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished
24/05/24 20:32:15.443 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 66 finished: collect at StringIndexer.scala:204, took 0,026666 s
24/05/24 20:32:15.521 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 190 (collect at StringIndexer.scala:204) as input to shuffle 17
24/05/24 20:32:15.521 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 67 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:15.521 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 83 (collect at StringIndexer.scala:204)
24/05/24 20:32:15.521 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:15.521 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:15.522 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 83 (MapPartitionsRDD[190] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:15.527 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:32:15.528 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.2 MiB)
24/05/24 20:32:15.528 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:32:15.528 dag-scheduler-event-loop INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:15.528 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 83 (MapPartitionsRDD[190] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:15.529 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks resource profile 0
24/05/24 20:32:15.641 dispatcher-event-loop-1 WARN TaskSetManager: Stage 83 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:15.641 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 66) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:32:15.641 Executor task launch worker for task 0.0 in stage 83.0 (TID 66) INFO Executor: Running task 0.0 in stage 83.0 (TID 66)
24/05/24 20:32:15.682 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_65_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:32:16.362 Executor task launch worker for task 0.0 in stage 83.0 (TID 66) INFO Executor: Finished task 0.0 in stage 83.0 (TID 66). 2342 bytes result sent to driver
24/05/24 20:32:16.363 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 66) in 834 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:16.363 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
24/05/24 20:32:16.363 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 83 (collect at StringIndexer.scala:204) finished in 0,841 s
24/05/24 20:32:16.363 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:32:16.363 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:32:16.363 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:32:16.364 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:32:16.376 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:32:16.377 dag-scheduler-event-loop INFO DAGScheduler: Got job 68 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:16.377 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 85 (collect at StringIndexer.scala:204)
24/05/24 20:32:16.377 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 84)
24/05/24 20:32:16.377 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:16.377 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 85 (MapPartitionsRDD[193] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:16.378 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 49.1 KiB, free 883.2 MiB)
24/05/24 20:32:16.380 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.2 MiB)
24/05/24 20:32:16.380 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:32:16.380 dag-scheduler-event-loop INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:16.380 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 85 (MapPartitionsRDD[193] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:16.381 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks resource profile 0
24/05/24 20:32:16.381 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 67) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:32:16.382 Executor task launch worker for task 0.0 in stage 85.0 (TID 67) INFO Executor: Running task 0.0 in stage 85.0 (TID 67)
24/05/24 20:32:16.386 Executor task launch worker for task 0.0 in stage 85.0 (TID 67) INFO ShuffleBlockFetcherIterator: Getting 1 (99.6 KiB) non-empty blocks including 1 (99.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:32:16.386 Executor task launch worker for task 0.0 in stage 85.0 (TID 67) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:32:16.413 Executor task launch worker for task 0.0 in stage 85.0 (TID 67) INFO Executor: Finished task 0.0 in stage 85.0 (TID 67). 101333 bytes result sent to driver
24/05/24 20:32:16.414 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 67) in 33 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:16.414 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
24/05/24 20:32:16.414 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 85 (collect at StringIndexer.scala:204) finished in 0,037 s
24/05/24 20:32:16.415 dag-scheduler-event-loop INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:16.415 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 85: Stage finished
24/05/24 20:32:16.415 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 68 finished: collect at StringIndexer.scala:204, took 0,038715 s
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 196 (collect at StringIndexer.scala:204) as input to shuffle 18
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 69 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 86 (collect at StringIndexer.scala:204)
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:16.670 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 86 (MapPartitionsRDD[196] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:16.676 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 52.5 KiB, free 883.2 MiB)
24/05/24 20:32:16.677 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 883.1 MiB)
24/05/24 20:32:16.677 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:32:16.678 dag-scheduler-event-loop INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:16.678 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 86 (MapPartitionsRDD[196] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:16.678 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0
24/05/24 20:32:16.792 dispatcher-event-loop-1 WARN TaskSetManager: Stage 86 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:16.792 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 68) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:32:16.792 Executor task launch worker for task 0.0 in stage 86.0 (TID 68) INFO Executor: Running task 0.0 in stage 86.0 (TID 68)
24/05/24 20:32:17.604 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_67_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.4 MiB)
24/05/24 20:32:17.607 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_66_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.4 MiB)
24/05/24 20:32:17.611 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_64_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:32:17.613 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_62_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.5 MiB)
24/05/24 20:32:17.664 Executor task launch worker for task 0.0 in stage 86.0 (TID 68) INFO Executor: Finished task 0.0 in stage 86.0 (TID 68). 2342 bytes result sent to driver
24/05/24 20:32:17.665 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 68) in 987 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:17.665 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
24/05/24 20:32:17.666 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 86 (collect at StringIndexer.scala:204) finished in 0,996 s
24/05/24 20:32:17.666 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:32:17.666 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:32:17.666 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:32:17.666 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:32:17.681 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:32:17.682 dag-scheduler-event-loop INFO DAGScheduler: Got job 70 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:32:17.682 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 88 (collect at StringIndexer.scala:204)
24/05/24 20:32:17.682 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 87)
24/05/24 20:32:17.682 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:17.682 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[199] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:32:17.684 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 49.1 KiB, free 883.4 MiB)
24/05/24 20:32:17.686 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 883.4 MiB)
24/05/24 20:32:17.686 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:32:17.687 dag-scheduler-event-loop INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:17.687 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[199] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:17.687 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
24/05/24 20:32:17.688 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 69) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:32:17.688 Executor task launch worker for task 0.0 in stage 88.0 (TID 69) INFO Executor: Running task 0.0 in stage 88.0 (TID 69)
24/05/24 20:32:17.691 Executor task launch worker for task 0.0 in stage 88.0 (TID 69) INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:32:17.692 Executor task launch worker for task 0.0 in stage 88.0 (TID 69) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:32:17.705 Executor task launch worker for task 0.0 in stage 88.0 (TID 69) INFO Executor: Finished task 0.0 in stage 88.0 (TID 69). 4963 bytes result sent to driver
24/05/24 20:32:17.705 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 69) in 17 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:17.705 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
24/05/24 20:32:17.706 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 88 (collect at StringIndexer.scala:204) finished in 0,023 s
24/05/24 20:32:17.706 dag-scheduler-event-loop INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:17.706 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished
24/05/24 20:32:17.706 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 70 finished: collect at StringIndexer.scala:204, took 0,025312 s
24/05/24 20:32:17.714 nioEventLoopGroup-2-2 INFO Instrumentation: [ad5caab7] training finished
24/05/24 20:32:17.905 nioEventLoopGroup-2-2 INFO Instrumentation: [8fc1f005] training finished
24/05/24 20:32:17.914 nioEventLoopGroup-2-2 INFO Instrumentation: [77efd4dc] Stage class: RandomForestClassifier
24/05/24 20:32:17.914 nioEventLoopGroup-2-2 INFO Instrumentation: [77efd4dc] Stage uid: random_forest__2fac9c92_f28f_47d8_9ae6_bf6cdb693494
24/05/24 20:32:17.983 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_69_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.5 KiB, free: 883.5 MiB)
24/05/24 20:32:18.050 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 51.4059 ms
24/05/24 20:32:18.090 nioEventLoopGroup-2-2 INFO Instrumentation: [77efd4dc] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
24/05/24 20:32:18.244 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 45.641 ms
24/05/24 20:32:18.279 nioEventLoopGroup-2-2 INFO Instrumentation: [77efd4dc] {"subsamplingRate":1.0,"impurity":"gini","featuresCol":"features","maxDepth":5,"minInstancesPerNode":1,"featureSubsetStrategy":"auto","checkpointInterval":10,"minInfoGain":0.0,"cacheNodeIds":false,"labelCol":"label","predictionCol":"prediction","maxMemoryInMB":256,"maxBins":32,"rawPredictionCol":"rawPrediction","probabilityCol":"probability","numTrees":10}
24/05/24 20:32:18.322 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
24/05/24 20:32:18.322 dag-scheduler-event-loop INFO DAGScheduler: Got job 71 (take at DecisionTreeMetadata.scala:119) with 1 output partitions
24/05/24 20:32:18.322 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 89 (take at DecisionTreeMetadata.scala:119)
24/05/24 20:32:18.322 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:18.322 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:18.323 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 89 (MapPartitionsRDD[210] at map at DecisionTreeMetadata.scala:119), which has no missing parents
24/05/24 20:32:18.341 dag-scheduler-event-loop WARN DAGScheduler: Broadcasting large task binary with size 1667.2 KiB
24/05/24 20:32:18.341 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 1667.3 KiB, free 881.8 MiB)
24/05/24 20:32:18.344 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 339.7 KiB, free 881.5 MiB)
24/05/24 20:32:18.345 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 339.7 KiB, free: 883.1 MiB)
24/05/24 20:32:18.345 dag-scheduler-event-loop INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:18.346 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (MapPartitionsRDD[210] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:18.346 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
24/05/24 20:32:18.458 dispatcher-event-loop-1 WARN TaskSetManager: Stage 89 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:18.458 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 70) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:32:18.458 Executor task launch worker for task 0.0 in stage 89.0 (TID 70) INFO Executor: Running task 0.0 in stage 89.0 (TID 70)
24/05/24 20:32:18.985 Executor task launch worker for task 0.0 in stage 89.0 (TID 70) INFO Executor: Finished task 0.0 in stage 89.0 (TID 70). 1528 bytes result sent to driver
24/05/24 20:32:18.985 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 70) in 639 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:32:18.985 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
24/05/24 20:32:18.986 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 89 (take at DecisionTreeMetadata.scala:119) finished in 0,663 s
24/05/24 20:32:18.986 dag-scheduler-event-loop INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:32:18.986 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
24/05/24 20:32:18.986 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 71 finished: take at DecisionTreeMetadata.scala:119, took 0,664613 s
24/05/24 20:32:18.990 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
24/05/24 20:32:18.990 dag-scheduler-event-loop INFO DAGScheduler: Got job 72 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions
24/05/24 20:32:18.990 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 90 (aggregate at DecisionTreeMetadata.scala:125)
24/05/24 20:32:18.990 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:32:18.990 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:32:18.991 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[209] at retag at RandomForest.scala:274), which has no missing parents
24/05/24 20:32:19.009 dag-scheduler-event-loop WARN DAGScheduler: Broadcasting large task binary with size 1667.3 KiB
24/05/24 20:32:19.009 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 1667.4 KiB, free 879.8 MiB)
24/05/24 20:32:19.012 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 339.8 KiB, free 879.5 MiB)
24/05/24 20:32:19.013 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 339.8 KiB, free: 882.8 MiB)
24/05/24 20:32:19.013 dag-scheduler-event-loop INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1535
24/05/24 20:32:19.013 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[209] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))
24/05/24 20:32:19.013 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0
24/05/24 20:32:19.126 dispatcher-event-loop-0 WARN TaskSetManager: Stage 90 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:32:19.126 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 71) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:32:19.126 Executor task launch worker for task 0.0 in stage 90.0 (TID 71) INFO Executor: Running task 0.0 in stage 90.0 (TID 71)
24/05/24 20:32:19.443 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_70_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 339.7 KiB, free: 883.1 MiB)
24/05/24 20:32:19.447 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_68_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.9 KiB, free: 883.2 MiB)
24/05/24 20:32:20.118 Executor task launch worker for task 0.0 in stage 90.0 (TID 71) ERROR Executor: Exception in task 0.0 in stage 90.0 (TID 71)
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more
24/05/24 20:32:20.120 task-result-getter-3 WARN TaskSetManager: Lost task 0.0 in stage 90.0 (TID 71) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:32:20.121 task-result-getter-3 ERROR TaskSetManager: Task 0 in stage 90.0 failed 1 times; aborting job
24/05/24 20:32:20.121 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
24/05/24 20:32:20.121 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 90
24/05/24 20:32:20.121 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage cancelled
24/05/24 20:32:20.121 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 90 (aggregate at DecisionTreeMetadata.scala:125) failed in 1,130 s due to Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 71) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
24/05/24 20:32:20.122 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 72 failed: aggregate at DecisionTreeMetadata.scala:125, took 1,130929 s
24/05/24 20:32:20.123 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 71) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:32:20.123 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 71) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<onehot_772cda59ba32:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Zip_Code_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,onehot_7b61a5588fd3:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,onehot_b848b1dddc12:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_H_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_M_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_0_14_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_15_24_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_25_64_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double,solarenergy:double,uvindex_double_r_formula__4ce3d1cb_f0c0_4e1b_bc9d_c07e612fb8b2:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:32:20.708 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:32:20.708 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:32:20.710 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:32:20.710 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:32:20.711 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:32:20.711 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:36:11.742 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:36:11.742 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:36:11.744 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:36:11.744 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:36:11.746 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:36:11.746 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:36:11.800 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:36:11.802 dag-scheduler-event-loop INFO DAGScheduler: Got job 73 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:11.802 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 91 (collect at utils.scala:26)
24/05/24 20:36:11.802 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:36:11.802 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:11.803 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 91 (MapPartitionsRDD[213] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:11.806 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 7.1 KiB, free 881.5 MiB)
24/05/24 20:36:11.807 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 881.5 MiB)
24/05/24 20:36:11.808 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 883.2 MiB)
24/05/24 20:36:11.808 dag-scheduler-event-loop INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:11.808 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 91 (MapPartitionsRDD[213] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:11.809 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
24/05/24 20:36:11.809 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 72) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 8399 bytes) 
24/05/24 20:36:11.812 Executor task launch worker for task 0.0 in stage 91.0 (TID 72) INFO Executor: Running task 0.0 in stage 91.0 (TID 72)
24/05/24 20:36:11.814 Executor task launch worker for task 0.0 in stage 91.0 (TID 72) INFO Executor: Finished task 0.0 in stage 91.0 (TID 72). 1725 bytes result sent to driver
24/05/24 20:36:11.815 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 72) in 6 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:11.815 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
24/05/24 20:36:11.815 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 91 (collect at utils.scala:26) finished in 0,012 s
24/05/24 20:36:11.815 dag-scheduler-event-loop INFO DAGScheduler: Job 73 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:36:11.816 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 91: Stage finished
24/05/24 20:36:11.816 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 73 finished: collect at utils.scala:26, took 0,015328 s
24/05/24 20:36:21.948 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_72_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 883.2 MiB)
24/05/24 20:36:25.316 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_71_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 339.8 KiB, free: 883.5 MiB)
24/05/24 20:36:28.323 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:36:28.324 dag-scheduler-event-loop INFO DAGScheduler: Got job 74 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:28.324 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 92 (collect at utils.scala:26)
24/05/24 20:36:28.324 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:36:28.324 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:28.324 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[216] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:28.326 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:36:28.327 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:36:28.328 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:36:28.328 dag-scheduler-event-loop INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:28.328 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[216] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:28.329 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
24/05/24 20:36:28.330 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 73) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:36:28.330 Executor task launch worker for task 0.0 in stage 92.0 (TID 73) INFO Executor: Running task 0.0 in stage 92.0 (TID 73)
24/05/24 20:36:28.333 Executor task launch worker for task 0.0 in stage 92.0 (TID 73) INFO Executor: Finished task 0.0 in stage 92.0 (TID 73). 1328 bytes result sent to driver
24/05/24 20:36:28.334 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 73) in 4 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:28.334 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
24/05/24 20:36:28.334 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 92 (collect at utils.scala:26) finished in 0,009 s
24/05/24 20:36:28.335 dag-scheduler-event-loop INFO DAGScheduler: Job 74 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:36:28.335 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
24/05/24 20:36:28.335 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 74 finished: collect at utils.scala:26, took 0,011635 s
24/05/24 20:36:29.228 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:36:29.229 dag-scheduler-event-loop INFO DAGScheduler: Got job 75 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:29.229 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 93 (collect at utils.scala:26)
24/05/24 20:36:29.229 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:36:29.229 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:29.229 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[218] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:29.230 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 9.4 KiB, free 883.5 MiB)
24/05/24 20:36:29.243 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 883.5 MiB)
24/05/24 20:36:29.243 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:36:29.243 dag-scheduler-event-loop INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:29.243 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[218] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:29.243 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
24/05/24 20:36:29.244 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 74) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:36:29.244 Executor task launch worker for task 0.0 in stage 93.0 (TID 74) INFO Executor: Running task 0.0 in stage 93.0 (TID 74)
24/05/24 20:36:29.246 Executor task launch worker for task 0.0 in stage 93.0 (TID 74) INFO Executor: Finished task 0.0 in stage 93.0 (TID 74). 1328 bytes result sent to driver
24/05/24 20:36:29.246 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 74) in 2 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:29.247 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
24/05/24 20:36:29.247 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 93 (collect at utils.scala:26) finished in 0,018 s
24/05/24 20:36:29.247 dag-scheduler-event-loop INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:36:29.247 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
24/05/24 20:36:29.247 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 75 finished: collect at utils.scala:26, took 0,018163 s
24/05/24 20:36:29.437 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_73_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:36:29.456 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_74_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 883.5 MiB)
24/05/24 20:36:30.210 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:36:30.210 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:36:30.213 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:36:30.213 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:36:30.214 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:36:30.215 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:36:54.320 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 220 (collect at utils.scala:26) as input to shuffle 19
24/05/24 20:36:54.321 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 76 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:54.321 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 94 (collect at utils.scala:26)
24/05/24 20:36:54.321 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:36:54.321 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:54.321 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 94 (MapPartitionsRDD[220] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:54.322 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 12.9 KiB, free 883.5 MiB)
24/05/24 20:36:54.323 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 883.5 MiB)
24/05/24 20:36:54.324 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 883.5 MiB)
24/05/24 20:36:54.324 dag-scheduler-event-loop INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:54.324 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 94 (MapPartitionsRDD[220] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:54.324 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks resource profile 0
24/05/24 20:36:54.607 dispatcher-event-loop-0 WARN TaskSetManager: Stage 94 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:36:54.608 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 75) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:36:54.608 Executor task launch worker for task 0.0 in stage 94.0 (TID 75) INFO Executor: Running task 0.0 in stage 94.0 (TID 75)
24/05/24 20:36:55.131 Executor task launch worker for task 0.0 in stage 94.0 (TID 75) INFO Executor: Finished task 0.0 in stage 94.0 (TID 75). 1845 bytes result sent to driver
24/05/24 20:36:55.132 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 75) in 807 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:55.132 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
24/05/24 20:36:55.132 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 94 (collect at utils.scala:26) finished in 0,811 s
24/05/24 20:36:55.132 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:36:55.132 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:36:55.132 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:36:55.132 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:36:55.144 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:36:55.145 dag-scheduler-event-loop INFO DAGScheduler: Got job 77 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:55.145 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 96 (collect at utils.scala:26)
24/05/24 20:36:55.145 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 95)
24/05/24 20:36:55.145 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:55.145 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 96 (MapPartitionsRDD[223] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:55.147 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 13.7 KiB, free 883.5 MiB)
24/05/24 20:36:55.148 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 883.5 MiB)
24/05/24 20:36:55.148 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:36:55.148 dag-scheduler-event-loop INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:55.148 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 96 (MapPartitionsRDD[223] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:55.148 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks resource profile 0
24/05/24 20:36:55.149 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 76) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:36:55.150 Executor task launch worker for task 0.0 in stage 96.0 (TID 76) INFO Executor: Running task 0.0 in stage 96.0 (TID 76)
24/05/24 20:36:55.152 Executor task launch worker for task 0.0 in stage 96.0 (TID 76) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:36:55.152 Executor task launch worker for task 0.0 in stage 96.0 (TID 76) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:36:55.154 Executor task launch worker for task 0.0 in stage 96.0 (TID 76) INFO Executor: Finished task 0.0 in stage 96.0 (TID 76). 3866 bytes result sent to driver
24/05/24 20:36:55.154 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 76) in 5 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:55.154 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
24/05/24 20:36:55.155 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 96 (collect at utils.scala:26) finished in 0,009 s
24/05/24 20:36:55.155 dag-scheduler-event-loop INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:36:55.155 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 96: Stage finished
24/05/24 20:36:55.155 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 77 finished: collect at utils.scala:26, took 0,010531 s
24/05/24 20:36:58.427 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:36:58.427 dag-scheduler-event-loop INFO DAGScheduler: Got job 78 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:36:58.427 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 97 (collect at utils.scala:26)
24/05/24 20:36:58.427 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:36:58.429 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:36:58.429 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[230] at collect at utils.scala:26), which has no missing parents
24/05/24 20:36:58.433 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 34.5 KiB, free 883.4 MiB)
24/05/24 20:36:58.435 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 883.4 MiB)
24/05/24 20:36:58.435 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 11.8 KiB, free: 883.5 MiB)
24/05/24 20:36:58.435 dag-scheduler-event-loop INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1535
24/05/24 20:36:58.435 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[230] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:36:58.436 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks resource profile 0
24/05/24 20:36:58.543 dispatcher-event-loop-0 WARN TaskSetManager: Stage 97 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:36:58.544 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 77) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:36:58.554 Executor task launch worker for task 0.0 in stage 97.0 (TID 77) INFO Executor: Running task 0.0 in stage 97.0 (TID 77)
24/05/24 20:36:58.608 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_76_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 883.5 MiB)
24/05/24 20:36:59.934 Executor task launch worker for task 0.0 in stage 97.0 (TID 77) INFO MemoryStore: Block rdd_226_0 stored as values in memory (estimated size 28.8 MiB, free 854.6 MiB)
24/05/24 20:36:59.935 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added rdd_226_0 in memory on DESKTOP-LH06ASP:51464 (size: 28.8 MiB, free: 854.7 MiB)
24/05/24 20:36:59.939 Executor task launch worker for task 0.0 in stage 97.0 (TID 77) INFO Executor: 1 block locks were not released by task 0.0 in stage 97.0 (TID 77)
[rdd_226_0]
24/05/24 20:36:59.939 Executor task launch worker for task 0.0 in stage 97.0 (TID 77) INFO Executor: Finished task 0.0 in stage 97.0 (TID 77). 2532 bytes result sent to driver
24/05/24 20:36:59.940 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 77) in 1504 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:36:59.940 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
24/05/24 20:36:59.940 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 97 (collect at utils.scala:26) finished in 1,510 s
24/05/24 20:36:59.940 dag-scheduler-event-loop INFO DAGScheduler: Job 78 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:36:59.941 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished
24/05/24 20:36:59.941 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 78 finished: collect at utils.scala:26, took 1,514760 s
24/05/24 20:37:13.608 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 32.1638 ms
24/05/24 20:37:13.618 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:37:13.619 dag-scheduler-event-loop INFO DAGScheduler: Got job 79 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:37:13.619 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 98 (collect at utils.scala:26)
24/05/24 20:37:13.619 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:37:13.619 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:37:13.620 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 98 (MapPartitionsRDD[232] at collect at utils.scala:26), which has no missing parents
24/05/24 20:37:13.621 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 19.6 KiB, free 854.6 MiB)
24/05/24 20:37:13.623 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 854.6 MiB)
24/05/24 20:37:13.623 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.5 KiB, free: 854.7 MiB)
24/05/24 20:37:13.624 dag-scheduler-event-loop INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1535
24/05/24 20:37:13.624 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 98 (MapPartitionsRDD[232] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:37:13.624 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks resource profile 0
24/05/24 20:37:13.818 dispatcher-event-loop-0 WARN TaskSetManager: Stage 98 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:37:13.818 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 78) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:37:13.818 Executor task launch worker for task 0.0 in stage 98.0 (TID 78) INFO Executor: Running task 0.0 in stage 98.0 (TID 78)
24/05/24 20:37:14.712 Executor task launch worker for task 0.0 in stage 98.0 (TID 78) INFO MemoryStore: Block taskresult_78 stored as bytes in memory (estimated size 11.0 MiB, free 843.6 MiB)
24/05/24 20:37:14.712 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_78 in memory on DESKTOP-LH06ASP:51464 (size: 11.0 MiB, free: 843.7 MiB)
24/05/24 20:37:14.712 Executor task launch worker for task 0.0 in stage 98.0 (TID 78) INFO Executor: Finished task 0.0 in stage 98.0 (TID 78). 11557087 bytes result sent via BlockManager)
24/05/24 20:37:14.783 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 78) in 1158 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:37:14.784 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
24/05/24 20:37:14.784 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 98 (collect at utils.scala:26) finished in 1,164 s
24/05/24 20:37:14.785 dag-scheduler-event-loop INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:37:14.785 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_78 on DESKTOP-LH06ASP:51464 in memory (size: 11.0 MiB, free: 854.7 MiB)
24/05/24 20:37:14.785 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 98: Stage finished
24/05/24 20:37:14.785 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 79 finished: collect at utils.scala:26, took 1,166370 s
24/05/24 20:37:14.844 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 14.2784 ms
24/05/24 20:37:16.770 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_75_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 854.7 MiB)
24/05/24 20:37:16.774 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_77_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 11.8 KiB, free: 854.7 MiB)
24/05/24 20:37:16.790 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_78_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.5 KiB, free: 854.7 MiB)
24/05/24 20:38:30.559 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 8.3988 ms
24/05/24 20:38:30.570 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:38:30.571 dag-scheduler-event-loop INFO DAGScheduler: Got job 80 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:38:30.571 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 99 (collect at utils.scala:26)
24/05/24 20:38:30.571 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:38:30.571 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:30.571 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[234] at collect at utils.scala:26), which has no missing parents
24/05/24 20:38:30.572 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 9.3 KiB, free 854.7 MiB)
24/05/24 20:38:30.573 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:38:30.573 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:38:30.574 dag-scheduler-event-loop INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:30.574 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[234] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:30.574 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks resource profile 0
24/05/24 20:38:30.575 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 79) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:38:30.576 Executor task launch worker for task 0.0 in stage 99.0 (TID 79) INFO Executor: Running task 0.0 in stage 99.0 (TID 79)
24/05/24 20:38:30.579 Executor task launch worker for task 0.0 in stage 99.0 (TID 79) INFO Executor: Finished task 0.0 in stage 99.0 (TID 79). 1328 bytes result sent to driver
24/05/24 20:38:30.580 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 79) in 6 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:38:30.580 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
24/05/24 20:38:30.580 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 99 (collect at utils.scala:26) finished in 0,009 s
24/05/24 20:38:30.580 dag-scheduler-event-loop INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:38:30.580 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished
24/05/24 20:38:30.580 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 80 finished: collect at utils.scala:26, took 0,010107 s
24/05/24 20:38:30.597 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 10.4895 ms
24/05/24 20:38:30.952 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:38:30.953 dag-scheduler-event-loop INFO DAGScheduler: Got job 81 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:38:30.953 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 100 (collect at utils.scala:26)
24/05/24 20:38:30.953 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:38:30.953 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:30.953 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[236] at collect at utils.scala:26), which has no missing parents
24/05/24 20:38:30.954 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 9.3 KiB, free 854.7 MiB)
24/05/24 20:38:30.955 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:38:30.955 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:38:30.955 dag-scheduler-event-loop INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:30.955 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[236] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:30.955 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks resource profile 0
24/05/24 20:38:30.956 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 80) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:38:30.956 Executor task launch worker for task 0.0 in stage 100.0 (TID 80) INFO Executor: Running task 0.0 in stage 100.0 (TID 80)
24/05/24 20:38:30.958 Executor task launch worker for task 0.0 in stage 100.0 (TID 80) INFO Executor: Finished task 0.0 in stage 100.0 (TID 80). 1285 bytes result sent to driver
24/05/24 20:38:30.958 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 80) in 2 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:38:30.958 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
24/05/24 20:38:30.958 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 100 (collect at utils.scala:26) finished in 0,005 s
24/05/24 20:38:30.959 dag-scheduler-event-loop INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:38:30.959 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished
24/05/24 20:38:30.959 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 81 finished: collect at utils.scala:26, took 0,006221 s
24/05/24 20:38:39.592 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 21.2027 ms
24/05/24 20:38:39.596 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 239 (collect at StringIndexer.scala:204) as input to shuffle 20
24/05/24 20:38:39.596 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 82 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:38:39.596 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 101 (collect at StringIndexer.scala:204)
24/05/24 20:38:39.596 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:38:39.596 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:39.597 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 101 (MapPartitionsRDD[239] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:38:39.601 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 51.4 KiB, free 854.6 MiB)
24/05/24 20:38:39.605 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 854.6 MiB)
24/05/24 20:38:39.605 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.8 KiB, free: 854.7 MiB)
24/05/24 20:38:39.606 dag-scheduler-event-loop INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:39.606 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 101 (MapPartitionsRDD[239] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:39.606 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks resource profile 0
24/05/24 20:38:39.906 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_79_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:38:39.911 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_80_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:38:39.970 dispatcher-event-loop-0 WARN TaskSetManager: Stage 101 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:38:39.970 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 81) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:38:39.970 Executor task launch worker for task 0.0 in stage 101.0 (TID 81) INFO Executor: Running task 0.0 in stage 101.0 (TID 81)
24/05/24 20:38:40.091 Executor task launch worker for task 0.0 in stage 101.0 (TID 81) INFO CodeGenerator: Code generated in 21.629 ms
24/05/24 20:38:40.097 Executor task launch worker for task 0.0 in stage 101.0 (TID 81) INFO CodeGenerator: Code generated in 4.0378 ms
24/05/24 20:38:40.866 Executor task launch worker for task 0.0 in stage 101.0 (TID 81) INFO Executor: Finished task 0.0 in stage 101.0 (TID 81). 2342 bytes result sent to driver
24/05/24 20:38:40.867 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 81) in 1261 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:38:40.867 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
24/05/24 20:38:40.867 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 101 (collect at StringIndexer.scala:204) finished in 1,270 s
24/05/24 20:38:40.868 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:38:40.868 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:38:40.868 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:38:40.868 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:38:40.880 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:38:40.881 dag-scheduler-event-loop INFO DAGScheduler: Got job 83 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:38:40.881 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 103 (collect at StringIndexer.scala:204)
24/05/24 20:38:40.881 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 102)
24/05/24 20:38:40.881 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:40.881 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 103 (MapPartitionsRDD[242] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:38:40.883 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 48.4 KiB, free 854.6 MiB)
24/05/24 20:38:40.911 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 854.6 MiB)
24/05/24 20:38:40.911 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.4 KiB, free: 854.7 MiB)
24/05/24 20:38:40.912 dag-scheduler-event-loop INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:40.912 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 103 (MapPartitionsRDD[242] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:40.912 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks resource profile 0
24/05/24 20:38:40.913 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 82) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:38:40.913 Executor task launch worker for task 0.0 in stage 103.0 (TID 82) INFO Executor: Running task 0.0 in stage 103.0 (TID 82)
24/05/24 20:38:40.916 Executor task launch worker for task 0.0 in stage 103.0 (TID 82) INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:38:40.916 Executor task launch worker for task 0.0 in stage 103.0 (TID 82) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:38:40.929 Executor task launch worker for task 0.0 in stage 103.0 (TID 82) INFO Executor: Finished task 0.0 in stage 103.0 (TID 82). 4920 bytes result sent to driver
24/05/24 20:38:40.930 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 82) in 17 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:38:40.930 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
24/05/24 20:38:40.930 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 103 (collect at StringIndexer.scala:204) finished in 0,048 s
24/05/24 20:38:40.930 dag-scheduler-event-loop INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:38:40.931 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 103: Stage finished
24/05/24 20:38:40.931 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 83 finished: collect at StringIndexer.scala:204, took 0,050312 s
24/05/24 20:38:40.943 nioEventLoopGroup-2-2 INFO Instrumentation: [4dfb4fdc] training finished
24/05/24 20:38:41.009 nioEventLoopGroup-2-2 INFO Instrumentation: [f85dd503] training finished
24/05/24 20:38:41.017 nioEventLoopGroup-2-2 INFO Instrumentation: [56d2c747] Stage class: RandomForestClassifier
24/05/24 20:38:41.018 nioEventLoopGroup-2-2 INFO Instrumentation: [56d2c747] Stage uid: random_forest__28ec8efb_28d2_44ec_803a_322b8f0362d7
24/05/24 20:38:41.100 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 39.9877 ms
24/05/24 20:38:41.114 nioEventLoopGroup-2-2 INFO Instrumentation: [56d2c747] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
24/05/24 20:38:41.201 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 28.5687 ms
24/05/24 20:38:41.213 nioEventLoopGroup-2-2 INFO Instrumentation: [56d2c747] {"subsamplingRate":1.0,"impurity":"gini","featuresCol":"features","maxDepth":5,"minInstancesPerNode":1,"featureSubsetStrategy":"auto","checkpointInterval":10,"minInfoGain":0.0,"cacheNodeIds":false,"labelCol":"label","predictionCol":"prediction","maxMemoryInMB":256,"maxBins":32,"rawPredictionCol":"rawPrediction","probabilityCol":"probability","numTrees":10}
24/05/24 20:38:41.223 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
24/05/24 20:38:41.224 dag-scheduler-event-loop INFO DAGScheduler: Got job 84 (take at DecisionTreeMetadata.scala:119) with 1 output partitions
24/05/24 20:38:41.224 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 104 (take at DecisionTreeMetadata.scala:119)
24/05/24 20:38:41.224 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:38:41.224 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:41.224 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[253] at map at DecisionTreeMetadata.scala:119), which has no missing parents
24/05/24 20:38:41.227 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 147.5 KiB, free 854.4 MiB)
24/05/24 20:38:41.228 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 854.4 MiB)
24/05/24 20:38:41.228 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 46.8 KiB, free: 854.6 MiB)
24/05/24 20:38:41.229 dag-scheduler-event-loop INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:41.229 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[253] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:41.229 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks resource profile 0
24/05/24 20:38:41.311 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_82_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.4 KiB, free: 854.6 MiB)
24/05/24 20:38:41.387 dispatcher-event-loop-0 WARN TaskSetManager: Stage 104 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:38:41.388 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 83) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:38:41.388 Executor task launch worker for task 0.0 in stage 104.0 (TID 83) INFO Executor: Running task 0.0 in stage 104.0 (TID 83)
24/05/24 20:38:42.067 Executor task launch worker for task 0.0 in stage 104.0 (TID 83) INFO Executor: Finished task 0.0 in stage 104.0 (TID 83). 1528 bytes result sent to driver
24/05/24 20:38:42.067 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 83) in 838 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:38:42.067 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
24/05/24 20:38:42.068 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 104 (take at DecisionTreeMetadata.scala:119) finished in 0,843 s
24/05/24 20:38:42.068 dag-scheduler-event-loop INFO DAGScheduler: Job 84 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:38:42.068 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished
24/05/24 20:38:42.068 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 84 finished: take at DecisionTreeMetadata.scala:119, took 0,843991 s
24/05/24 20:38:42.072 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
24/05/24 20:38:42.072 dag-scheduler-event-loop INFO DAGScheduler: Got job 85 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions
24/05/24 20:38:42.072 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 105 (aggregate at DecisionTreeMetadata.scala:125)
24/05/24 20:38:42.072 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:38:42.072 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:38:42.072 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 105 (MapPartitionsRDD[252] at retag at RandomForest.scala:274), which has no missing parents
24/05/24 20:38:42.075 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 147.5 KiB, free 854.3 MiB)
24/05/24 20:38:42.076 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 854.2 MiB)
24/05/24 20:38:42.076 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 46.9 KiB, free: 854.6 MiB)
24/05/24 20:38:42.077 dag-scheduler-event-loop INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1535
24/05/24 20:38:42.077 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 105 (MapPartitionsRDD[252] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))
24/05/24 20:38:42.077 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks resource profile 0
24/05/24 20:38:42.194 dispatcher-event-loop-1 WARN TaskSetManager: Stage 105 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:38:42.194 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 84) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:38:42.194 Executor task launch worker for task 0.0 in stage 105.0 (TID 84) INFO Executor: Running task 0.0 in stage 105.0 (TID 84)
24/05/24 20:38:42.740 Executor task launch worker for task 0.0 in stage 105.0 (TID 84) ERROR Executor: Exception in task 0.0 in stage 105.0 (TID 84)
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more
24/05/24 20:38:42.743 task-result-getter-0 WARN TaskSetManager: Lost task 0.0 in stage 105.0 (TID 84) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:38:42.743 task-result-getter-0 ERROR TaskSetManager: Task 0 in stage 105.0 failed 1 times; aborting job
24/05/24 20:38:42.744 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
24/05/24 20:38:42.744 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 105
24/05/24 20:38:42.744 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 105: Stage cancelled
24/05/24 20:38:42.744 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 105 (aggregate at DecisionTreeMetadata.scala:125) failed in 0,671 s due to Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 84) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
24/05/24 20:38:42.744 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 85 failed: aggregate at DecisionTreeMetadata.scala:125, took 0,672431 s
24/05/24 20:38:42.745 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 84) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:38:42.746 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 84) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_TOTAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_PROP_OCUP_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_RHABITUAL_ARRENDADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_H_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_M_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_0_14_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_15_24_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_25_64_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double,solarenergy:double,uvindex_double_r_formula__4e9b0827_e5c6_41e1_a559_8af8e36d901b:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:43:28.385 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:43:28.385 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:43:28.393 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:43:28.393 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:43:28.400 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:43:28.400 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:43:28.534 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:43:28.535 dag-scheduler-event-loop INFO DAGScheduler: Got job 86 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:28.535 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 106 (collect at utils.scala:26)
24/05/24 20:43:28.535 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:43:28.535 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:28.535 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[256] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:28.536 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 7.1 KiB, free 854.2 MiB)
24/05/24 20:43:28.537 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 854.2 MiB)
24/05/24 20:43:28.537 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 854.6 MiB)
24/05/24 20:43:28.537 dag-scheduler-event-loop INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:28.537 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[256] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:28.538 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks resource profile 0
24/05/24 20:43:28.538 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 85) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 8609 bytes) 
24/05/24 20:43:28.540 Executor task launch worker for task 0.0 in stage 106.0 (TID 85) INFO Executor: Running task 0.0 in stage 106.0 (TID 85)
24/05/24 20:43:28.542 Executor task launch worker for task 0.0 in stage 106.0 (TID 85) INFO Executor: Finished task 0.0 in stage 106.0 (TID 85). 1815 bytes result sent to driver
24/05/24 20:43:28.543 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 85) in 5 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:28.543 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
24/05/24 20:43:28.543 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 106 (collect at utils.scala:26) finished in 0,007 s
24/05/24 20:43:28.543 dag-scheduler-event-loop INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:43:28.544 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished
24/05/24 20:43:28.544 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 86 finished: collect at utils.scala:26, took 0,008718 s
24/05/24 20:43:43.864 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_85_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 854.6 MiB)
24/05/24 20:43:43.873 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_84_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 46.9 KiB, free: 854.6 MiB)
24/05/24 20:43:43.881 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_81_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.8 KiB, free: 854.7 MiB)
24/05/24 20:43:43.886 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_83_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 46.8 KiB, free: 854.7 MiB)
24/05/24 20:43:48.220 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:43:48.221 dag-scheduler-event-loop INFO DAGScheduler: Got job 87 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:48.221 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 107 (collect at utils.scala:26)
24/05/24 20:43:48.222 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:43:48.222 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:48.222 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[259] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:48.224 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 9.4 KiB, free 854.7 MiB)
24/05/24 20:43:48.226 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:43:48.226 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:43:48.227 dag-scheduler-event-loop INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:48.227 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[259] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:48.227 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks resource profile 0
24/05/24 20:43:48.230 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 86) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:43:48.231 Executor task launch worker for task 0.0 in stage 107.0 (TID 86) INFO Executor: Running task 0.0 in stage 107.0 (TID 86)
24/05/24 20:43:48.239 Executor task launch worker for task 0.0 in stage 107.0 (TID 86) INFO Executor: Finished task 0.0 in stage 107.0 (TID 86). 1371 bytes result sent to driver
24/05/24 20:43:48.240 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 86) in 11 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:48.240 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
24/05/24 20:43:48.241 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 107 (collect at utils.scala:26) finished in 0,018 s
24/05/24 20:43:48.241 dag-scheduler-event-loop INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:43:48.241 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished
24/05/24 20:43:48.241 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 87 finished: collect at utils.scala:26, took 0,021213 s
24/05/24 20:43:48.766 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:43:48.767 dag-scheduler-event-loop INFO DAGScheduler: Got job 88 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:48.767 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_86_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:43:48.767 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 108 (collect at utils.scala:26)
24/05/24 20:43:48.767 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:43:48.768 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:48.768 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[261] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:48.769 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 9.4 KiB, free 854.7 MiB)
24/05/24 20:43:48.770 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:43:48.770 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:43:48.771 dag-scheduler-event-loop INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:48.771 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[261] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:48.771 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks resource profile 0
24/05/24 20:43:48.772 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 87) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:43:48.772 Executor task launch worker for task 0.0 in stage 108.0 (TID 87) INFO Executor: Running task 0.0 in stage 108.0 (TID 87)
24/05/24 20:43:48.775 Executor task launch worker for task 0.0 in stage 108.0 (TID 87) INFO Executor: Finished task 0.0 in stage 108.0 (TID 87). 1328 bytes result sent to driver
24/05/24 20:43:48.775 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 87) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:48.775 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
24/05/24 20:43:48.776 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 108 (collect at utils.scala:26) finished in 0,008 s
24/05/24 20:43:48.776 dag-scheduler-event-loop INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:43:48.776 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished
24/05/24 20:43:48.776 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 88 finished: collect at utils.scala:26, took 0,009716 s
24/05/24 20:43:49.816 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 263 (collect at utils.scala:26) as input to shuffle 21
24/05/24 20:43:49.816 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 89 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:49.816 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 109 (collect at utils.scala:26)
24/05/24 20:43:49.817 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:43:49.817 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:49.817 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 109 (MapPartitionsRDD[263] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:49.818 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 12.9 KiB, free 854.7 MiB)
24/05/24 20:43:49.819 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 854.7 MiB)
24/05/24 20:43:49.819 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.3 KiB, free: 854.7 MiB)
24/05/24 20:43:49.819 dag-scheduler-event-loop INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:49.821 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 109 (MapPartitionsRDD[263] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:49.821 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks resource profile 0
24/05/24 20:43:49.959 dispatcher-event-loop-0 WARN TaskSetManager: Stage 109 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:43:49.959 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 88) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:43:49.960 Executor task launch worker for task 0.0 in stage 109.0 (TID 88) INFO Executor: Running task 0.0 in stage 109.0 (TID 88)
24/05/24 20:43:51.535 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_87_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:43:51.813 Executor task launch worker for task 0.0 in stage 109.0 (TID 88) INFO Executor: Finished task 0.0 in stage 109.0 (TID 88). 1845 bytes result sent to driver
24/05/24 20:43:51.815 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 88) in 1994 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:51.815 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
24/05/24 20:43:51.816 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 109 (collect at utils.scala:26) finished in 1,999 s
24/05/24 20:43:51.817 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:43:51.817 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:43:51.817 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:43:51.817 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:43:51.874 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:43:51.875 dag-scheduler-event-loop INFO DAGScheduler: Got job 90 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:51.875 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 111 (collect at utils.scala:26)
24/05/24 20:43:51.875 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 110)
24/05/24 20:43:51.875 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:51.875 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 111 (MapPartitionsRDD[266] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:51.877 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 13.7 KiB, free 854.7 MiB)
24/05/24 20:43:51.888 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 854.7 MiB)
24/05/24 20:43:51.889 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.4 KiB, free: 854.7 MiB)
24/05/24 20:43:51.889 dag-scheduler-event-loop INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:51.889 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 111 (MapPartitionsRDD[266] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:51.889 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks resource profile 0
24/05/24 20:43:51.890 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 89) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:43:51.890 Executor task launch worker for task 0.0 in stage 111.0 (TID 89) INFO Executor: Running task 0.0 in stage 111.0 (TID 89)
24/05/24 20:43:51.892 Executor task launch worker for task 0.0 in stage 111.0 (TID 89) INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:43:51.893 Executor task launch worker for task 0.0 in stage 111.0 (TID 89) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:43:51.912 Executor task launch worker for task 0.0 in stage 111.0 (TID 89) INFO Executor: Finished task 0.0 in stage 111.0 (TID 89). 3909 bytes result sent to driver
24/05/24 20:43:51.912 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 89) in 22 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:51.912 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
24/05/24 20:43:51.913 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 111 (collect at utils.scala:26) finished in 0,037 s
24/05/24 20:43:51.913 dag-scheduler-event-loop INFO DAGScheduler: Job 90 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:43:51.913 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 111: Stage finished
24/05/24 20:43:51.913 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 90 finished: collect at utils.scala:26, took 0,039264 s
24/05/24 20:43:52.779 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:43:52.780 dag-scheduler-event-loop INFO DAGScheduler: Got job 91 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:43:52.780 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 112 (collect at utils.scala:26)
24/05/24 20:43:52.780 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:43:52.780 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:43:52.780 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[268] at collect at utils.scala:26), which has no missing parents
24/05/24 20:43:52.781 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 19.6 KiB, free 854.6 MiB)
24/05/24 20:43:52.784 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 854.6 MiB)
24/05/24 20:43:52.785 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 6.5 KiB, free: 854.7 MiB)
24/05/24 20:43:52.785 dag-scheduler-event-loop INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1535
24/05/24 20:43:52.786 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[268] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:43:52.786 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks resource profile 0
24/05/24 20:43:53.049 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_89_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.4 KiB, free: 854.7 MiB)
24/05/24 20:43:53.068 dispatcher-event-loop-0 WARN TaskSetManager: Stage 112 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:43:53.068 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 90) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:43:53.069 Executor task launch worker for task 0.0 in stage 112.0 (TID 90) INFO Executor: Running task 0.0 in stage 112.0 (TID 90)
24/05/24 20:43:53.303 Executor task launch worker for task 0.0 in stage 112.0 (TID 90) INFO MemoryStore: Block taskresult_90 stored as bytes in memory (estimated size 11.0 MiB, free 843.6 MiB)
24/05/24 20:43:53.304 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added taskresult_90 in memory on DESKTOP-LH06ASP:51464 (size: 11.0 MiB, free: 843.7 MiB)
24/05/24 20:43:53.305 Executor task launch worker for task 0.0 in stage 112.0 (TID 90) INFO Executor: Finished task 0.0 in stage 112.0 (TID 90). 11557087 bytes result sent via BlockManager)
24/05/24 20:43:53.383 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 90) in 596 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:43:53.384 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
24/05/24 20:43:53.386 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 112 (collect at utils.scala:26) finished in 0,603 s
24/05/24 20:43:53.386 dag-scheduler-event-loop INFO DAGScheduler: Job 91 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:43:53.386 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished
24/05/24 20:43:53.386 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed taskresult_90 on DESKTOP-LH06ASP:51464 in memory (size: 11.0 MiB, free: 854.7 MiB)
24/05/24 20:43:53.386 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 91 finished: collect at utils.scala:26, took 0,606037 s
24/05/24 20:43:55.796 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_88_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.3 KiB, free: 854.7 MiB)
24/05/24 20:43:55.800 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_90_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 6.5 KiB, free: 854.7 MiB)
24/05/24 20:43:59.301 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:43:59.301 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:43:59.306 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:43:59.306 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:43:59.309 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:43:59.309 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:46:40.368 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:46:40.369 dag-scheduler-event-loop INFO DAGScheduler: Got job 92 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:46:40.369 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 113 (collect at utils.scala:26)
24/05/24 20:46:40.369 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:46:40.369 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:40.369 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[270] at collect at utils.scala:26), which has no missing parents
24/05/24 20:46:40.371 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 9.3 KiB, free 854.7 MiB)
24/05/24 20:46:40.377 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:46:40.377 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:46:40.378 dag-scheduler-event-loop INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:40.379 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[270] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:40.380 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0
24/05/24 20:46:40.381 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 91) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:46:40.382 Executor task launch worker for task 0.0 in stage 113.0 (TID 91) INFO Executor: Running task 0.0 in stage 113.0 (TID 91)
24/05/24 20:46:40.385 Executor task launch worker for task 0.0 in stage 113.0 (TID 91) INFO Executor: Finished task 0.0 in stage 113.0 (TID 91). 1328 bytes result sent to driver
24/05/24 20:46:40.387 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 91) in 6 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:46:40.387 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
24/05/24 20:46:40.387 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 113 (collect at utils.scala:26) finished in 0,017 s
24/05/24 20:46:40.387 dag-scheduler-event-loop INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:46:40.387 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished
24/05/24 20:46:40.388 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 92 finished: collect at utils.scala:26, took 0,019799 s
24/05/24 20:46:40.730 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:46:40.731 dag-scheduler-event-loop INFO DAGScheduler: Got job 93 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:46:40.731 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 114 (collect at utils.scala:26)
24/05/24 20:46:40.731 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:46:40.731 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:40.731 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[272] at collect at utils.scala:26), which has no missing parents
24/05/24 20:46:40.732 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 9.3 KiB, free 854.7 MiB)
24/05/24 20:46:40.732 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 854.7 MiB)
24/05/24 20:46:40.733 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:46:40.733 dag-scheduler-event-loop INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:40.733 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[272] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:40.733 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks resource profile 0
24/05/24 20:46:40.734 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 92) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 7571 bytes) 
24/05/24 20:46:40.734 Executor task launch worker for task 0.0 in stage 114.0 (TID 92) INFO Executor: Running task 0.0 in stage 114.0 (TID 92)
24/05/24 20:46:40.736 Executor task launch worker for task 0.0 in stage 114.0 (TID 92) INFO Executor: Finished task 0.0 in stage 114.0 (TID 92). 1285 bytes result sent to driver
24/05/24 20:46:40.736 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 92) in 2 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:46:40.736 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
24/05/24 20:46:40.736 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 114 (collect at utils.scala:26) finished in 0,005 s
24/05/24 20:46:40.737 dag-scheduler-event-loop INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:46:40.737 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished
24/05/24 20:46:40.737 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 93 finished: collect at utils.scala:26, took 0,006573 s
24/05/24 20:46:44.613 dag-scheduler-event-loop INFO DAGScheduler: Registering RDD 275 (collect at StringIndexer.scala:204) as input to shuffle 22
24/05/24 20:46:44.613 dag-scheduler-event-loop INFO DAGScheduler: Got map stage job 94 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:46:44.613 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ShuffleMapStage 115 (collect at StringIndexer.scala:204)
24/05/24 20:46:44.613 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:46:44.614 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:44.614 dag-scheduler-event-loop INFO DAGScheduler: Submitting ShuffleMapStage 115 (MapPartitionsRDD[275] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:46:44.618 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 51.4 KiB, free 854.6 MiB)
24/05/24 20:46:44.620 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 854.6 MiB)
24/05/24 20:46:44.621 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 19.8 KiB, free: 854.7 MiB)
24/05/24 20:46:44.622 dag-scheduler-event-loop INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:44.622 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 115 (MapPartitionsRDD[275] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:44.622 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks resource profile 0
24/05/24 20:46:44.684 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_92_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:46:44.687 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_91_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.9 KiB, free: 854.7 MiB)
24/05/24 20:46:44.811 dispatcher-event-loop-1 WARN TaskSetManager: Stage 115 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:46:44.811 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 93) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695731 bytes) 
24/05/24 20:46:44.811 Executor task launch worker for task 0.0 in stage 115.0 (TID 93) INFO Executor: Running task 0.0 in stage 115.0 (TID 93)
24/05/24 20:46:45.895 Executor task launch worker for task 0.0 in stage 115.0 (TID 93) INFO Executor: Finished task 0.0 in stage 115.0 (TID 93). 2342 bytes result sent to driver
24/05/24 20:46:45.896 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 93) in 1273 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:46:45.896 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
24/05/24 20:46:45.897 dag-scheduler-event-loop INFO DAGScheduler: ShuffleMapStage 115 (collect at StringIndexer.scala:204) finished in 1,283 s
24/05/24 20:46:45.897 dag-scheduler-event-loop INFO DAGScheduler: looking for newly runnable stages
24/05/24 20:46:45.897 dag-scheduler-event-loop INFO DAGScheduler: running: Set()
24/05/24 20:46:45.897 dag-scheduler-event-loop INFO DAGScheduler: waiting: Set()
24/05/24 20:46:45.897 dag-scheduler-event-loop INFO DAGScheduler: failed: Set()
24/05/24 20:46:45.910 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at StringIndexer.scala:204
24/05/24 20:46:45.911 dag-scheduler-event-loop INFO DAGScheduler: Got job 95 (collect at StringIndexer.scala:204) with 1 output partitions
24/05/24 20:46:45.911 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 117 (collect at StringIndexer.scala:204)
24/05/24 20:46:45.911 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 116)
24/05/24 20:46:45.911 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:45.911 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 117 (MapPartitionsRDD[278] at collect at StringIndexer.scala:204), which has no missing parents
24/05/24 20:46:45.913 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 48.4 KiB, free 854.6 MiB)
24/05/24 20:46:45.914 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 854.6 MiB)
24/05/24 20:46:45.915 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 18.4 KiB, free: 854.7 MiB)
24/05/24 20:46:45.915 dag-scheduler-event-loop INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:45.915 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 117 (MapPartitionsRDD[278] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:45.915 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0
24/05/24 20:46:45.917 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 94) (DESKTOP-LH06ASP, executor driver, partition 0, NODE_LOCAL, 7363 bytes) 
24/05/24 20:46:45.917 Executor task launch worker for task 0.0 in stage 117.0 (TID 94) INFO Executor: Running task 0.0 in stage 117.0 (TID 94)
24/05/24 20:46:45.921 Executor task launch worker for task 0.0 in stage 117.0 (TID 94) INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/05/24 20:46:45.921 Executor task launch worker for task 0.0 in stage 117.0 (TID 94) INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/05/24 20:46:45.939 Executor task launch worker for task 0.0 in stage 117.0 (TID 94) INFO Executor: Finished task 0.0 in stage 117.0 (TID 94). 4920 bytes result sent to driver
24/05/24 20:46:45.941 task-result-getter-2 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 94) in 24 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:46:45.941 task-result-getter-2 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
24/05/24 20:46:45.942 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 117 (collect at StringIndexer.scala:204) finished in 0,029 s
24/05/24 20:46:45.942 dag-scheduler-event-loop INFO DAGScheduler: Job 95 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:46:45.942 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 117: Stage finished
24/05/24 20:46:45.942 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 95 finished: collect at StringIndexer.scala:204, took 0,031605 s
24/05/24 20:46:45.955 nioEventLoopGroup-2-2 INFO Instrumentation: [aa81f31a] training finished
24/05/24 20:46:46.029 nioEventLoopGroup-2-2 INFO Instrumentation: [ce65f7cf] training finished
24/05/24 20:46:46.038 nioEventLoopGroup-2-2 INFO Instrumentation: [fed77a3c] Stage class: RandomForestClassifier
24/05/24 20:46:46.038 nioEventLoopGroup-2-2 INFO Instrumentation: [fed77a3c] Stage uid: random_forest__78b79add_167d_46f0_b76b_57329e65d4af
24/05/24 20:46:46.135 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_94_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 18.4 KiB, free: 854.7 MiB)
24/05/24 20:46:46.153 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 66.0692 ms
24/05/24 20:46:46.163 nioEventLoopGroup-2-2 INFO Instrumentation: [fed77a3c] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)
24/05/24 20:46:46.255 nioEventLoopGroup-2-2 INFO CodeGenerator: Code generated in 33.8174 ms
24/05/24 20:46:46.272 nioEventLoopGroup-2-2 INFO Instrumentation: [fed77a3c] {"subsamplingRate":1.0,"impurity":"gini","featuresCol":"features","maxDepth":5,"minInstancesPerNode":1,"featureSubsetStrategy":"auto","checkpointInterval":10,"minInfoGain":0.0,"cacheNodeIds":false,"labelCol":"label","predictionCol":"prediction","maxMemoryInMB":256,"maxBins":32,"rawPredictionCol":"rawPrediction","probabilityCol":"probability","numTrees":10}
24/05/24 20:46:46.284 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
24/05/24 20:46:46.285 dag-scheduler-event-loop INFO DAGScheduler: Got job 96 (take at DecisionTreeMetadata.scala:119) with 1 output partitions
24/05/24 20:46:46.285 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 118 (take at DecisionTreeMetadata.scala:119)
24/05/24 20:46:46.285 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:46:46.285 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:46.285 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 118 (MapPartitionsRDD[289] at map at DecisionTreeMetadata.scala:119), which has no missing parents
24/05/24 20:46:46.287 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 147.5 KiB, free 854.5 MiB)
24/05/24 20:46:46.288 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 854.4 MiB)
24/05/24 20:46:46.289 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 46.8 KiB, free: 854.6 MiB)
24/05/24 20:46:46.289 dag-scheduler-event-loop INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:46.289 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 118 (MapPartitionsRDD[289] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:46.289 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks resource profile 0
24/05/24 20:46:46.396 dispatcher-event-loop-1 WARN TaskSetManager: Stage 118 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:46:46.396 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 95) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:46:46.397 Executor task launch worker for task 0.0 in stage 118.0 (TID 95) INFO Executor: Running task 0.0 in stage 118.0 (TID 95)
24/05/24 20:46:46.729 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_93_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 19.8 KiB, free: 854.7 MiB)
24/05/24 20:46:47.289 Executor task launch worker for task 0.0 in stage 118.0 (TID 95) INFO Executor: Finished task 0.0 in stage 118.0 (TID 95). 1528 bytes result sent to driver
24/05/24 20:46:47.290 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 95) in 1000 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:46:47.290 task-result-getter-3 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
24/05/24 20:46:47.291 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 118 (take at DecisionTreeMetadata.scala:119) finished in 1,006 s
24/05/24 20:46:47.291 dag-scheduler-event-loop INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:46:47.291 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 118: Stage finished
24/05/24 20:46:47.291 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 96 finished: take at DecisionTreeMetadata.scala:119, took 1,006590 s
24/05/24 20:46:47.297 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
24/05/24 20:46:47.297 dag-scheduler-event-loop INFO DAGScheduler: Got job 97 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions
24/05/24 20:46:47.297 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 119 (aggregate at DecisionTreeMetadata.scala:125)
24/05/24 20:46:47.297 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:46:47.297 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:46:47.298 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[288] at retag at RandomForest.scala:274), which has no missing parents
24/05/24 20:46:47.301 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 147.5 KiB, free 854.4 MiB)
24/05/24 20:46:47.302 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 854.3 MiB)
24/05/24 20:46:47.303 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 46.8 KiB, free: 854.6 MiB)
24/05/24 20:46:47.304 dag-scheduler-event-loop INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1535
24/05/24 20:46:47.304 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[288] at retag at RandomForest.scala:274) (first 15 tasks are for partitions Vector(0))
24/05/24 20:46:47.304 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0
24/05/24 20:46:47.449 dispatcher-event-loop-0 WARN TaskSetManager: Stage 119 contains a task of very large size (81734 KiB). The maximum recommended task size is 1000 KiB.
24/05/24 20:46:47.449 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 96) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 83695742 bytes) 
24/05/24 20:46:47.450 Executor task launch worker for task 0.0 in stage 119.0 (TID 96) INFO Executor: Running task 0.0 in stage 119.0 (TID 96)
24/05/24 20:46:48.251 Executor task launch worker for task 0.0 in stage 119.0 (TID 96) ERROR Executor: Exception in task 0.0 in stage 119.0 (TID 96)
org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more
24/05/24 20:46:48.254 task-result-getter-0 WARN TaskSetManager: Lost task 0.0 in stage 119.0 (TID 96) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:46:48.254 task-result-getter-0 ERROR TaskSetManager: Task 0 in stage 119.0 failed 1 times; aborting job
24/05/24 20:46:48.254 task-result-getter-0 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
24/05/24 20:46:48.254 dag-scheduler-event-loop INFO TaskSchedulerImpl: Cancelling stage 119
24/05/24 20:46:48.254 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage cancelled
24/05/24 20:46:48.255 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 119 (aggregate at DecisionTreeMetadata.scala:125) failed in 0,955 s due to Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 96) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
24/05/24 20:46:48.255 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 97 failed: aggregate at DecisionTreeMetadata.scala:125, took 0,957852 s
24/05/24 20:46:48.256 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 96) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:46:48.256 nioEventLoopGroup-2-2 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 96) (DESKTOP-LH06ASP executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1198)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1191)
	at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)
	at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)
	at org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)
	at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:114)
	at org.apache.spark.ml.Predictor.fit(Predictor.scala:78)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)
	at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)
	at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)
	at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)
	at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)
	at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sparklyr.Invoke.invoke(invoke.scala:161)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:141)
	at sparklyr.StreamHandler.read(stream.scala:62)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:60)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:41)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$3924/1548365532: (struct<Zip_Code_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,Active_Energy_kWh_:double,N_EDIFICIOS_CLASSICOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_1_OU_2_ALOJ_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CLASS_CONST_3_OU_MAIS_ALOJAMENTOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_EXCLUSIV_RESID_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_1_OU_2_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_3_OU_MAIS_PISOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_ANTES_1945_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1946_1980_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_1981_2000_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2001_2010_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_CONSTR_2011_2021_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_EDIFICIOS_COM_NECESSIDADES_REPARACAO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_TOTAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_RHABITUAL_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ALOJAMENTOS_FAM_CLASS_VAGOS_OU_RESID_SECUNDARIA_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ACESSIVEL_CADEIRAS_RODAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_COM_ESTACIONAMENTO_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_PROP_OCUP_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_RHABITUAL_ARRENDADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_AGREGADOS_DOMESTICOS_PRIVADOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_1_OU_2_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_ADP_3_OU_MAIS_PESSOAS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_NUCLEOS_FAMILIARES_COM_FILHOS_TENDO_O_MAIS_NOVO_MENOS_DE_25_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_H_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_M_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_0_14_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_15_24_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_25_64_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,N_INDIVIDUOS_65_OU_MAIS_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,temp:double,feelslike:double,dew:double,humidity:double,precip:double,precipprob_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,windgust:double,windspeed:double,winddir:double,sealevelpressure:double,cloudcover:double,visibility:double,solarradiation_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double,solarenergy:double,uvindex_double_r_formula__2fa0470d_a74e_4ef0_91cf_306eefa7340e:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)
	at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)
	at scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)
	at scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)
	at scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)
	at org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1196)
	at org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2357)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = "error". Consider
removing nulls from dataset or using handleInvalid = "keep" or "skip".
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)
	at org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)
	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)
	... 26 more

24/05/24 20:47:13.363 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:47:13.363 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:47:13.365 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_database: default
24/05/24 20:47:13.365 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_database: default	
24/05/24 20:47:13.366 nioEventLoopGroup-2-2 INFO HiveMetaStore: 0: get_tables: db=default pat=*
24/05/24 20:47:13.366 nioEventLoopGroup-2-2 INFO audit: ugi=pedro	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
24/05/24 20:47:13.411 nioEventLoopGroup-2-2 INFO SparkContext: Starting job: collect at utils.scala:26
24/05/24 20:47:13.411 dag-scheduler-event-loop INFO DAGScheduler: Got job 98 (collect at utils.scala:26) with 1 output partitions
24/05/24 20:47:13.411 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 120 (collect at utils.scala:26)
24/05/24 20:47:13.411 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()
24/05/24 20:47:13.411 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()
24/05/24 20:47:13.411 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[292] at collect at utils.scala:26), which has no missing parents
24/05/24 20:47:13.412 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 7.1 KiB, free 854.3 MiB)
24/05/24 20:47:13.413 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 854.3 MiB)
24/05/24 20:47:13.413 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on DESKTOP-LH06ASP:51464 (size: 3.7 KiB, free: 854.6 MiB)
24/05/24 20:47:13.414 dag-scheduler-event-loop INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1535
24/05/24 20:47:13.414 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[292] at collect at utils.scala:26) (first 15 tasks are for partitions Vector(0))
24/05/24 20:47:13.414 dag-scheduler-event-loop INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks resource profile 0
24/05/24 20:47:13.415 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 97) (DESKTOP-LH06ASP, executor driver, partition 0, PROCESS_LOCAL, 8819 bytes) 
24/05/24 20:47:13.415 Executor task launch worker for task 0.0 in stage 120.0 (TID 97) INFO Executor: Running task 0.0 in stage 120.0 (TID 97)
24/05/24 20:47:13.417 Executor task launch worker for task 0.0 in stage 120.0 (TID 97) INFO Executor: Finished task 0.0 in stage 120.0 (TID 97). 1856 bytes result sent to driver
24/05/24 20:47:13.417 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 97) in 3 ms on DESKTOP-LH06ASP (executor driver) (1/1)
24/05/24 20:47:13.417 task-result-getter-1 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
24/05/24 20:47:13.418 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 120 (collect at utils.scala:26) finished in 0,006 s
24/05/24 20:47:13.418 dag-scheduler-event-loop INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job
24/05/24 20:47:13.418 dag-scheduler-event-loop INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished
24/05/24 20:47:13.418 nioEventLoopGroup-2-2 INFO DAGScheduler: Job 98 finished: collect at utils.scala:26, took 0,007400 s
24/05/24 20:48:40.333 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_97_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 3.7 KiB, free: 854.6 MiB)
24/05/24 20:48:40.337 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_96_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 46.8 KiB, free: 854.7 MiB)
24/05/24 20:48:40.339 dispatcher-BlockManagerMaster INFO BlockManagerInfo: Removed broadcast_95_piece0 on DESKTOP-LH06ASP:51464 in memory (size: 46.8 KiB, free: 854.7 MiB)
24/05/24 20:50:59.941 shutdown-hook-0 INFO SparkContext: Invoking stop() from shutdown hook
24/05/24 20:50:59.942 shutdown-hook-0 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/05/24 20:50:59.970 shutdown-hook-0 INFO SparkUI: Stopped Spark web UI at http://DESKTOP-LH06ASP:4040
24/05/24 20:51:00.007 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/05/24 20:51:00.172 shutdown-hook-0 INFO MemoryStore: MemoryStore cleared
24/05/24 20:51:00.172 shutdown-hook-0 INFO BlockManager: BlockManager stopped
24/05/24 20:51:00.176 shutdown-hook-0 INFO BlockManagerMaster: BlockManagerMaster stopped
24/05/24 20:51:00.180 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/05/24 20:51:00.188 shutdown-hook-0 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2175)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2081)
	at org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 20:51:00.190 shutdown-hook-0 INFO SparkContext: Successfully stopped SparkContext
24/05/24 20:51:00.190 shutdown-hook-0 INFO ShutdownHookManager: Shutdown hook called
24/05/24 20:51:00.191 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e
24/05/24 20:51:00.194 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 20:51:00.194 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68
24/05/24 20:51:00.196 shutdown-hook-0 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68
java.io.IOException: Failed to delete: C:\Users\pedro\AppData\Local\spark\spark-3.4.2-bin-hadoop3\tmp\local\spark-13a2f283-95c4-48d5-8486-b2607096219e\userFiles-0e97dab0-300e-4f91-8e18-8266486b9f68\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:150)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:133)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:121)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
24/05/24 20:51:00.197 shutdown-hook-0 INFO ShutdownHookManager: Deleting directory C:\Users\pedro\AppData\Local\Temp\spark-bdc9d87b-88c9-4366-bebe-1c3419523f5d
